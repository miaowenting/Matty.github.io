<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matty&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2022-02-09T06:29:28.592Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>miaowenting</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Alink 代码连接 Kafka 数据源</title>
    <link href="http://yoursite.com/2022/01/31/Alink%E8%BF%9E%E6%8E%A5Kafka%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>http://yoursite.com/2022/01/31/Alink连接Kafka数据源/</id>
    <published>2022-01-31T06:01:20.000Z</published>
    <updated>2022-02-09T06:29:28.592Z</updated>
    
    <content type="html"><![CDATA[<p>MacOS M1 上，测试使用 Alink 的 java 代码读写 Kafka 数据源，了解一下 Alink API 的基本构成。</p><a id="more"></a><h2 id="Kafka-数据源准备"><a href="#Kafka-数据源准备" class="headerlink" title="Kafka 数据源准备"></a>Kafka 数据源准备</h2><h3 id="在-MacOS-上搭建-Kafka"><a href="#在-MacOS-上搭建-Kafka" class="headerlink" title="在 MacOS 上搭建 Kafka"></a>在 MacOS 上搭建 Kafka</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">brew install Kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># M1 使用以下命令</span></span><br><span class="line">brew install -s Kafka</span><br></pre></td></tr></table></figure><p>安装过程会自动安装zookeeper，并给出zookeeper和kafka的路径、文件个数及存储大小：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/opt/homebrew/Cellar/zookeeper/3.7.0_1: 430 files, 36.5MB</span><br><span class="line">/opt/homebrew/Cellar/kafka/3.0.0: 171 files, 59.5MB</span><br></pre></td></tr></table></figure><p>进入 kafka 的 bin 目录：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/homebrew/Cellar/kafka/2.3.1/bin</span><br></pre></td></tr></table></figure><p>启动 zookeeper：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zookeeper-server-start /opt/homebrew/etc/kafka/zookeeper.properties &amp;</span><br></pre></td></tr></table></figure><p>启动 kafka：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start /opt/homebrew/etc/kafka/server.properties &amp;</span><br></pre></td></tr></table></figure><h3 id="Topic-相关命令行"><a href="#Topic-相关命令行" class="headerlink" title="Topic 相关命令行"></a>Topic 相关命令行</h3><p>创建 Topic：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic iris</span><br><span class="line"></span><br><span class="line">kafka-topics --list --bootstrap-server localhost:9092</span><br></pre></td></tr></table></figure><p>写入 Topic：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer --broker-list localhost:9092 --topic iris</span><br></pre></td></tr></table></figure><p>消费 Topic：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --topic iris --from-beginning</span><br></pre></td></tr></table></figure><h2 id="Alink-写入-Topic"><a href="#Alink-写入-Topic" class="headerlink" title="Alink 写入 Topic"></a>Alink 写入 Topic</h2><h3 id="Kafka011SinkStreamOp"><a href="#Kafka011SinkStreamOp" class="headerlink" title="Kafka011SinkStreamOp"></a>Kafka011SinkStreamOp</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeKafka</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 共 150 条数据</span></span><br><span class="line">String URL = <span class="string">"https://alink-release.oss-cn-beijing.aliyuncs.com/data-files/iris.csv"</span>;</span><br><span class="line">String SCHEMA_STR</span><br><span class="line">= <span class="string">"sepal_length double, sepal_width double, petal_length double, petal_width double, category string"</span>;</span><br><span class="line">CsvSourceStreamOp csvStreamSource = <span class="keyword">new</span> CsvSourceStreamOp().setFilePath(URL).setSchemaStr(SCHEMA_STR);</span><br><span class="line"></span><br><span class="line">Kafka011SinkStreamOp kafkaStreamSink = <span class="keyword">new</span> Kafka011SinkStreamOp()</span><br><span class="line">.setBootstrapServers(<span class="string">"localhost:9092"</span>)</span><br><span class="line">.setDataFormat(<span class="string">"json"</span>)</span><br><span class="line">.setTopic(<span class="string">"iris"</span>);</span><br><span class="line"></span><br><span class="line">csvStreamSource.link(kafkaStreamSink);</span><br><span class="line"></span><br><span class="line">StreamOperator.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Alink-读取-Topic"><a href="#Alink-读取-Topic" class="headerlink" title="Alink 读取 Topic"></a>Alink 读取 Topic</h2><h3 id="Kafka011SourceStreamOp"><a href="#Kafka011SourceStreamOp" class="headerlink" title="Kafka011SourceStreamOp"></a>Kafka011SourceStreamOp</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">readKafka</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Kafka011SourceStreamOp kafkaStreamSource = <span class="keyword">new</span> Kafka011SourceStreamOp()</span><br><span class="line">.setBootstrapServers(<span class="string">"localhost:9092"</span>)</span><br><span class="line">.setTopic(<span class="string">"iris"</span>)</span><br><span class="line">.setStartupMode(<span class="string">"EARLIEST"</span>)</span><br><span class="line">.setGroupId(<span class="string">"alink_group"</span>);</span><br><span class="line"></span><br><span class="line">kafkaStreamSource.print();</span><br><span class="line"></span><br><span class="line">StreamOperator.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="%E7%AE%80%E5%8D%95%E8%AF%BB%E5%8F%96Kafka%E6%95%B0%E6%8D%AE%E6%BA%90.png" alt></p><h3 id="JsonValueStreamOp-进行大-json-处理"><a href="#JsonValueStreamOp-进行大-json-处理" class="headerlink" title="JsonValueStreamOp 进行大 json 处理"></a>JsonValueStreamOp 进行大 json 处理</h3><p>处理从 kafka 读取的 message 大 json 字段，重新设置输出列，使用 JsonValueStreamOp ，通过设置需要提取内容的 JsonPath ，提取出各列数据，字段类型均为 String 。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">readKafka</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Kafka011SourceStreamOp kafkaStreamSource = <span class="keyword">new</span> Kafka011SourceStreamOp()</span><br><span class="line">.setBootstrapServers(<span class="string">"localhost:9092"</span>)</span><br><span class="line">.setTopic(<span class="string">"iris"</span>)</span><br><span class="line">.setStartupMode(<span class="string">"EARLIEST"</span>)</span><br><span class="line">.setGroupId(<span class="string">"alink_group"</span>);</span><br><span class="line"></span><br><span class="line">StreamOperator jsonValueExtractor = kafkaStreamSource</span><br><span class="line">.link(</span><br><span class="line"><span class="keyword">new</span> JsonValueStreamOp()</span><br><span class="line">.setSelectedCol(<span class="string">"message"</span>)</span><br><span class="line">.setReservedCols(<span class="keyword">new</span> String[] &#123;&#125;)</span><br><span class="line">.setOutputCols(</span><br><span class="line"><span class="keyword">new</span> String[] &#123;<span class="string">"sepal_length"</span>, <span class="string">"sepal_width"</span>, <span class="string">"petal_length"</span>, <span class="string">"petal_width"</span>, <span class="string">"category"</span>&#125;)</span><br><span class="line">.setJsonPath(<span class="keyword">new</span> String[] &#123;<span class="string">"$.sepal_length"</span>, <span class="string">"$.sepal_width"</span>, <span class="string">"$.petal_length"</span>, <span class="string">"$.petal_width"</span>,</span><br><span class="line"><span class="string">"$.category"</span>&#125;)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">System.out.print(jsonValueExtractor.getSchema());</span><br><span class="line"></span><br><span class="line">jsonValueExtractor.print();</span><br><span class="line"></span><br><span class="line">StreamOperator.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="%E8%AF%BB%E5%8F%96Kafka%E6%95%B0%E6%8D%AE%E6%BA%90%E5%B9%B6%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E8%BE%93%E5%87%BA%E5%AD%97%E6%AE%B5%E5%88%97.png" alt></p><h3 id="select-进行字段类型转换"><a href="#select-进行字段类型转换" class="headerlink" title="select 进行字段类型转换"></a>select 进行字段类型转换</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">readKafka</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Kafka011SourceStreamOp kafkaStreamSource = <span class="keyword">new</span> Kafka011SourceStreamOp()</span><br><span class="line">.setBootstrapServers(<span class="string">"localhost:9092"</span>)</span><br><span class="line">.setTopic(<span class="string">"iris"</span>)</span><br><span class="line">.setStartupMode(<span class="string">"EARLIEST"</span>)</span><br><span class="line">.setGroupId(<span class="string">"alink_group"</span>);</span><br><span class="line"></span><br><span class="line">StreamOperator jsonValueExtractor = kafkaStreamSource</span><br><span class="line">.link(</span><br><span class="line"><span class="keyword">new</span> JsonValueStreamOp()</span><br><span class="line">.setSelectedCol(<span class="string">"message"</span>)</span><br><span class="line">.setReservedCols(<span class="keyword">new</span> String[] &#123;&#125;)</span><br><span class="line">.setOutputCols(</span><br><span class="line"><span class="keyword">new</span> String[] &#123;<span class="string">"sepal_length"</span>, <span class="string">"sepal_width"</span>, <span class="string">"petal_length"</span>, <span class="string">"petal_width"</span>, <span class="string">"category"</span>&#125;)</span><br><span class="line">.setJsonPath(<span class="keyword">new</span> String[] &#123;<span class="string">"$.sepal_length"</span>, <span class="string">"$.sepal_width"</span>, <span class="string">"$.petal_length"</span>, <span class="string">"$.petal_width"</span>,</span><br><span class="line"><span class="string">"$.category"</span>&#125;)</span><br><span class="line">)</span><br><span class="line">.select(<span class="string">"CAST(sepal_length AS DOUBLE) AS sepal_length, "</span></span><br><span class="line">+ <span class="string">"CAST(sepal_width AS DOUBLE) AS sepal_width, "</span></span><br><span class="line">+ <span class="string">"CAST(petal_length AS DOUBLE) AS petal_length, "</span></span><br><span class="line">+ <span class="string">"CAST(petal_width AS DOUBLE) AS petal_width, category"</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">System.out.print(jsonValueExtractor.getSchema());</span><br><span class="line"></span><br><span class="line">jsonValueExtractor.print();</span><br><span class="line"></span><br><span class="line">StreamOperator.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="%E8%AF%BB%E5%8F%96Kafka%E6%95%B0%E6%8D%AE%E6%BA%90%E5%B9%B6%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E8%BE%93%E5%87%BA%E5%AD%97%E6%AE%B5%E5%88%97_%E8%BD%AC%E6%8D%A2%E5%AD%97%E6%AE%B5%E7%B1%BB%E5%9E%8B.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MacOS M1 上，测试使用 Alink 的 java 代码读写 Kafka 数据源，了解一下 Alink API 的基本构成。&lt;/p&gt;
    
    </summary>
    
      <category term="Alink" scheme="http://yoursite.com/categories/Alink/"/>
    
    
  </entry>
  
  <entry>
    <title>Alink 快速使用 WebUI</title>
    <link href="http://yoursite.com/2022/01/30/Alink%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8%E5%92%8C%E5%BC%80%E5%8F%91/"/>
    <id>http://yoursite.com/2022/01/30/Alink快速使用和开发/</id>
    <published>2022-01-30T06:01:20.000Z</published>
    <updated>2022-02-09T07:12:02.403Z</updated>
    
    <content type="html"><![CDATA[<p>MacOS M1 上，使用 Docker 构建快速使用和开发 Alink 的相关镜像，启动 Alink 的 server、web、notebook以及数据库服务等，可以在 web 界面上进行 Alink 相关功能的使用。<br>Alink 提供的 Web UI 工具，可以理解成是 AIStudio 的简易操作版，供学习使用。</p><a id="more"></a><h2 id="快速上手"><a href="#快速上手" class="headerlink" title="快速上手"></a>快速上手</h2><ol><li><p>参考 <a href="https://docs.docker.com/get-docker/" target="_blank" rel="noopener">https://docs.docker.com/get-docker/</a> 安装 Docker；<br><img src="docker%E5%90%AF%E5%8A%A8%E7%8A%B6%E6%80%81%E5%9B%BE.png" alt></p></li><li><p>参考 <a href="https://umijs.org/zh-CN/docs/getting-started" target="_blank" rel="noopener">https://umijs.org/zh-CN/docs/getting-started</a> 准备好 node 和 yarn；</p></li><li><p>克隆 Alink 代码：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/alibaba/Alink.git</span><br><span class="line"><span class="built_in">cd</span> Alink</span><br></pre></td></tr></table></figure></li><li><p>下载 shade 后的 Alink 包到 webui/tools/flink-with-alink-jar/</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://alink-release.oss-cn-beijing.aliyuncs.com/v1.5.0/alink_core_flink-1.9_2.11-1.5.0.jar -P webui/tools/flink-with-alink-jar/</span><br></pre></td></tr></table></figure></li><li><p>进入到目录 webui/web/，执行 yarn：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> webui/web/</span><br><span class="line">yarn</span><br></pre></td></tr></table></figure></li><li><p>进入到目录 webui/tools/，执行 sh build-image.sh，等待编译 docker 镜像；<br>M1 芯片由于底层架构的问题：</p></li></ol><ul><li><p>需要使用 linux/x86_64 平台的 mysql 镜像，在 <code>webui/tools/docker-compose/alink/docker-compose.yml</code> 的第 41 行前增加一行 <code>platform: linux/x86_64</code>：<br><img src="M1_%E4%BF%AE%E6%94%B9Alink_docker%E6%96%87%E4%BB%B6_1.png" alt></p></li><li><p>需要使用 linux/x86_64 平台的 flink 镜像，修改 <code>webui/tools/flink-with-alink-jar/Dockerfile</code> 的第一行，修改后为：<code>FROM --platform=linux/x86_64 flink:1.9.2-scala_2.11</code>：<br><img src="M1_%E4%BF%AE%E6%94%B9Alink_docker%E6%96%87%E4%BB%B6_2.png" alt></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../tools</span><br><span class="line">sh build-image.sh</span><br></pre></td></tr></table></figure></li></ul><ol start="7"><li><p>编译成功后，进入到目录 webui/tools/docker-compose/alink/nfs/，执行 docker-compose up -d，这将启动一个 NFS 服务；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> docker-compose/alink/nfs/</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure></li><li><p>进入到目录 webui/tools/docker-compose/alink，执行 docker-compose up -d，这将启动 Alink 的 server、web、notebook以及数据库等服务；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure></li><li><p>在浏览器打开 localhost:9090，就能开始使用了。<br><img src="Alink_WebUI%E5%BC%80%E5%8F%91%E7%95%8C%E9%9D%A2.png" alt></p></li></ol><h2 id="开发步骤"><a href="#开发步骤" class="headerlink" title="开发步骤"></a>开发步骤</h2><h3 id="单启-Web"><a href="#单启-Web" class="headerlink" title="单启 Web"></a>单启 Web</h3><ol><li><p>设置环境变量：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim etc/profile</span><br><span class="line"><span class="built_in">export</span> WEB_CONTAINER_ID=$(docker ps -a -q --filter=<span class="string">"name=alink_web"</span>)</span><br></pre></td></tr></table></figure></li><li><p>将在<code>快速上手</code>中启动的 Web 服务杀掉：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">kill</span> <span class="variable">$WEB_CONTAINER_ID</span> &amp;&amp; docker rm <span class="variable">$WEB_CONTAINER_ID</span></span><br></pre></td></tr></table></figure></li><li><p>通过代码启动 Web 服务：<br><img src="%E9%80%9A%E8%BF%87%E4%BB%A3%E7%A0%81%E5%90%AF%E5%8A%A8web%E6%9C%8D%E5%8A%A1.png" alt></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> webui/web/</span><br><span class="line">yarn start</span><br></pre></td></tr></table></figure></li><li><p>在浏览器打开 localhost:8000，可以实时预览 Web 代码的修改。<br><img src="%E9%80%9A%E8%BF%87%E4%BB%A3%E7%A0%81%E5%90%AF%E5%8A%A8web%E6%9C%8D%E5%8A%A1%E5%90%8E%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8%E7%95%8C%E9%9D%A2.png" alt></p></li></ol><h3 id="单启-Server"><a href="#单启-Server" class="headerlink" title="单启 Server"></a>单启 Server</h3><ol><li><p>设置环境变量：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim etc/profile</span><br><span class="line"><span class="built_in">export</span> SERVER_CONTAINER_ID=$(docker ps -a -q --filter=<span class="string">"name=alink_server"</span>)</span><br></pre></td></tr></table></figure></li><li><p>将在<code>快速上手</code>中启动的 Server 服务杀掉：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">kill</span> <span class="variable">$SERVER_CONTAINER_ID</span> &amp;&amp; docker rm <span class="variable">$SERVER_CONTAINER_ID</span></span><br></pre></td></tr></table></figure></li><li><p>使用 Intellij IDEA 等工具打开 Server 部分的代码 webui/server/，启动 com.alibaba.alink.server.ServerApplication 类。</p></li></ol><ul><li>修改 application.properties 配置文件，配置信息从 <code>webui/tools/docker-compose/alink/docker-compose.yml</code> 文件中获取：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spring.datasource.driverClassName=com.mysql.jdbc.Driver</span><br><span class="line">spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect</span><br><span class="line">spring.jpa.hibernate.ddl-auto: update</span><br><span class="line">#spring.datasource.url=$&#123;SPRING_DATASOURCE_URL&#125;</span><br><span class="line">#spring.datasource.username=$&#123;SPRING_DATASOURCE_USERNAME&#125;</span><br><span class="line">#spring.datasource.password=$&#123;SPRING_DATASOURCE_PASSWORD&#125;</span><br><span class="line">#域名修改为 localhost</span><br><span class="line">spring.datasource.url=jdbc:mysql://localhost:3306/alink?useSSL=false&amp;serverTimezone=UTC&amp;useLegacyDatetimeCode=false</span><br><span class="line">spring.datasource.username=alinkalink</span><br><span class="line">spring.datasource.password=alinkalink</span><br><span class="line"></span><br><span class="line">alink.execution.type=remote</span><br><span class="line">#alink.execution.remote-cluster-host=$&#123;ALINK_REMOTE_CLUSTER_HOST&#125;</span><br><span class="line">#alink.execution.remote-cluster-port=$&#123;ALINK_REMOTE_CLUSTER_PORT&#125;</span><br><span class="line">#alink.execution.remote-cluster-host=flink-jobmanager</span><br><span class="line">#域名修改为 localhost</span><br><span class="line">alink.execution.remote-cluster-host=localhost</span><br><span class="line">alink.execution.remote-cluster-port=8081</span><br></pre></td></tr></table></figure></li></ul><p><img src="IDEA%E4%B8%AD%E5%90%AF%E5%8A%A8Server.png" alt></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.alink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>alink_core_flink-1.9_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="流程操作示例"><a href="#流程操作示例" class="headerlink" title="流程操作示例"></a>流程操作示例</h3><h4 id="csv-gt-Kafka"><a href="#csv-gt-Kafka" class="headerlink" title="csv -&gt; Kafka"></a>csv -&gt; Kafka</h4><ul><li><p>读取csv，需要设置的相关参数如下<br>filePath：”<a href="https://alink-release.oss-cn-beijing.aliyuncs.com/data-files/iris.csv&quot;" target="_blank" rel="noopener">https://alink-release.oss-cn-beijing.aliyuncs.com/data-files/iris.csv&quot;</a><br>schemaStr：”sepal_length double, sepal_width double, petal_length double, petal_width double, category string”<br><img src="%E8%AF%BB%E5%8F%96csv.png" alt></p></li><li><p>写入Kafka，需要设置的相关参数如下：<br>bootstrapServers：”localhost:9092”<br><img src="%E5%86%99%E5%85%A5Kafka.png" alt></p></li></ul><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><code>webui/tools/docker-compose/alink/docker-compose.yml</code> 启动文件，包括 Alink 的 server、web、notebook以及数据库服务等：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Docker Compose file Reference (https://docs.docker.com/compose/compose-file/)</span></span><br><span class="line"><span class="comment"># Refs: https://www.alinkalink.com/spring-boot-mysql-react-docker-compose-example/</span></span><br><span class="line"></span><br><span class="line"><span class="attr">version:</span> <span class="string">'3.7'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define services</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="comment"># App backend service</span></span><br><span class="line"><span class="attr">  server:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">alink_server:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8080:8080"</span> <span class="comment"># Forward the exposed port 8080 on the container to port 8080 on the host machine</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      ALINK_REMOTE_CLUSTER_HOST:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">      ALINK_REMOTE_CLUSTER_PORT:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">      SPRING_DATASOURCE_URL:</span> <span class="attr">jdbc:mysql://db:3306/alink?useSSL=false&amp;serverTimezone=UTC&amp;useLegacyDatetimeCode=false</span></span><br><span class="line"><span class="attr">      SPRING_DATASOURCE_USERNAME:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">      SPRING_DATASOURCE_PASSWORD:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">db</span> <span class="comment"># This service depends on mysql. Start that first.</span></span><br><span class="line"><span class="attr">    networks:</span> <span class="comment"># Networks to join (Services on the same network can communicate with each other using their name)</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Frontend Service</span></span><br><span class="line"><span class="attr">  web:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">alink_web:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9090:9090"</span> <span class="comment"># Map the exposed port 80 on the container to port 9090 on the host machine</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">server</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Database Service (Mysql)</span></span><br><span class="line"><span class="attr">  db:</span></span><br><span class="line"><span class="attr">    platform:</span> <span class="string">linux/x86_64</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">mysql:5.7</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"3306:3306"</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      MYSQL_DATABASE:</span> <span class="string">alink</span></span><br><span class="line"><span class="attr">      MYSQL_USER:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">      MYSQL_PASSWORD:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">      MYSQL_ROOT_PASSWORD:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - db-data:</span><span class="string">/var/lib/mysql</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  flink-jobmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">flink_with_alink_jar:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8081:8081"</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      FLINK_PROPERTIES:</span> <span class="string">|-</span></span><br><span class="line">        <span class="string">jobmanager.rpc.address:</span> <span class="string">flink-jobmanager</span></span><br><span class="line">        <span class="string">jobmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">taskmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">classloader.resolve-order:</span> <span class="string">parent-first</span></span><br><span class="line">        <span class="string">taskmanager.memory.preallocate:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.fraction:</span> <span class="number">0.3</span><span class="string">f</span></span><br><span class="line">        <span class="string">akka.ask.timeout:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line">        <span class="string">akka.client.timeout:</span> <span class="number">120</span><span class="string">s</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">jobmanager</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  flink-taskmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">flink_with_alink_jar:v0.1</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      FLINK_PROPERTIES:</span> <span class="string">|-</span></span><br><span class="line">        <span class="string">jobmanager.rpc.address:</span> <span class="string">flink-jobmanager</span></span><br><span class="line">        <span class="string">taskmanager.numberOfTaskSlots:</span> <span class="number">1</span></span><br><span class="line">        <span class="string">jobmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">taskmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">classloader.resolve-order:</span> <span class="string">parent-first</span></span><br><span class="line">        <span class="string">taskmanager.memory.preallocate:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.fraction:</span> <span class="number">0.3</span><span class="string">f</span></span><br><span class="line">        <span class="string">akka.ask.timeout:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line">        <span class="string">akka.client.timeout:</span> <span class="number">120</span><span class="string">s</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span>    </span><br><span class="line"><span class="attr">    command:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">    deploy:</span></span><br><span class="line"><span class="attr">      replicas:</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  notebook:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">alink_notebook:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8888:8888"</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - notebook-data:</span><span class="string">/home/jovyan/</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">"start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Volumes</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="attr">  db-data:</span></span><br><span class="line"><span class="attr">  notebook-data:</span></span><br><span class="line"><span class="attr">  shared-data:</span></span><br><span class="line"><span class="attr">    driver:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">    driver_opts:</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">      o:</span> <span class="string">nfsvers=4,addr=localhost,rw</span></span><br><span class="line"><span class="attr">      device:</span> <span class="string">":/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Networks to be created to facilitate communication between containers</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line"><span class="attr">  backend:</span></span><br><span class="line"><span class="attr">  frontend:</span></span><br><span class="line"><span class="attr">  flink:</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MacOS M1 上，使用 Docker 构建快速使用和开发 Alink 的相关镜像，启动 Alink 的 server、web、notebook以及数据库服务等，可以在 web 界面上进行 Alink 相关功能的使用。&lt;br&gt;Alink 提供的 Web UI 工具，可以理解成是 AIStudio 的简易操作版，供学习使用。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Alink 快速使用 WebUI</title>
    <link href="http://yoursite.com/2022/01/30/Alink%20%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8%20WebUI/"/>
    <id>http://yoursite.com/2022/01/30/Alink 快速使用 WebUI/</id>
    <published>2022-01-30T06:01:20.000Z</published>
    <updated>2022-02-09T07:12:02.403Z</updated>
    
    <content type="html"><![CDATA[<p>MacOS M1 上，使用 Docker 构建快速使用和开发 Alink 的相关镜像，启动 Alink 的 server、web、notebook以及数据库服务等，可以在 web 界面上进行 Alink 相关功能的使用。<br>Alink 提供的 Web UI 工具，可以理解成是 AIStudio 的简易操作版，供学习使用。</p><a id="more"></a><h2 id="快速上手"><a href="#快速上手" class="headerlink" title="快速上手"></a>快速上手</h2><ol><li><p>参考 <a href="https://docs.docker.com/get-docker/" target="_blank" rel="noopener">https://docs.docker.com/get-docker/</a> 安装 Docker；<br><img src="docker%E5%90%AF%E5%8A%A8%E7%8A%B6%E6%80%81%E5%9B%BE.png" alt></p></li><li><p>参考 <a href="https://umijs.org/zh-CN/docs/getting-started" target="_blank" rel="noopener">https://umijs.org/zh-CN/docs/getting-started</a> 准备好 node 和 yarn；</p></li><li><p>克隆 Alink 代码：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/alibaba/Alink.git</span><br><span class="line"><span class="built_in">cd</span> Alink</span><br></pre></td></tr></table></figure></li><li><p>下载 shade 后的 Alink 包到 webui/tools/flink-with-alink-jar/</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://alink-release.oss-cn-beijing.aliyuncs.com/v1.5.0/alink_core_flink-1.9_2.11-1.5.0.jar -P webui/tools/flink-with-alink-jar/</span><br></pre></td></tr></table></figure></li><li><p>进入到目录 webui/web/，执行 yarn：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> webui/web/</span><br><span class="line">yarn</span><br></pre></td></tr></table></figure></li><li><p>进入到目录 webui/tools/，执行 sh build-image.sh，等待编译 docker 镜像；<br>M1 芯片由于底层架构的问题：</p></li></ol><ul><li><p>需要使用 linux/x86_64 平台的 mysql 镜像，在 <code>webui/tools/docker-compose/alink/docker-compose.yml</code> 的第 41 行前增加一行 <code>platform: linux/x86_64</code>：<br><img src="M1_%E4%BF%AE%E6%94%B9Alink_docker%E6%96%87%E4%BB%B6_1.png" alt></p></li><li><p>需要使用 linux/x86_64 平台的 flink 镜像，修改 <code>webui/tools/flink-with-alink-jar/Dockerfile</code> 的第一行，修改后为：<code>FROM --platform=linux/x86_64 flink:1.9.2-scala_2.11</code>：<br><img src="M1_%E4%BF%AE%E6%94%B9Alink_docker%E6%96%87%E4%BB%B6_2.png" alt></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../tools</span><br><span class="line">sh build-image.sh</span><br></pre></td></tr></table></figure></li></ul><ol start="7"><li><p>编译成功后，进入到目录 webui/tools/docker-compose/alink/nfs/，执行 docker-compose up -d，这将启动一个 NFS 服务；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> docker-compose/alink/nfs/</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure></li><li><p>进入到目录 webui/tools/docker-compose/alink，执行 docker-compose up -d，这将启动 Alink 的 server、web、notebook以及数据库等服务；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure></li><li><p>在浏览器打开 localhost:9090，就能开始使用了。<br><img src="Alink_WebUI%E5%BC%80%E5%8F%91%E7%95%8C%E9%9D%A2.png" alt></p></li></ol><h2 id="开发步骤"><a href="#开发步骤" class="headerlink" title="开发步骤"></a>开发步骤</h2><h3 id="单启-Web"><a href="#单启-Web" class="headerlink" title="单启 Web"></a>单启 Web</h3><ol><li><p>设置环境变量：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim etc/profile</span><br><span class="line"><span class="built_in">export</span> WEB_CONTAINER_ID=$(docker ps -a -q --filter=<span class="string">"name=alink_web"</span>)</span><br></pre></td></tr></table></figure></li><li><p>将在<code>快速上手</code>中启动的 Web 服务杀掉：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">kill</span> <span class="variable">$WEB_CONTAINER_ID</span> &amp;&amp; docker rm <span class="variable">$WEB_CONTAINER_ID</span></span><br></pre></td></tr></table></figure></li><li><p>通过代码启动 Web 服务：<br><img src="%E9%80%9A%E8%BF%87%E4%BB%A3%E7%A0%81%E5%90%AF%E5%8A%A8web%E6%9C%8D%E5%8A%A1.png" alt></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> webui/web/</span><br><span class="line">yarn start</span><br></pre></td></tr></table></figure></li><li><p>在浏览器打开 localhost:8000，可以实时预览 Web 代码的修改。<br><img src="%E9%80%9A%E8%BF%87%E4%BB%A3%E7%A0%81%E5%90%AF%E5%8A%A8web%E6%9C%8D%E5%8A%A1%E5%90%8E%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8%E7%95%8C%E9%9D%A2.png" alt></p></li></ol><h3 id="单启-Server"><a href="#单启-Server" class="headerlink" title="单启 Server"></a>单启 Server</h3><ol><li><p>设置环境变量：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim etc/profile</span><br><span class="line"><span class="built_in">export</span> SERVER_CONTAINER_ID=$(docker ps -a -q --filter=<span class="string">"name=alink_server"</span>)</span><br></pre></td></tr></table></figure></li><li><p>将在<code>快速上手</code>中启动的 Server 服务杀掉：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">kill</span> <span class="variable">$SERVER_CONTAINER_ID</span> &amp;&amp; docker rm <span class="variable">$SERVER_CONTAINER_ID</span></span><br></pre></td></tr></table></figure></li><li><p>使用 Intellij IDEA 等工具打开 Server 部分的代码 webui/server/，启动 com.alibaba.alink.server.ServerApplication 类。</p></li></ol><ul><li>修改 application.properties 配置文件，配置信息从 <code>webui/tools/docker-compose/alink/docker-compose.yml</code> 文件中获取：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spring.datasource.driverClassName=com.mysql.jdbc.Driver</span><br><span class="line">spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect</span><br><span class="line">spring.jpa.hibernate.ddl-auto: update</span><br><span class="line">#spring.datasource.url=$&#123;SPRING_DATASOURCE_URL&#125;</span><br><span class="line">#spring.datasource.username=$&#123;SPRING_DATASOURCE_USERNAME&#125;</span><br><span class="line">#spring.datasource.password=$&#123;SPRING_DATASOURCE_PASSWORD&#125;</span><br><span class="line">#域名修改为 localhost</span><br><span class="line">spring.datasource.url=jdbc:mysql://localhost:3306/alink?useSSL=false&amp;serverTimezone=UTC&amp;useLegacyDatetimeCode=false</span><br><span class="line">spring.datasource.username=alinkalink</span><br><span class="line">spring.datasource.password=alinkalink</span><br><span class="line"></span><br><span class="line">alink.execution.type=remote</span><br><span class="line">#alink.execution.remote-cluster-host=$&#123;ALINK_REMOTE_CLUSTER_HOST&#125;</span><br><span class="line">#alink.execution.remote-cluster-port=$&#123;ALINK_REMOTE_CLUSTER_PORT&#125;</span><br><span class="line">#alink.execution.remote-cluster-host=flink-jobmanager</span><br><span class="line">#域名修改为 localhost</span><br><span class="line">alink.execution.remote-cluster-host=localhost</span><br><span class="line">alink.execution.remote-cluster-port=8081</span><br></pre></td></tr></table></figure></li></ul><p><img src="IDEA%E4%B8%AD%E5%90%AF%E5%8A%A8Server.png" alt></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.alink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>alink_core_flink-1.9_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="流程操作示例"><a href="#流程操作示例" class="headerlink" title="流程操作示例"></a>流程操作示例</h3><h4 id="csv-gt-Kafka"><a href="#csv-gt-Kafka" class="headerlink" title="csv -&gt; Kafka"></a>csv -&gt; Kafka</h4><ul><li><p>读取csv，需要设置的相关参数如下<br>filePath：”<a href="https://alink-release.oss-cn-beijing.aliyuncs.com/data-files/iris.csv&quot;" target="_blank" rel="noopener">https://alink-release.oss-cn-beijing.aliyuncs.com/data-files/iris.csv&quot;</a><br>schemaStr：”sepal_length double, sepal_width double, petal_length double, petal_width double, category string”<br><img src="%E8%AF%BB%E5%8F%96csv.png" alt></p></li><li><p>写入Kafka，需要设置的相关参数如下：<br>bootstrapServers：”localhost:9092”<br><img src="%E5%86%99%E5%85%A5Kafka.png" alt></p></li></ul><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><code>webui/tools/docker-compose/alink/docker-compose.yml</code> 启动文件，包括 Alink 的 server、web、notebook以及数据库服务等：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Docker Compose file Reference (https://docs.docker.com/compose/compose-file/)</span></span><br><span class="line"><span class="comment"># Refs: https://www.alinkalink.com/spring-boot-mysql-react-docker-compose-example/</span></span><br><span class="line"></span><br><span class="line"><span class="attr">version:</span> <span class="string">'3.7'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define services</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="comment"># App backend service</span></span><br><span class="line"><span class="attr">  server:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">alink_server:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8080:8080"</span> <span class="comment"># Forward the exposed port 8080 on the container to port 8080 on the host machine</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      ALINK_REMOTE_CLUSTER_HOST:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">      ALINK_REMOTE_CLUSTER_PORT:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">      SPRING_DATASOURCE_URL:</span> <span class="attr">jdbc:mysql://db:3306/alink?useSSL=false&amp;serverTimezone=UTC&amp;useLegacyDatetimeCode=false</span></span><br><span class="line"><span class="attr">      SPRING_DATASOURCE_USERNAME:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">      SPRING_DATASOURCE_PASSWORD:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">db</span> <span class="comment"># This service depends on mysql. Start that first.</span></span><br><span class="line"><span class="attr">    networks:</span> <span class="comment"># Networks to join (Services on the same network can communicate with each other using their name)</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Frontend Service</span></span><br><span class="line"><span class="attr">  web:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">alink_web:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9090:9090"</span> <span class="comment"># Map the exposed port 80 on the container to port 9090 on the host machine</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">server</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Database Service (Mysql)</span></span><br><span class="line"><span class="attr">  db:</span></span><br><span class="line"><span class="attr">    platform:</span> <span class="string">linux/x86_64</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">mysql:5.7</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"3306:3306"</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      MYSQL_DATABASE:</span> <span class="string">alink</span></span><br><span class="line"><span class="attr">      MYSQL_USER:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">      MYSQL_PASSWORD:</span> <span class="string">alinkalink</span></span><br><span class="line"><span class="attr">      MYSQL_ROOT_PASSWORD:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - db-data:</span><span class="string">/var/lib/mysql</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  flink-jobmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">flink_with_alink_jar:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8081:8081"</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      FLINK_PROPERTIES:</span> <span class="string">|-</span></span><br><span class="line">        <span class="string">jobmanager.rpc.address:</span> <span class="string">flink-jobmanager</span></span><br><span class="line">        <span class="string">jobmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">taskmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">classloader.resolve-order:</span> <span class="string">parent-first</span></span><br><span class="line">        <span class="string">taskmanager.memory.preallocate:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.fraction:</span> <span class="number">0.3</span><span class="string">f</span></span><br><span class="line">        <span class="string">akka.ask.timeout:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line">        <span class="string">akka.client.timeout:</span> <span class="number">120</span><span class="string">s</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">jobmanager</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  flink-taskmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">flink_with_alink_jar:v0.1</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      FLINK_PROPERTIES:</span> <span class="string">|-</span></span><br><span class="line">        <span class="string">jobmanager.rpc.address:</span> <span class="string">flink-jobmanager</span></span><br><span class="line">        <span class="string">taskmanager.numberOfTaskSlots:</span> <span class="number">1</span></span><br><span class="line">        <span class="string">jobmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">taskmanager.heap.size:</span> <span class="number">2048</span><span class="string">m</span></span><br><span class="line">        <span class="string">classloader.resolve-order:</span> <span class="string">parent-first</span></span><br><span class="line">        <span class="string">taskmanager.memory.preallocate:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line">        <span class="string">taskmanager.memory.fraction:</span> <span class="number">0.3</span><span class="string">f</span></span><br><span class="line">        <span class="string">akka.ask.timeout:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line">        <span class="string">akka.client.timeout:</span> <span class="number">120</span><span class="string">s</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span>    </span><br><span class="line"><span class="attr">    command:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">    deploy:</span></span><br><span class="line"><span class="attr">      replicas:</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  notebook:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">alink_notebook:v0.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8888:8888"</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">backend</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - notebook-data:</span><span class="string">/home/jovyan/</span></span><br><span class="line"><span class="attr">      - shared-data:</span><span class="string">/alink</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">"start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Volumes</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="attr">  db-data:</span></span><br><span class="line"><span class="attr">  notebook-data:</span></span><br><span class="line"><span class="attr">  shared-data:</span></span><br><span class="line"><span class="attr">    driver:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">    driver_opts:</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">      o:</span> <span class="string">nfsvers=4,addr=localhost,rw</span></span><br><span class="line"><span class="attr">      device:</span> <span class="string">":/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Networks to be created to facilitate communication between containers</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line"><span class="attr">  backend:</span></span><br><span class="line"><span class="attr">  frontend:</span></span><br><span class="line"><span class="attr">  flink:</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MacOS M1 上，使用 Docker 构建快速使用和开发 Alink 的相关镜像，启动 Alink 的 server、web、notebook以及数据库服务等，可以在 web 界面上进行 Alink 相关功能的使用。&lt;br&gt;Alink 提供的 Web UI 工具，可以理解成是 AIStudio 的简易操作版，供学习使用。&lt;/p&gt;
    
    </summary>
    
      <category term="Alink" scheme="http://yoursite.com/categories/Alink/"/>
    
    
  </entry>
  
  <entry>
    <title>内向者优势</title>
    <link href="http://yoursite.com/2021/12/05/%E5%86%85%E5%90%91%E8%80%85%E4%BC%98%E5%8A%BF/"/>
    <id>http://yoursite.com/2021/12/05/内向者优势/</id>
    <published>2021-12-05T14:59:29.000Z</published>
    <updated>2021-12-07T03:48:14.012Z</updated>
    
    <content type="html"><![CDATA[<p>原来我不是不如别人，而是因为我本来就不是那样子的人。</p><a id="more"></a><table><thead><tr><th>内向者</th><th>外向者</th></tr></thead><tbody><tr><td>通过独处获得能量</td><td>通过与其他人相处来获得能量</td></tr><tr><td>避免成为焦点</td><td>喜欢成为焦点</td></tr><tr><td>思考周全后再行动</td><td>边行动边思考</td></tr><tr><td>注重隐私，只与少数人分享个人信息</td><td>会较自由地分享个人信息</td></tr><tr><td>听多于讲</td><td>讲多于听</td></tr><tr><td>不太需要外在刺激</td><td>容易觉得无聊，需要外在刺激</td></tr><tr><td>思考周全之后才做出回复，喜欢较慢的步调</td><td>快速回复，喜欢较快的步调</td></tr><tr><td>注重深度胜于广度</td><td>注重广度胜于深度</td></tr><tr><td>不易受干扰</td><td>容易分心</td></tr><tr><td>偏好书面沟通</td><td>偏好口头沟通</td></tr><tr><td>喜欢、擅长独立作业</td><td>喜欢在团队中与他人合作</td></tr><tr><td>表达方式谨慎，字斟句酌</td><td>很快说出自己的感觉，表达具渲染力与喜剧效果</td></tr><tr><td>重视细节</td><td>喜欢不复杂、容易获取的信息</td></tr><tr><td>即使在漫长而复杂的决策过程中，仍较容易保持专注</td><td>对漫长而复杂的决策过程容易感到疲惫、失去耐心</td></tr></tbody></table><ul><li><p>Daily Work = Natural Work + Imposed Work.</p></li><li><p>展现亮眼的工作能力，以及被看到，一是向上管理，二是在做关键简报时漂亮出击。</p></li><li><p>成熟的标识是，不是非黑即白，允许灰色地带。</p></li><li><p>直接站在别人面前，可以传递许多键盘和屏幕所无法表达的事情。</p></li><li><p>让听者愉悦地接受才是沟通的重点。</p></li><li><p>营销就是一切，一切都是营销，无处不在。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原来我不是不如别人，而是因为我本来就不是那样子的人。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink用法-CDC-实现 MySQL 数据实时写入 Apache Doris</title>
    <link href="http://yoursite.com/2021/12/05/Flink%E7%94%A8%E6%B3%95-CDC-%E5%AE%9E%E7%8E%B0-MySQL-%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5-Apache-Doris/"/>
    <id>http://yoursite.com/2021/12/05/Flink用法-CDC-实现-MySQL-数据实时写入-Apache-Doris/</id>
    <published>2021-12-05T07:50:48.000Z</published>
    <updated>2021-12-13T06:07:48.812Z</updated>
    
    <content type="html"><![CDATA[<p>MacOS M1 上，使用 Docker 构建  MySQL 和 Doris 上的 Streaming ETL，试用 Flink CDC 相关功能。</p><a id="more"></a><h2 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h2><h3 id="下载-Flink-Doris-Connector"><a href="#下载-Flink-Doris-Connector" class="headerlink" title="下载 Flink Doris Connector"></a>下载 Flink Doris Connector</h3><p>Apache Doris 是一个现代化的 MPP 分析型数据库产品。仅需亚秒级响应时间即可获得查询结果，有效地支持实时数据分析。Apache Doris 的分布式架构非常简洁，易于运维，并且可以支持 10PB 以上的超大数据集。</p><p>Apache Doris 可以满足多种数据分析需求，例如固定历史报表，实时数据分析，交互式数据分析和探索式数据分析等。</p><p>Flink Doris Connector 是 Doris 社区为了方便用户使用 Flink 读写 Doris 数据表的一个扩展，目前 Doris 支持 Flink 1.11.x ，1.12.x ，1.13.x ； Scala 2.12.x 。</p><p>目前 Flink Doris Connector 控制写入有两个参数，这两个参数同时起作用，哪个条件先到就触发写入 Doris 表操作：</p><ul><li>sink.batch.size： 每多少条写入一次，默认100条；</li><li>sink.batch.interval：批次间的写入间隔，默认1秒；</li></ul><p>下载 Flink Doris Connector jar 包： <img src="doris-flink-1.0-SNAPSHOT.jar" alt="doris-flink-1.0-SNAPSHOT.jar"></p><h3 id="下载-Flink-MySQL-CDC-Connector"><a href="#下载-Flink-MySQL-CDC-Connector" class="headerlink" title="下载 Flink MySQL CDC Connector"></a>下载 Flink MySQL CDC Connector</h3><p>下载 Flink MySQL CDC Connector jar 包：  <a href="https://repo1.maven.org/maven2/com/ververica/flink-connector-mysql-cdc/2.0.2/flink-connector-mysql-cdc-2.0.2.jar" target="_blank" rel="noopener">flink-connector-mysql-cdc-2.0.2.jar</a></p><h3 id="安装-Flink"><a href="#安装-Flink" class="headerlink" title="安装 Flink"></a>安装 Flink</h3><p>下载 Flink-1.13.3 安装包： <a href="https://dlcdn.apache.org/flink/flink-1.13.3/flink-1.13.3-bin-scala_2.12.tgz" target="_blank" rel="noopener">flink-1.13.3-bin-scala_2.12.tgz</a></p><ol><li><p>进入 Flink 安装目录：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$FLINK_HOME</span></span><br></pre></td></tr></table></figure></li><li><p>将 Flink Doris Connector jar 包、Flink MySQL CDC Connector jar 包放入 Flink lib 目录下</p></li><li><p>启动 Flink 集群：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-cluster.sh</span><br></pre></td></tr></table></figure></li></ol><p>启动成功的话，可以在 <a href="http://localhost:8081/" target="_blank" rel="noopener">http://localhost:8081/</a> 访问到 Flink Web UI。</p><h3 id="安装-MySQL"><a href="#安装-MySQL" class="headerlink" title="安装 MySQL"></a>安装 MySQL</h3><ol><li><p>docker 仓库搜索 MySQL</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search mysql</span><br></pre></td></tr></table></figure></li><li><p>docker 仓库拉取 MySQL 8.0</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull mysql:8.0</span><br></pre></td></tr></table></figure></li><li><p>查看本地仓库镜像是否拉取成功</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images mysql:8.0</span><br></pre></td></tr></table></figure></li><li><p>安装运行 MySQL 8.0</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 3307:3306 -name mysql8.0 -e MYSQL_ROOT_PASSWORD=root -d mysql:8.0</span><br></pre></td></tr></table></figure></li><li><p>查看 MySQL 8.0 容器的运行情况</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure></li><li><p>docker 登录 MySQL 8.0</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mysql8.0 bash</span><br><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure></li></ol><h3 id="安装-Doris"><a href="#安装-Doris" class="headerlink" title="安装 Doris"></a>安装 Doris</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -v <span class="variable">$MAVEN_HOME</span>:/root/.m2 -v ~/Work/third/incubator-doris:/root/incubator-doris apache/incubator-doris:build-env-1.2</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/incubator-doris</span><br><span class="line">sh build.sh --fe --be --clean</span><br></pre></td></tr></table></figure><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><h3 id="创建-MySQL-表"><a href="#创建-MySQL-表" class="headerlink" title="创建 MySQL 表"></a>创建 MySQL 表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`mysql_source`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line"> ) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span></span><br></pre></td></tr></table></figure><h3 id="创建-Doris-表"><a href="#创建-Doris-表" class="headerlink" title="创建 Doris 表"></a>创建 Doris 表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`doris_sink`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">100</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span></span><br><span class="line"> ) <span class="keyword">ENGINE</span>=OLAP</span><br><span class="line"> <span class="keyword">UNIQUE</span> <span class="keyword">KEY</span>(<span class="string">`id`</span>)</span><br><span class="line"> <span class="keyword">COMMENT</span> <span class="string">"OLAP"</span></span><br><span class="line"> <span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(<span class="string">`id`</span>) BUCKETS <span class="number">1</span></span><br><span class="line"> PROPERTIES (</span><br><span class="line"> <span class="string">"replication_num"</span> = <span class="string">"3"</span>,</span><br><span class="line"> <span class="string">"in_memory"</span> = <span class="string">"false"</span>,</span><br><span class="line"> <span class="string">"storage_format"</span> = <span class="string">"V2"</span></span><br><span class="line"> );</span><br></pre></td></tr></table></figure><h2 id="启动-Flink-Sql-Client"><a href="#启动-Flink-Sql-Client" class="headerlink" title="启动 Flink Sql Client"></a>启动 Flink Sql Client</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/sql-client.sh embedded</span><br><span class="line">&gt; <span class="built_in">set</span> execution.result-mode=tableau;</span><br></pre></td></tr></table></figure><h3 id="创建-Flink-CDC-Mysql-映射表"><a href="#创建-Flink-CDC-Mysql-映射表" class="headerlink" title="创建 Flink CDC Mysql 映射表"></a>创建 Flink CDC Mysql 映射表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mysql_source ( </span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">INT</span>, </span><br><span class="line">  <span class="keyword">name</span> <span class="keyword">STRING</span>,</span><br><span class="line">  primary <span class="keyword">key</span>(<span class="keyword">id</span>)  <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> ( </span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'mysql-cdc'</span>, </span><br><span class="line">  <span class="string">'hostname'</span> = <span class="string">'localhost'</span>, </span><br><span class="line">  <span class="string">'port'</span> = <span class="string">'3306'</span>, </span><br><span class="line">  <span class="string">'username'</span> = <span class="string">'root'</span>, </span><br><span class="line">  <span class="string">'password'</span> = <span class="string">'password'</span>, </span><br><span class="line">  <span class="string">'database-name'</span> = <span class="string">'demo'</span>, </span><br><span class="line">  <span class="string">'table-name'</span> = <span class="string">'mysql_source'</span> </span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="创建-Flink-Doris-Table-映射表"><a href="#创建-Flink-Doris-Table-映射表" class="headerlink" title="创建 Flink Doris Table 映射表"></a>创建 Flink Doris Table 映射表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> doris_sink (</span><br><span class="line">   <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">   <span class="keyword">name</span> <span class="keyword">STRING</span></span><br><span class="line">) </span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'doris'</span>,</span><br><span class="line">  <span class="string">'fenodes'</span> = <span class="string">'localhost:8030'</span>,</span><br><span class="line">  <span class="string">'table.identifier'</span> = <span class="string">'db_audit.doris_test'</span>,</span><br><span class="line">  <span class="string">'sink.batch.size'</span> = <span class="string">'2'</span>,</span><br><span class="line">  <span class="string">'sink.batch.interval'</span>=<span class="string">'1'</span>,</span><br><span class="line">  <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">  <span class="string">'password'</span> = <span class="string">''</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="将-MySQL-里的数据插入到-Doris-中"><a href="#将-MySQL-里的数据插入到-Doris-中" class="headerlink" title="将 MySQL 里的数据插入到 Doris 中"></a>将 MySQL 里的数据插入到 Doris 中</h3><ol><li><p>将 MySQL 里的数据通过 Flink CDC 结合 Doris Flink Connector 方式插入到 Doris 中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> doris_sink <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> test_flink_cdc;</span><br></pre></td></tr></table></figure></li><li><p>向 MySQL 表中插入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">123</span>, <span class="string">'this is a update'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">1212</span>, <span class="string">'测试flink CDC'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">1234</span>, <span class="string">'这是测试'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">11233</span>, <span class="string">'yunian_1'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">21233</span>, <span class="string">'yunian_2'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">31233</span>, <span class="string">'yunian_3'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">41233</span>, <span class="string">'yunian_4'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">51233</span>, <span class="string">'yunian_5'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">61233</span>, <span class="string">'yunian_6'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">71233</span>, <span class="string">'yunian_7'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">81233</span>, <span class="string">'yunian_8'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> mysql_source <span class="keyword">VALUES</span> (<span class="number">91233</span>, <span class="string">'yunian_8'</span>);</span><br></pre></td></tr></table></figure></li><li><p>观察 Doris 表的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> doris_sink;</span><br></pre></td></tr></table></figure></li><li><p>修改 MySQL 的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> mysql_source <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'这个是验证修改的操作'</span> <span class="keyword">where</span> <span class="keyword">id</span> =<span class="number">123</span>;</span><br></pre></td></tr></table></figure></li><li><p>删除数据操作<br>目前 Flink Doris Connector 还不支持删除操作。</p></li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://developer.aliyun.com/article/819607" target="_blank" rel="noopener">Flink CDC 系列 - 实现 MySQL 数据实时写入 Apache Doris</a><br><a href="https://segmentfault.com/a/1190000021523570?spm=a2c6h.12873639.0.0.1c143224F2b6gL" target="_blank" rel="noopener">Docker 安装 MySQL 8.0</a><br><a href="https://hf200012.github.io/2021/09/Apache-Doris-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/?spm=a2c6h.12873639.0.0.1c143224F2b6gL" target="_blank" rel="noopener">Apache Doris 环境安装部署</a><br><a href="https://www.cxyzjd.com/article/weixin_39408343/106035334" target="_blank" rel="noopener">flex从2.5.37升级至2.6.4</a><br><a href="https://doris.apache.org/master/zh-CN/installing/compilation.html" target="_blank" rel="noopener">Apache Doris官网快速入门</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MacOS M1 上，使用 Docker 构建  MySQL 和 Doris 上的 Streaming ETL，试用 Flink CDC 相关功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink用法-CDC-构建 MySQL 和 Postgres 上的 Streaming ETL</title>
    <link href="http://yoursite.com/2021/12/01/Flink%E7%94%A8%E6%B3%95-CDC-%E6%9E%84%E5%BB%BA-MySQL-%E5%92%8C-Postgres-%E4%B8%8A%E7%9A%84-Streaming-ETL/"/>
    <id>http://yoursite.com/2021/12/01/Flink用法-CDC-构建-MySQL-和-Postgres-上的-Streaming-ETL/</id>
    <published>2021-12-01T07:37:16.000Z</published>
    <updated>2021-12-05T08:49:24.710Z</updated>
    
    <content type="html"><![CDATA[<p>MacOS M1 上，使用 Docker 构建  MySQL 和 Postgres 上的 Streaming ETL，试用 Flink CDC 相关功能。</p><a id="more"></a><h2 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h2><h3 id="准备-docker-compose-yml-文件"><a href="#准备-docker-compose-yml-文件" class="headerlink" title="准备 docker-compose.yml 文件"></a>准备 docker-compose.yml 文件</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'2.1'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  postgres:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">debezium/example-postgres:1.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"5432:5432"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">POSTGRES_PASSWORD=1234</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">POSTGRES_DB=postgres</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">POSTGRES_USER=postgres</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">POSTGRES_PASSWORD=postgres</span></span><br><span class="line"><span class="attr">  mysql:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">debezium/example-mysql:1.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"3306:3306"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">MYSQL_ROOT_PASSWORD=123456</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">MYSQL_USER=mysqluser</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">MYSQL_PASSWORD=mysqlpw</span></span><br><span class="line"><span class="attr">  elasticsearch:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">docker.elastic.co/elasticsearch/elasticsearch:7.10.2</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">cluster.name=docker-cluster</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">bootstrap.memory_lock=true</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"ES_JAVA_OPTS=-Xms512m -Xmx512m"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">discovery.type=single-node</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9200:9200"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9300:9300"</span></span><br><span class="line"><span class="attr">    ulimits:</span></span><br><span class="line"><span class="attr">      memlock:</span></span><br><span class="line"><span class="attr">        soft:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">        hard:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">      nofile:</span></span><br><span class="line"><span class="attr">        soft:</span> <span class="number">65536</span></span><br><span class="line"><span class="attr">        hard:</span> <span class="number">65536</span></span><br><span class="line"><span class="attr">  kibana:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">elastic/kibana:7.10.2</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"5601:5601"</span></span><br></pre></td></tr></table></figure><p>Docker Compose 中包含的容器有：</p><ul><li>MySQL：商品表 products 和 订单表 orders 将存储在该数据库中， 这两张表将和 Postgres 数据库中的物流表 shipments 进行关联，得到一张包含更多信息的订单表 enriched_orders；</li><li>Postgres：物流表 shipments 将存储在该数据库中；</li><li>Elasticsearch：最终的订单表 enriched_orders 将写到 Elasticsearch；</li><li>Kibana：用来可视化 ElasticSearch 的数据。</li></ul><p>Docker Compose 启动命令，后台启动：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>查看 Docker 进程，也可以通过访问 <a href="http://localhost:5601/" target="_blank" rel="noopener">http://localhost:5601/</a> 来查看 Kibana 是否运行正常：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br><span class="line"></span><br><span class="line">CONTAINER ID   IMAGE                                                  COMMAND                  CREATED          STATUS          PORTS                                            NAMES</span><br><span class="line">79e04d74474b   elastic/kibana:7.10.2                                  <span class="string">"/usr/local/bin/dumb…"</span>   32 minutes ago   Up 32 minutes   0.0.0.0:5601-&gt;5601/tcp                           docker_kibana_1</span><br><span class="line">f7020940f3bc   debezium/example-mysql:1.1                             <span class="string">"docker-entrypoint.s…"</span>   32 minutes ago   Up 32 minutes   0.0.0.0:3306-&gt;3306/tcp, 33060/tcp                docker_mysql_1</span><br><span class="line">823b10a437a2   docker.elastic.co/elasticsearch/elasticsearch:7.10.2   <span class="string">"/tini -- /usr/local…"</span>   32 minutes ago   Up 32 minutes   0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp   docker_elasticsearch_1</span><br><span class="line">330a1dfd6571   debezium/example-postgres:1.1                          <span class="string">"docker-entrypoint.s…"</span>   32 minutes ago   Up 32 minutes   0.0.0.0:5432-&gt;5432/tcp                           docker_postgres_1</span><br></pre></td></tr></table></figure><p>查看某一个 Docker 进程的日志：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs docker_elasticsearch_1</span><br></pre></td></tr></table></figure><p>Docker Compose 停止命令：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose down</span><br></pre></td></tr></table></figure><p>注：Mac M1 请使用elasticsearch原生镜像，docker.elastic.co/elasticsearch/elasticsearch:7.10.2 ，否则启动不起来。<br><a href="https://stackoverflow.com/questions/65962810/m1-mac-issue-bringing-up-elasticsearch-cannot-run-jdk-bin-java" target="_blank" rel="noopener">解决问题的链接</a></p><h3 id="下载-Flink-1-13-2-和依赖的-jars"><a href="#下载-Flink-1-13-2-和依赖的-jars" class="headerlink" title="下载 Flink-1.13.2 和依赖的 jars"></a>下载 Flink-1.13.2 和依赖的 jars</h3><ol><li><p>下载 Flink-1.13.2 安装包，也可以自行下载源码进行编译：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -DskipTests -Dfast -T 4 -Dmaven.compile.fork=<span class="literal">true</span> -Dscala-2.11</span><br></pre></td></tr></table></figure></li><li><p>下载下面列出的依赖包，并将它们放到目录 flink-1.13.2/lib/ 下</p><ul><li><p><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch7_2.11/1.13.2/flink-sql-connector-elasticsearch7_2.11-1.13.2.jar?spm=a2c6h.12873639.0.0.648a1a36FkhAvX&amp;file=flink-sql-connector-elasticsearch7_2.11-1.13.2.jar" target="_blank" rel="noopener">flink-sql-connector-elasticsearch7_2.11-1.13.2.jar</a></p></li><li><p><a href="https://repo1.maven.org/maven2/com/ververica/flink-sql-connector-mysql-cdc/2.1.0/flink-sql-connector-mysql-cdc-2.1.0.jar" target="_blank" rel="noopener">flink-sql-connector-mysql-cdc-2.1.0.jar</a></p></li><li><p><a href="https://repo1.maven.org/maven2/com/ververica/flink-sql-connector-postgres-cdc/2.1.0/flink-sql-connector-postgres-cdc-2.1.0.jar" target="_blank" rel="noopener">flink-sql-connector-postgres-cdc-2.1.0.jar</a></p></li></ul></li></ol><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><h4 id="在-MySQL-数据库中准备数据"><a href="#在-MySQL-数据库中准备数据" class="headerlink" title="在 MySQL 数据库中准备数据"></a>在 MySQL 数据库中准备数据</h4><ol><li><p>进入到 docker-compose.yml 所在目录，执行如下命令进入 MySQL 容器：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose <span class="built_in">exec</span> mysql mysql -uroot -p123456</span><br></pre></td></tr></table></figure></li><li><p>创建数据库和表 products，orders，并插入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- MySQL</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> mydb;</span><br><span class="line"><span class="keyword">USE</span> mydb;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> products (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">INTEGER</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  description <span class="built_in">VARCHAR</span>(<span class="number">512</span>)</span><br><span class="line">);</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> products AUTO_INCREMENT = <span class="number">101</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> products</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="keyword">default</span>,<span class="string">"scooter"</span>,<span class="string">"Small 2-wheel scooter"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"car battery"</span>,<span class="string">"12V car battery"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"12-pack drill bits"</span>,<span class="string">"12-pack of drill bits with sizes ranging from #40 to #3"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"hammer"</span>,<span class="string">"12oz carpenter's hammer"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"hammer"</span>,<span class="string">"14oz carpenter's hammer"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"hammer"</span>,<span class="string">"16oz carpenter's hammer"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"rocks"</span>,<span class="string">"box of assorted rocks"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"jacket"</span>,<span class="string">"water resistent black wind breaker"</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">"spare tire"</span>,<span class="string">"24 inch spare tire"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders (</span><br><span class="line">  order_id <span class="built_in">INTEGER</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">  order_date DATETIME <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  customer_name <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">5</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  product_id <span class="built_in">INTEGER</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  order_status <span class="built_in">BOOLEAN</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="comment">-- Whether order has been placed</span></span><br><span class="line">) AUTO_INCREMENT = <span class="number">10001</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> orders</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="keyword">default</span>, <span class="string">'2020-07-30 10:08:22'</span>, <span class="string">'Jark'</span>, <span class="number">50.50</span>, <span class="number">102</span>, <span class="literal">false</span>),</span><br><span class="line">       (<span class="keyword">default</span>, <span class="string">'2020-07-30 10:11:09'</span>, <span class="string">'Sally'</span>, <span class="number">15.00</span>, <span class="number">105</span>, <span class="literal">false</span>),</span><br><span class="line">       (<span class="keyword">default</span>, <span class="string">'2020-07-30 12:00:30'</span>, <span class="string">'Edward'</span>, <span class="number">25.25</span>, <span class="number">106</span>, <span class="literal">false</span>);</span><br></pre></td></tr></table></figure></li></ol><h4 id="在-Postgres-数据库中准备数据"><a href="#在-Postgres-数据库中准备数据" class="headerlink" title="在 Postgres 数据库中准备数据"></a>在 Postgres 数据库中准备数据</h4><ol><li><p>进入到 docker-compose.yml 所在目录，执行如下命令进入 Postgre 容器：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose <span class="built_in">exec</span> postgres psql -h localhost -U postgres</span><br></pre></td></tr></table></figure></li><li><p>创建表 shipments，并插入3条物流数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- PG</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> shipments (</span><br><span class="line">   shipment_id <span class="built_in">SERIAL</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">   order_id <span class="built_in">SERIAL</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">   origin <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">   destination <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">   is_arrived <span class="built_in">BOOLEAN</span> <span class="keyword">NOT</span> <span class="literal">NULL</span></span><br><span class="line"> );</span><br><span class="line"> <span class="keyword">ALTER</span> <span class="keyword">SEQUENCE</span> public.shipments_shipment_id_seq RESTART <span class="keyword">WITH</span> <span class="number">1001</span>;</span><br><span class="line"> <span class="keyword">ALTER</span> <span class="keyword">TABLE</span> public.shipments REPLICA <span class="keyword">IDENTITY</span> <span class="keyword">FULL</span>;</span><br><span class="line"> <span class="keyword">INSERT</span> <span class="keyword">INTO</span> shipments</span><br><span class="line"> <span class="keyword">VALUES</span> (<span class="keyword">default</span>,<span class="number">10001</span>,<span class="string">'Beijing'</span>,<span class="string">'Shanghai'</span>,<span class="literal">false</span>),</span><br><span class="line">        (<span class="keyword">default</span>,<span class="number">10002</span>,<span class="string">'Hangzhou'</span>,<span class="string">'Shanghai'</span>,<span class="literal">false</span>),</span><br><span class="line">        (<span class="keyword">default</span>,<span class="number">10003</span>,<span class="string">'Shanghai'</span>,<span class="string">'Hangzhou'</span>,<span class="literal">false</span>);</span><br></pre></td></tr></table></figure></li></ol><h2 id="启动-Flink-集群和-Flink-SQL-CLI"><a href="#启动-Flink-集群和-Flink-SQL-CLI" class="headerlink" title="启动 Flink 集群和 Flink SQL CLI"></a>启动 Flink 集群和 Flink SQL CLI</h2><ol><li><p>进入 Flink 安装目录：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$FLINK_HOME</span></span><br></pre></td></tr></table></figure></li><li><p>启动 Flink 集群：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-cluster.sh</span><br></pre></td></tr></table></figure></li></ol><p>启动成功的话，可以在 <a href="http://localhost:8081/" target="_blank" rel="noopener">http://localhost:8081/</a> 访问到 Flink Web UI。</p><ol start="3"><li>启动 Flink SQL CLI<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/sql-client.sh</span><br></pre></td></tr></table></figure></li></ol><h2 id="在-Flink-SQL-CLI-使用-Flink-DDL-创建表"><a href="#在-Flink-SQL-CLI-使用-Flink-DDL-创建表" class="headerlink" title="在 Flink SQL CLI 使用 Flink DDL 创建表"></a>在 Flink SQL CLI 使用 Flink DDL 创建表</h2><ol><li><p>开启 checkpoint ，每隔3秒做一次 checkpoint</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> execution.checkpointing.interval = <span class="number">3</span>s;</span><br></pre></td></tr></table></figure></li><li><p>对于数据库中的表 products, orders, shipments，使用 Flink SQL CLI 创建对应的表，用于同步这些底层数据库表的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> products (    </span><br><span class="line"><span class="keyword">id</span> <span class="built_in">INT</span>,    </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">STRING</span>,    </span><br><span class="line">description <span class="keyword">STRING</span>,    </span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span>  </span><br><span class="line">) <span class="keyword">WITH</span> (    </span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'mysql-cdc'</span>,    </span><br><span class="line"><span class="string">'hostname'</span> = <span class="string">'localhost'</span>,    </span><br><span class="line"><span class="string">'port'</span> = <span class="string">'3306'</span>,    </span><br><span class="line"><span class="string">'username'</span> = <span class="string">'root'</span>,    </span><br><span class="line"><span class="string">'password'</span> = <span class="string">'123456'</span>,    </span><br><span class="line"><span class="string">'database-name'</span> = <span class="string">'mydb'</span>,    </span><br><span class="line"><span class="string">'table-name'</span> = <span class="string">'products'</span>  </span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders (   </span><br><span class="line">order_id <span class="built_in">INT</span>,   </span><br><span class="line">order_date <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>),   </span><br><span class="line">customer_name <span class="keyword">STRING</span>,   </span><br><span class="line">price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">5</span>),   </span><br><span class="line">product_id <span class="built_in">INT</span>,   </span><br><span class="line">order_status <span class="built_in">BOOLEAN</span>,   </span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (order_id) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span> </span><br><span class="line">) <span class="keyword">WITH</span> (   </span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'mysql-cdc'</span>,   </span><br><span class="line"><span class="string">'hostname'</span> = <span class="string">'localhost'</span>,   </span><br><span class="line"><span class="string">'port'</span> = <span class="string">'3306'</span>,   </span><br><span class="line"><span class="string">'username'</span> = <span class="string">'root'</span>,   </span><br><span class="line"><span class="string">'password'</span> = <span class="string">'123456'</span>,   </span><br><span class="line"><span class="string">'database-name'</span> = <span class="string">'mydb'</span>,   </span><br><span class="line"><span class="string">'table-name'</span> = <span class="string">'orders'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> shipments (   </span><br><span class="line">shipment_id <span class="built_in">INT</span>,   </span><br><span class="line">order_id <span class="built_in">INT</span>,   </span><br><span class="line">origin <span class="keyword">STRING</span>,   </span><br><span class="line">destination <span class="keyword">STRING</span>,   </span><br><span class="line">is_arrived <span class="built_in">BOOLEAN</span>,   </span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (shipment_id) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span> </span><br><span class="line">) <span class="keyword">WITH</span> (   </span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'postgres-cdc'</span>,   </span><br><span class="line"><span class="string">'hostname'</span> = <span class="string">'localhost'</span>,   </span><br><span class="line"><span class="string">'port'</span> = <span class="string">'5432'</span>,   </span><br><span class="line"><span class="string">'username'</span> = <span class="string">'postgres'</span>,   </span><br><span class="line"><span class="string">'password'</span> = <span class="string">'postgres'</span>,   </span><br><span class="line"><span class="string">'database-name'</span> = <span class="string">'postgres'</span>,   </span><br><span class="line"><span class="string">'schema-name'</span> = <span class="string">'public'</span>,   </span><br><span class="line"><span class="string">'table-name'</span> = <span class="string">'shipments'</span> </span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>创建 Elasticsearch 表 enriched_orders ，用来将关联后的订单数据写入 Elasticsearch 中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> enriched_orders (   </span><br><span class="line">order_id <span class="built_in">INT</span>,   </span><br><span class="line">order_date <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>),   </span><br><span class="line">customer_name <span class="keyword">STRING</span>,   </span><br><span class="line">price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">5</span>),   </span><br><span class="line">product_id <span class="built_in">INT</span>,   </span><br><span class="line">order_status <span class="built_in">BOOLEAN</span>,   </span><br><span class="line">product_name <span class="keyword">STRING</span>,   </span><br><span class="line">product_description <span class="keyword">STRING</span>,   </span><br><span class="line">shipment_id <span class="built_in">INT</span>,   </span><br><span class="line">origin <span class="keyword">STRING</span>,   </span><br><span class="line">destination <span class="keyword">STRING</span>,   </span><br><span class="line">is_arrived <span class="built_in">BOOLEAN</span>,   </span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (order_id) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span> </span><br><span class="line">) <span class="keyword">WITH</span> (     </span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'elasticsearch-7'</span>,     </span><br><span class="line"><span class="string">'hosts'</span> = <span class="string">'http://localhost:9200'</span>,     </span><br><span class="line"><span class="string">'index'</span> = <span class="string">'enriched_orders'</span> </span><br><span class="line">);</span><br></pre></td></tr></table></figure></li></ol><h2 id="关联订单数据并且将其写入-ES-中"><a href="#关联订单数据并且将其写入-ES-中" class="headerlink" title="关联订单数据并且将其写入 ES 中"></a>关联订单数据并且将其写入 ES 中</h2><p>使用 Flink SQL 将订单表 order 与 商品表 products，物流信息表 shipments 关联，并将关联后的订单信息写入 Elasticsearch 中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> enriched_orders </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">o.*, </span><br><span class="line">p.name, </span><br><span class="line">p.description, </span><br><span class="line">s.shipment_id, </span><br><span class="line">s.origin, </span><br><span class="line">s.destination, </span><br><span class="line">s.is_arrived </span><br><span class="line"><span class="keyword">FROM</span> orders <span class="keyword">AS</span> o </span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> </span><br><span class="line">products <span class="keyword">AS</span> p </span><br><span class="line"><span class="keyword">ON</span> o.product_id = p.id </span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> shipments <span class="keyword">AS</span> s </span><br><span class="line"><span class="keyword">ON</span> o.order_id = s.order_id;</span><br></pre></td></tr></table></figure><p>首先访问 <a href="http://localhost:5601/app/kibana#/management/kibana/index_pattern" target="_blank" rel="noopener"></a> 创建 index pattern enriched_orders：<br><img src="Kibana%E4%B8%AD%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95enriched_orders.png" alt></p><p>查看初始化写入的3条 enriched order 数据：<br><img src="Flink_ETL%E4%BB%BB%E5%8A%A1%E5%90%AF%E5%8A%A8%E4%B9%8B%E5%90%8E_%E5%88%9D%E5%A7%8B%E5%8C%96%E5%86%99%E5%85%A5Kibana%E4%B8%AD%E7%9A%843%E6%9D%A1%E6%95%B0%E6%8D%AE.png" alt></p><p>接下来，修改 MySQL 和 Postgre 数据库中的表数据，Kibana 中的订单数据也将实时更新：</p><ol><li>在 MySQL 的 orders 表中插入一条数据：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> orders <span class="keyword">VALUES</span> (<span class="keyword">default</span>, <span class="string">'2020-07-30 15:22:00'</span>, <span class="string">'Jark'</span>, <span class="number">29.71</span>, <span class="number">104</span>, <span class="literal">false</span>);</span><br></pre></td></tr></table></figure><p><img src="%E5%AE%9E%E6%97%B6%E6%8F%92%E5%85%A5%E4%B8%80%E6%9D%A1%E8%AE%A2%E5%8D%95%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%90%8E%E7%9A%84%E5%8F%98%E5%8C%96.png" alt></p><ol start="2"><li>在 Postgre 的 shipments 表中插入一条数据：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--PG</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> shipments <span class="keyword">VALUES</span> (<span class="keyword">default</span>,<span class="number">10004</span>,<span class="string">'Shanghai'</span>,<span class="string">'Beijing'</span>,<span class="literal">false</span>);</span><br></pre></td></tr></table></figure><p><img src="%E5%AE%9E%E6%97%B6%E6%8F%92%E5%85%A5%E4%B8%80%E6%9D%A1%E7%89%A9%E6%B5%81%E6%95%B0%E6%8D%AE%E5%90%8E%E7%9A%84%E5%8F%98%E5%8C%96.png" alt></p><ol start="3"><li>在 MySQL 的 orders 表中更新订单状态：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">UPDATE</span> orders <span class="keyword">SET</span> order_status = <span class="literal">true</span> <span class="keyword">WHERE</span> order_id = <span class="number">10004</span>;</span><br></pre></td></tr></table></figure><p><img src="%E5%AE%9E%E6%97%B6%E6%9B%B4%E6%96%B0%E8%AE%A2%E5%8D%95%E7%8A%B6%E6%80%81%E5%90%8E%E7%9A%84%E5%8F%98%E5%8C%96.png" alt></p><ol start="4"><li>在 Postgre 的 shipments 表中更新物流状态：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--PG</span></span><br><span class="line"><span class="keyword">UPDATE</span> shipments <span class="keyword">SET</span> is_arrived = <span class="literal">true</span> <span class="keyword">WHERE</span> shipment_id = <span class="number">1004</span>;</span><br></pre></td></tr></table></figure><p><img src="%E5%AE%9E%E6%97%B6%E6%9B%B4%E6%96%B0%E7%89%A9%E6%B5%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%9A%84%E5%8F%98%E5%8C%96.png" alt></p><ol start="5"><li>在 MySQL 的 orders 表中删除一条数据：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> orders <span class="keyword">WHERE</span> order_id = <span class="number">10004</span>;</span><br></pre></td></tr></table></figure><p><img src="%E5%AE%9E%E6%97%B6%E6%9B%B4%E6%96%B0%E7%89%A9%E6%B5%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%9A%84%E5%8F%98%E5%8C%96.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://developer.aliyun.com/article/815226?groupCode=sc&accounttraceid=c336dbdddec54b029ca0513e1ce6a81aanxb#slide-1" target="_blank" rel="noopener">Flink CDC 系列 - 构建 MySQL 和 Postgres 上的 Streaming ETL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MacOS M1 上，使用 Docker 构建  MySQL 和 Postgres 上的 Streaming ETL，试用 Flink CDC 相关功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink源码剖析-flink-streaming-java_ProcessFunction</title>
    <link href="http://yoursite.com/2021/07/05/Flink%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-flink-streaming-java-ProcessFunction/"/>
    <id>http://yoursite.com/2021/07/05/Flink源码剖析-flink-streaming-java-ProcessFunction/</id>
    <published>2021-07-05T11:05:08.000Z</published>
    <updated>2021-09-02T14:27:36.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="01153083c6d8698f9a02df416d4b0aa8d48e8a397e1f373c7996a8f9302865a7">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Here&#39;s something encrypted, password is required to continue reading.
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
  </entry>
  
  <entry>
    <title>Apache Pulsar</title>
    <link href="http://yoursite.com/2021/05/25/Apache-Pulsar/"/>
    <id>http://yoursite.com/2021/05/25/Apache-Pulsar/</id>
    <published>2021-05-25T05:09:47.000Z</published>
    <updated>2021-06-14T12:05:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文为 Apache Pulsar 的入门学习笔记。 </p><p>企业在考虑部署实时消息系统时，总体硬件成本很重要的。Kafka 是基于磁盘存储数据的，存储成本较高。<br>而 Pulsar Broker 不直接存储数据，而是使用 Apache BookKeeper 来存储数据。数据发送/接收和存储的解耦使得 BookKeeper 可以在运行在独立的物理计算机或容器上。</p><p>当新生事物出现有两种角度去观察它，要么把它看小，要么把它放大。对 Apache Pulsar，把它看小的角度通常是”Apache Pulsar 只是一个新的消息队列而已”，或者“Apache Pulsar 只是一个新的数据管道而已”，“队列系统早就有了，只是 Apache Pulsar 更具扩展性也能解决某些场景问题而已，基本没啥本质区别”。很明显上述认识都不对，Apache Pulsar 在消息、流、数据管理和技术基础设施层面都有技术演进，详看正文。</p><a id="more"></a><h2 id="Pulsar-兴起的原因"><a href="#Pulsar-兴起的原因" class="headerlink" title="Pulsar 兴起的原因"></a>Pulsar 兴起的原因</h2><p>消息队列的发展史：<br><img src="%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%8F%91%E5%B1%95%E5%8F%B2.png" alt></p><p>在 pulsar 之前，消息系统主要有两个应用场景：</p><ul><li>应用之间的解耦缓冲，出现了大量的MQ。</li><li>随着流计算的兴起，Kafka 被大量使用。   </li></ul><p><img src="pulsar%E4%B9%8B%E5%89%8D.png" alt></p><p>未来实时技术设施建设通常有以下3大趋势：</p><ul><li>消息系统的融合</li><li>批流存储融合</li><li>云原生</li></ul><h3 id="Converged-Messaging-消息系统的融合"><a href="#Converged-Messaging-消息系统的融合" class="headerlink" title="Converged Messaging 消息系统的融合"></a>Converged Messaging 消息系统的融合</h3><ul><li>Queuing 模式<br>应用系统消息处理；<br>系统间异步通信；<br>常用的有 RocketMQ、RabbitMQ、AMQP、JMS；</li></ul><ul><li>Streaming 模式<br>流通常用在数据流水线里，数据量较大，高吞吐量；<br>常用的有 Kafka、Kinesis；</li></ul><ul><li>Queuing vs Streaming</li></ul><p>队列和流通常用于不同的应用场景下。<br><img src="Queuing_vs_Streaming.png" alt></p><ul><li>Queuing + Streaming</li></ul><p>队列和流本身是做不到完全隔离的，队列中的数据通常需要通过流导入到数仓中。</p><p><img src="Queuing+Streaming.png" alt></p><p>一个业务系统通常需要使用多个消息系统，举例如下：</p><p>刚开始做业务的时候，创建了两个微服务，如果两个微服务之间需要通信，就会拉一个消息队列（RabbitMQ、ActiveMQ等）进来。随着业务的增长，需要接入一些 IoT 设备，使用 MQTT broker 去查一些 IoT 设备。此时业务需要对 IoT 设备进行流计算计数等处理，此时就会引入 Kafka，所有数据通过 Kafka Connector 集中汇聚到 Kafka，做集中的流计算处理。同时流计算的处理结果，可能需要写回 MQTT 或者其他业务系统。</p><p><img src="%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%9C%A8%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F.png" alt></p><p>此时调用链路和数据存储方式是比较混乱的，业务人员可能不知道从哪里取数：</p><p><img src="%E5%A4%9A%E7%A7%8D%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AF%BC%E8%87%B4%E7%B3%BB%E7%BB%9F%E4%B9%8B%E9%97%B4%E4%B8%80%E5%9B%A2%E4%B9%B1%E9%BA%BB.png" alt></p><p>Pulsar = Queues + Streams 的融合：</p><p><img src="Pulsar=Queues+Streams%E7%9A%84%E8%9E%8D%E5%90%88.png" alt></p><p>据调研，有42%的用户在考虑用 Pulsar 替换业务中使用的2种及以上的消息队列。</p><h3 id="Unified-Batch-and-Event-Stream-Storage-批流存储融合"><a href="#Unified-Batch-and-Event-Stream-Storage-批流存储融合" class="headerlink" title="Unified Batch and Event-Stream Storage 批流存储融合"></a>Unified Batch and Event-Stream Storage 批流存储融合</h3><p>传统的数据库处理，就是不停的插入数据，再用 SQL 批量查询出数据：</p><p><img src="%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8.png" alt></p><p>举一个快递员送餐的例子，用户一旦下单，他最关心的是餐什么时候能送到？<br>不仅要看当前的实时位置信息，还需要结合历史经验数据中天气、交通等因素对送餐速度的影响：</p><p><img src="%E5%BF%AB%E9%80%92%E5%91%98%E9%80%81%E9%A4%90%E6%A1%88%E4%BE%8B_%E5%AE%9E%E6%97%B6%E4%BD%8D%E7%BD%AE.png" alt></p><p><img src="%E5%BF%AB%E9%80%92%E5%91%98%E9%80%81%E9%A4%90%E6%A1%88%E4%BE%8B_%E5%AE%9E%E6%97%B6%E4%BD%8D%E7%BD%AE_%E7%BB%93%E5%90%88%E5%8E%86%E5%8F%B2%E7%BB%8F%E9%AA%8C%E6%95%B0%E6%8D%AE.png" alt></p><p>Pulsar = 批 + 流 的存储融合：</p><p><img src="Pulsar%E7%9A%84%E6%89%B9%E6%B5%81%E5%AD%98%E5%82%A8%E8%9E%8D%E5%90%88%E5%AE%9E%E7%8E%B0.png" alt></p><h3 id="Cloud-Native-Operations-以云原生的方式部署和运维"><a href="#Cloud-Native-Operations-以云原生的方式部署和运维" class="headerlink" title="Cloud-Native Operations 以云原生的方式部署和运维"></a>Cloud-Native Operations 以云原生的方式部署和运维</h3><p>Cloud Native Computing Foundation，云原生计算基金会。<br><code>Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid cloud. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.</code></p><p><img src="%E4%BA%91%E5%8E%9F%E7%94%9F%E7%B3%BB%E7%BB%9F%E6%A6%82%E5%BF%B5.png" alt></p><ul><li>Kubernets ，应该是跑在 k8s</li><li>Infinite，可以支持无限扩容</li><li>Serverless，业务服务是无状态的，可随时扩展</li><li>Multi Tenant，支持多租户</li><li>Geo Replicated，支持运行在不同的云厂商上，并且可以支持在不同的云厂商之间复制服务</li><li>Elastic</li><li>Secure and reliable，协议和数据存储应该是加密的，端到端的传输也是加密的</li><li>API-driven operations，透出统一的 API 服务</li></ul><p>Pulsar 的 Hot data、Warm data、Cold data 存储分离：<br><img src="data_tiering_and_io_isolation.png" alt></p><p>据调研，有50%的用户倾向于部署在 k8s/Cloud。</p><h2 id="Apache-Pulsar-简介"><a href="#Apache-Pulsar-简介" class="headerlink" title="Apache Pulsar 简介"></a>Apache Pulsar 简介</h2><p>综上，Pulsar 可以很容易的运行在 k8s 上，同时能集成多种云厂商作为其层级存储。Pulsar 提供了队列和流的统一模型，同时通过 Protocol Handler 的方式支持了其他消息系统的协议，如 Kafka Clients、AMQP Clients、MQTT Clients。这些都使得 Pulsar 完美的成为消息、流、存储融合的解决方案。<br><img src="pulsar%E7%9A%84%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt></p><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul><li><p>计算与存储分离<br>Pulsar 的 Producer 和 Consumer 都与 broker 相连接，broker 作为无状态服务，可以横向扩缩容，扩缩容时不会影响数据的整体生产和消费；broker 不存储数据，数据存储在 broker 的下一层，即 bookie 中，实现了计算与存储分离。<br><img src="pulsar%E7%9A%84%E4%BA%91%E5%8E%9F%E7%94%9F%E6%9E%B6%E6%9E%84.png" alt></p></li><li><p>节点对等，独立扩展，弹性水平扩容<br>对于云端产品而言，Pulsar 无需重平衡即可实现 broker 扩缩容。相比之下，Kafka 扩缩容前需要先进行重平衡操作，可能会导致集群负载较高，也会对整体服务产生影响。其次，Pulsar topic 分区也可以实现无限扩容，扩容之后，通过负载均衡策略自动平衡整个分片和流量。</p></li><li><p>Pulsar 多租户<br>Pulsar 原生支持多租户。在日志服务中也有租户的概念，每一条产品线属于一个租户，实现了产品线之间的数据隔离。Pulsar 集群支持数百万个 topic，整个 topic 也通过租户实现了隔离，在租户级别，Pulsar 实现了存储配额、消息过期删除、隔离策略等优越特性，且支持单独的认证和授权机制。<br><img src="pulsar%E6%94%AF%E6%8C%81%E5%A4%9A%E7%A7%9F%E6%88%B7.png" alt></p></li><li><p>负载均衡策略<br>Pulsar 在命名空间级别有 bundle 的概念，如果当前 broker 负载较高，bundle 会通过管理 topic 分区策略进行 bundle split 的操作，自动将子分区均衡到其他负载较低的 broker 上。在创建 topic 时，Pulsar 也会自动把 topic 优先分配到当前负载较低的 broker 上。</p></li></ul><ul><li><p>Pulsar IO 模型<br>写入操作中，broker 并发向 BookKeeper 写入数据；当 bookie 向 broker 反馈数据写入成功时，在 broker 层面，内部只维护一个队列。如果当前的消费模式是实时消费，则可以直接从 broker 获取数据，无需经过 bookie 查询，从而提升消息吞吐量。在 append 读场景中，查询历史数据才需要查询 bookie； append 读还支持数据卸载功能，即将数据卸载到其他存储介质中（比如 HDFS），实现冷存历史数据。</p></li><li><p>Topic 创建、生产与消费<br>在控制台创建 topic 后，将 topic 信息和租户信息记录到 etcd 和 MySQL 中，producer 和 consumer 两类服务会监听 etcd。producer 类服务，监听创建或删除 topic 的内部操作。consumer 服务监听到创建 topic 操作后，对应的服务会连接到 Pulsar topic，实时消费 topic 上的数据。 producer 开始接收数据，并判断应该向哪一个 topic 写入数据，consumer 消费数据并在判断后写入，或转存再写入到 ES 等其他存储中。</p></li><li><p>Topic 逻辑抽象<br>Pulsar 有3个级别：topic、命名空间和租户。在日志服务中，topic 对应 Pulsar 逻辑上的分片，命名空间对应 Pulsar 逻辑上的 topic。通过将整体概念往上提一层，实现了两个功能，一是动态增加和减少分片数量，二是在后台启动的 Flink 任务可以消费单个项目级别的数据。<br><img src="Pulsar%E7%9A%84topic%E9%80%BB%E8%BE%91%E6%8A%BD%E8%B1%A1.png" alt></p></li><li><p>消息订阅模型<br>Pulsar 提供四种消息订阅模型：</p><ol><li>独占模式（Exclusive）：当有多个 consumer 使用同一个订阅名称订阅 topic 时，只有一个 consumer 可以消费数据。</li><li>故障转移模式（Failover）：当多个 consumer 通过同一个订阅名称订阅 topic 时，如果某一个 consumer 出现故障或连接中断，Pulsar 会自动切换到下一个 consumer，实现单点消费。</li><li>共享模式（Shared）：应用比较广泛的一个模型，如果启动多个 consumer，但只通过一个订阅者订阅 topic 信息，Pulsar 会通过轮询方式依次向 consumer 发送数据；如果某一个 consumer 宕机或连接中断，则消息会被轮询到下一个 consumer 中。LogHub 使用的就是共享订阅模型，整个 Hub 运行在容器中，可以根据整体负载或其他指标动态扩缩容消费端。</li><li>Keyed_Shared 消息订阅模式：通过 Key Hash 方式保持数据消费的一致性。</li></ol></li><li><p>Broker 故障恢复<br>由于 broker 无状态，所以某一个 broker 宕机对整体的生产和消费没有任何影响，同时会有一个新的 broker 担任 owner 的角色，从 Zookeeper 中获取 topic 元数据，并自动演进到新 owner 中，数据的存储层也不会发生变化。此外，无需拷贝 topic 内的数据，避免数据冗余。</p></li><li><p>Bookie 故障恢复<br>bookie 层使用分片存储信息。由于 bookie 本身有多副本机制，当某个 bookie 出现故障时，系统会从其他 bookie 读取对应分片的信息，并进行重平衡，因此整个 bookie 的写入不会受到影响，保证整个 topic 的可用性。</p></li><li><p>原生跨地域复制<br><img src="pulsar%E6%94%AF%E6%8C%81%E5%8E%9F%E7%94%9F%E8%B7%A8%E5%9C%B0%E5%9F%9F%E5%A4%8D%E5%88%B6.png" alt></p></li></ul><h3 id="数据视图"><a href="#数据视图" class="headerlink" title="数据视图"></a>数据视图</h3><p>pulsar视图之topic_partiiton分区：</p><p><img src="pulsar%E6%95%B0%E6%8D%AE%E8%A7%86%E5%9B%BE_topic_partition%E5%88%86%E5%8C%BA.png" alt></p><p>pulsar视图之partiiton_segment分片：</p><p><img src="pulsar%E6%95%B0%E6%8D%AE%E8%A7%86%E5%9B%BE_partition_segment%E5%88%86%E7%89%87.png" alt></p><p>pulsar计算历史数据：</p><ul><li>获取 Segments 列表</li><li>选择需要扫描的 Segments</li><li>使用 Segments reader 直接访问 Segments</li></ul><p><img src="pulsar%E8%AE%A1%E7%AE%97%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE.png" alt></p><p>pulsar计算实时数据：</p><ul><li>使用 Pulsar broker 的接口：Pub/Sub API </li></ul><p><img src="pulsar%E8%AE%A1%E7%AE%97%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE.png" alt></p><h3 id="功能完整性"><a href="#功能完整性" class="headerlink" title="功能完整性"></a>功能完整性</h3><p>Pulsar 是一个完整的消息和流融合的产品：</p><p><img src="pulsar%E7%9A%84%E5%AE%8C%E6%95%B4%E6%80%A7.png" alt></p><h3 id="高吞吐量"><a href="#高吞吐量" class="headerlink" title="高吞吐量"></a>高吞吐量</h3><p><img src="pulsar%E9%AB%98%E5%90%9E%E5%90%90.png" alt></p><h3 id="稳定的低延时"><a href="#稳定的低延时" class="headerlink" title="稳定的低延时"></a>稳定的低延时</h3><p>pulsar 的时延基本稳定在 5～10ms 之间，Kafka 随着partitions数据的增多，时延会升高。<br><img src="pulsar%E7%A8%B3%E5%AE%9A%E7%9A%84%E4%BD%8E%E5%BB%B6%E6%97%B6%E7%89%B9%E6%80%A7.png" alt></p><h3 id="生态社区支持"><a href="#生态社区支持" class="headerlink" title="生态社区支持"></a>生态社区支持</h3><p><img src="pulsar%E7%9A%84%E7%94%9F%E6%80%81%E6%94%AF%E6%8C%81.png" alt></p><h3 id="Cloud-offerings"><a href="#Cloud-offerings" class="headerlink" title="Cloud offerings"></a>Cloud offerings</h3><p>StreamNative Cloud<br>Apache Pulsar as a Service</p><p>除了阿里云，还会部署到 Google Cloud、Microsoft Azure、亚马逊、腾讯云等。</p><h3 id="Roadmap"><a href="#Roadmap" class="headerlink" title="Roadmap"></a>Roadmap</h3><ul><li>Pulsar Transaction (2.7.0 developer preview)，2.7.2 中已支持</li><li>Function Mesh</li><li>Auto-Scaling Topic Partitions</li><li>Pulsar No-keeper</li><li>Columnar Tiered Storage</li></ul><h2 id="Pulsar-vs-Kafka-vs-RocketMQ"><a href="#Pulsar-vs-Kafka-vs-RocketMQ" class="headerlink" title="Pulsar vs Kafka vs RocketMQ"></a>Pulsar vs Kafka vs RocketMQ</h2><table>    <tr>        <td>分类</td>        <td>对比项</td>        <td>RocketMQ</td>        <td>Kafka</td>        <td>Pulsar</td>    </tr>    <tr>        <td>定位</td>        <td></td>        <td>轻量级数据处理平台</td>        <td>分布式事件流平台</td>        <td>云原生的分布式数据处理平台</td>    </tr>    <tr>        <td rowspan="11">基础功能对比</td>        <td>消费模式</td>        <td>推、拉</td>        <td>拉</td>        <td>推</td>    </tr>    <tr>        <td>订阅模式</td>        <td>点对点，发布订阅</td>        <td>发布订阅</td>        <td>发布订阅，点对点</td>    </tr>    <tr>        <td>消息存储</td>        <td>支持，较高</td>        <td>支持，较高</td>        <td>支持，较高</td>    </tr>    <tr>        <td>多租户</td>        <td>不支持</td>        <td>不支持</td>        <td>支持</td>    </tr>    <tr>        <td>写入性能</td>        <td>好</td>        <td>非常好</td>        <td>非常好</td>    </tr>    <tr>        <td>消费性能</td>        <td>好</td>        <td>非常好</td>        <td>非常好</td>    </tr>    <tr>        <td>稳定性</td>        <td>好</td>        <td>分区过多或扩容时，写入性能下降</td>        <td>分区较多时，性能稳定</td>    </tr>    <tr>        <td>支持topic数量</td>        <td>单机5万队列，支撑较好</td>        <td>单机超过 60+ topic，负载升高</td>        <td>5万topic，性能稳定</td>    </tr>    <tr>        <td>消息优先级</td>        <td>支持</td>        <td>不支持</td>        <td>不支持</td>    </tr>    <tr>        <td>死信队列</td>        <td>支持</td>        <td>不支持</td>        <td>支持</td>    </tr>    <tr>        <td>消息TTL</td>        <td>支持</td>        <td>支持</td>        <td>支持</td>    </tr>    <tr>        <td>可靠性对比</td>        <td>可靠性</td>        <td>很好</td>        <td>很好</td>        <td>很好</td>    </tr>    <tr>        <td rowspan="2">优缺点对比</td>        <td>优点</td>        <td>            高吞吐量，低延迟;<br>            支持顺序消息，事务等，功能较全;<br>            不受分区限制，水平扩展能力强;        </td>        <td>            高吞吐量，低延迟，高可用，高容错;<br>            生态较好，大数据领域使用较广;<br>        </td>        <td>            高吞吐量，低延迟，高可靠，高容错;<br>            计算存储分离，水平扩展和不需要重平衡;<br>            支持的 topic 分区数可达百万级;        </td>    </tr>    <tr>        <td>缺点</td>        <td>            吞吐量不如Kafka;<br>            不支持 master 主动切换，客户端只支持java;        </td>        <td>            集群消费受分区数目限制;<br>            单机 partition 过多，性能下降明显;<br>            重平衡对生产运行影响较大;        </td>        <td>            使用案例较少;<br>            社区文档不成熟;        </td>    </tr></table><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ol><li>Pulsar 在日志服务中的应用</li></ol><p>日志服务系统的最底层是数据采集工具，我们基于开源的数据采集工具（如 Logstash、Flume、Beats）进行了定制化开发。数据存储中日志池是一个逻辑概念，对应于 Pulsar 中的 topic。日志服务系统的上层为查询分析和业务应用，查询分析指在日志服务的工作台进行检索分析，或通过 SQL 语法进行查询；业务应用指在控制台定制仪表盘和图表，实现实时告警等。<br>查询分析和业务应用都支持数据转存，即把日志数据转存到存储介质或价格较低的存储设备中，如基于 KS3 的对象存储、ElasticSearch 集群或 Kafka。</p><p><img src="Pulsar%E5%9C%A8%E6%97%A5%E5%BF%97%E6%9C%8D%E5%8A%A1%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.png" alt></p><ol start="2"><li><p>计费平台、支付平台、交易系统</p></li><li><p>Worker Queue / Push Notifications /  Task Queue</p></li><li><p>Unified Messaging Backbone (Queuing + Streaming)</p></li><li><p>IoT 车联网、物联网领域</p></li><li><p>Unified Data Processing - Flink</p></li></ol><h2 id="Flink-和-Pulsar-的批流融合"><a href="#Flink-和-Pulsar-的批流融合" class="headerlink" title="Flink 和 Pulsar 的批流融合"></a>Flink 和 Pulsar 的批流融合</h2><p>Flink 角色：</p><ul><li>对批流处理提供统一的模型和 API</li><li>处理大规模的历史数据和低延迟的实时数据</li></ul><p>Pulsar 角色：</p><ul><li>提供对批流数据的统一存储视图</li><li>通过层级存储提供无限的流存储</li><li>提供统一的消费模型</li></ul><p>pulsar flink connector 目前还没有合并到 flink 仓库，维护在单独的仓库中：<a href="https://github.com/streamnative/pulsar-flink" target="_blank" rel="noopener">pulsar-flink github地址</a></p><ul><li>Pulsar Schema<br>第一种是常见的消息元数据，包括消息的key、消息产生时间、或其他元数据的信息。Primitive Schema:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- value: double (nullable = false)</span><br><span class="line">|-- __key: binary (nullable = true)</span><br><span class="line">|-- __topic: string (nullable = true)</span><br><span class="line">|-- __messageId: binary (nullable = true)</span><br><span class="line">|-- __publishTime: timestamp (nullable = true)</span><br><span class="line">|-- __eventTime: timestamp (nullable = true)</span><br></pre></td></tr></table></figure><p>第二种是对消息的内容的数据结构的描述，常见的是 Avro 格式，用户访问的时候就可以通过 Schema 知道每个消息对应的数据结构。Avro Schema:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params">i: <span class="type">Int</span>, f: <span class="type">Float</span>, bar: <span class="type">Bar</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Bar</span>(<span class="params">b: <span class="type">Boolean</span>, s: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">s</span> </span>= <span class="type">Schema</span>.<span class="type">AVRO</span>(<span class="type">Foo</span>.getClass)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- i: integer (nullable = false)</span><br><span class="line">|-- f: float (nullable = false)</span><br><span class="line">|-- bar: struct (nullable = true)</span><br><span class="line">|    |-- b: boolean (nullable = false)</span><br><span class="line">|    |-- s: string (nullable = false)</span><br><span class="line">|-- __key: binary (nullable = true)</span><br><span class="line">|-- __topic: string (nullable = true)</span><br><span class="line">|-- __messageId: binary (nullable = true)</span><br><span class="line">|-- __publishTime: timestamp (nullable = true)</span><br><span class="line">|-- __eventTime: timestamp (nullable = true)</span><br></pre></td></tr></table></figure><ul><li>Pulsar Source<br>有了 Schema ，就可以把它作为一个 source：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.setProperty(<span class="string">"topic"</span>,<span class="string">"test-source-topic"</span>);</span><br><span class="line">props.setProperty(<span class="string">"partitiondiscoveryintervalmillis"</span>,<span class="string">"5000"</span>);</span><br><span class="line"></span><br><span class="line">FlinkPulsarSource&lt;String&gt; source =</span><br><span class="line"><span class="keyword">new</span> FlinkPulsarSource&lt;&gt;(serviceUrl, adminUrl, <span class="keyword">new</span> SimpleStringSchema(), props);</span><br><span class="line"></span><br><span class="line"><span class="comment">// or setStartFromLatest, setStartFromSpecificOffsets, setStartFromSubscription</span></span><br><span class="line">source.setStartFromEarliest();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; stream = see.addSource(source);</span><br><span class="line"></span><br><span class="line">see.execute();</span><br></pre></td></tr></table></figure><ul><li>Pulsar Sink<br>也可以把 Flink 计算的结果返回给 Pulsar：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FlinkPulsarSink&lt;Person&gt; sink = <span class="keyword">new</span> FlinkPulsarSink(</span><br><span class="line">serviceUrl,</span><br><span class="line">adminUrl,</span><br><span class="line">Optional.of(topic),</span><br><span class="line">props,</span><br><span class="line">TopicKeyExtractor.NULL,</span><br><span class="line">Person.class,</span><br><span class="line">RecordSchemaType.AVRO</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">stream.addSink(sink);</span><br></pre></td></tr></table></figure><ul><li>Metadata</li></ul><table><thead><tr><th>元数据</th><th>数据类型</th><th>描述</th><th>R/W</th></tr></thead><tbody><tr><td>topic</td><td>STRING NOT NULL</td><td>Pulsar 消息所在的 topic 名称</td><td>R ｜</td></tr><tr><td>messageId</td><td>BYTES NOT NULL</td><td>Pulsar 消息Id</td><td>R ｜</td></tr><tr><td>sequenceId</td><td>BIGINT NOT NULL</td><td>Pulsar 消息的序列号</td><td>R ｜</td></tr><tr><td>publishTime</td><td>TIMESTAMP(3) WITH TIME ZONE NOT NULL</td><td>Pulsar 消息的发布时间</td><td>R ｜</td></tr><tr><td>eventTime</td><td>TIMESTAMP(3) WITH TIME ZONE NOT NULL</td><td>Pulsar 消息的生成时间</td><td>R/W ｜</td></tr><tr><td>properties</td><td>Map&lt;STRING,STRING&gt; NOT NULL</td><td>Pulsar 消息的扩展信息</td><td>R/W ｜</td></tr></tbody></table><ul><li>Stream Tables<br>有了 source 和 sink 的支持，就自然能支持 table。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">see.setParallelism(<span class="number">1</span>);</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(see);</span><br><span class="line"></span><br><span class="line">String topic  = <span class="string">""</span>;</span><br><span class="line">String topicName = <span class="string">"pulsarTable"</span>;</span><br><span class="line"></span><br><span class="line">TableSchame tSchema = TableSchame.builder().field(<span class="string">"value"</span>, DataTypes.BOOLEAN()).build();</span><br><span class="line"></span><br><span class="line">tEnv.connect(</span><br><span class="line"><span class="keyword">new</span> Pulsar()</span><br><span class="line">.urls(getServiceUrl(), getAdminUrl())</span><br><span class="line">.topic(topic)</span><br><span class="line">.setStartFromEarliest()</span><br><span class="line">.property(PulsarOptions.PARTITION_DISCOVERY_INTERVAL_MS_OPTION_KEY, <span class="string">"5000"</span>))</span><br><span class="line">     .withSchema(<span class="keyword">new</span> Schema().schema(tSchema))</span><br><span class="line">     .inAppendMode()</span><br><span class="line">     .createTemporaryTable(tableName);</span><br><span class="line"></span><br><span class="line">Table t = tEnv.sqlQuery(<span class="string">"select `value` from "</span> + tableName);</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_pulsar (</span><br><span class="line">...</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span>=<span class="string">'pulsar'</span>,</span><br><span class="line"><span class="string">'topic'</span>=<span class="string">'persist://public/default/topic123'</span>,</span><br><span class="line"><span class="string">'key.format'</span>=<span class="string">'raw'</span>,</span><br><span class="line"><span class="string">'key.fields'</span>=<span class="string">'key'</span>,</span><br><span class="line"><span class="string">'value.format'</span>=<span class="string">'avro'</span>,</span><br><span class="line"><span class="string">'service-url'</span>=<span class="string">'pulsar://localhost:6650'</span>,</span><br><span class="line"><span class="string">'admin-url'</span>=<span class="string">'http://localhost:8080'</span>,</span><br><span class="line"><span class="string">'scan.startup.mode'</span>=<span class="string">'earliest'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li>Pulsar Catalog</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">catalog:</span></span><br><span class="line"><span class="attr">- name:</span> <span class="string">pulsarcatalog</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">pulsar</span></span><br><span class="line"><span class="attr">    default-database:</span> <span class="string">tn/ns</span></span><br><span class="line">    <span class="string">service.url:</span>  <span class="string">"pulsar://localhost:6650"</span></span><br><span class="line">    <span class="string">admin.url:</span> <span class="string">"http://localhost:8080"</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.useCatalog(<span class="string">"pulsarcatalog"</span>);</span><br><span class="line">tableEnv.useDatabase<span class="string">"public/default"</span>);</span><br><span class="line">tableEnv.scan<span class="string">"topic0"</span>);</span><br></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; USE CATALOG pulsarcatalog;</span><br><span class="line">Flink SQL&gt; USE `public/default`;</span><br><span class="line">Flink SQL&gt; SELECT * FROM topic0;</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://pulsar.apache.org/" target="_blank" rel="noopener">Pulsar 官网</a><br><a href="http://pulsar.apache.org/en/powered-by/" target="_blank" rel="noopener">Pulsar 社区</a><br><a href="https://segmentfault.com/a/1190000023087253" target="_blank" rel="noopener">Apache Pulsar 分层存储帮你省钱</a><br><a href="https://developer.aliyun.com/article/784282?spm=a2c6h.13148508.0.0.6dce4f0eQtZ5Kx" target="_blank" rel="noopener">Flink 和 Pulsar 的批流融合 文章</a><br><a href="https://www.bilibili.com/video/BV11X4y1P7PD?from=search&seid=4562849170423649332" target="_blank" rel="noopener">Flink 和 Pulsar 的批流融合 视频</a><br><a href="https://www.bilibili.com/video/BV12U4y1b7Tr?from=search&seid=4562849170423649332" target="_blank" rel="noopener">Apache Pulsar 在腾讯大数据场景下的落地实践 视频</a><br><a href="https://www.bilibili.com/video/BV1da4y1p7pv?from=search&seid=4562849170423649332" target="_blank" rel="noopener">Pulsar Summit Asia 2020 主题演讲：大融合：消息、流、存储三位一体（郭斯杰）</a><br><a href="https://www.bilibili.com/video/BV1HK4y1S7aF?from=search&seid=4562849170423649332" target="_blank" rel="noopener">Apache Pulsar 消息、流和存储的融合 视频</a><br><a href="https://mp.weixin.qq.com/s/N7JJYLv8p-dzgZfpkA1CEg" target="_blank" rel="noopener">RocketMQ 淘汰倒计时！这个新一代消息中间件，腾讯、华为都用疯了？</a><br><a href="https://mp.weixin.qq.com/s/84HsT9SFlBF7EZhiQBj-Rw" target="_blank" rel="noopener">为了处理日均TB级数据量，金山云选择用 Pulsar 实现日志服务</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文为 Apache Pulsar 的入门学习笔记。 &lt;/p&gt;
&lt;p&gt;企业在考虑部署实时消息系统时，总体硬件成本很重要的。Kafka 是基于磁盘存储数据的，存储成本较高。&lt;br&gt;而 Pulsar Broker 不直接存储数据，而是使用 Apache BookKeeper 来存储数据。数据发送/接收和存储的解耦使得 BookKeeper 可以在运行在独立的物理计算机或容器上。&lt;/p&gt;
&lt;p&gt;当新生事物出现有两种角度去观察它，要么把它看小，要么把它放大。对 Apache Pulsar，把它看小的角度通常是”Apache Pulsar 只是一个新的消息队列而已”，或者“Apache Pulsar 只是一个新的数据管道而已”，“队列系统早就有了，只是 Apache Pulsar 更具扩展性也能解决某些场景问题而已，基本没啥本质区别”。很明显上述认识都不对，Apache Pulsar 在消息、流、数据管理和技术基础设施层面都有技术演进，详看正文。&lt;/p&gt;
    
    </summary>
    
      <category term="MQ" scheme="http://yoursite.com/categories/MQ/"/>
    
      <category term="Pulsar" scheme="http://yoursite.com/categories/MQ/Pulsar/"/>
    
    
  </entry>
  
  <entry>
    <title>Mac单机安装Apache Pulsar</title>
    <link href="http://yoursite.com/2021/05/25/Mac%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85Apache-Pulsar/"/>
    <id>http://yoursite.com/2021/05/25/Mac单机安装Apache-Pulsar/</id>
    <published>2021-05-25T05:09:47.000Z</published>
    <updated>2021-06-14T12:16:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录一下 Mac 本机安装 Pulsar 的过程。单机安装 Pulsar， ZooKeeper 和 BookKeeper 会和 Pulsar 运行在同一个 JVM 进程中。</p><a id="more"></a><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>Mac：10.15.7<br>JDK：1.8.0_151<br>Pulsar：2.7.2</p><h2 id="本机单机安装-Pulsar"><a href="#本机单机安装-Pulsar" class="headerlink" title="本机单机安装 Pulsar"></a>本机单机安装 Pulsar</h2><h3 id="下载并解压"><a href="#下载并解压" class="headerlink" title="下载并解压"></a>下载并解压</h3><p>从官网下载解压到某个目录下：</p><ul><li>bin：pulsar 的命令行</li><li>conf：pulsar 的配置文件，包括 broker、 ZooKeeper 的配置文件等</li><li>examples：pulsar functions 示例 jar 包</li><li>lib：pulsar 的依赖包目录</li><li>license：license 文件</li></ul><p>并在 /etc/profile 设置环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PULSAR_HOME=/usr/local/pulsar</span><br><span class="line">export PATH=$PATH:$PULSAR_HOME/bin</span><br></pre></td></tr></table></figure><p>使环境变量生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="修改-Pulsar-配置"><a href="#修改-Pulsar-配置" class="headerlink" title="修改 Pulsar 配置"></a>修改 Pulsar 配置</h3><h4 id="修改-pulsar-env-sh-文件"><a href="#修改-pulsar-env-sh-文件" class="headerlink" title="修改 pulsar_env.sh 文件"></a>修改 pulsar_env.sh 文件</h4><p>默认的 JVM 内存为 2G，可以通过更改配置项：<code>PULSAR_MEM</code> </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extra options to be passed to the jvm</span></span><br><span class="line">PULSAR_MEM=<span class="variable">$&#123;PULSAR_MEM:-"-Xms2g -Xmx2g -XX:MaxDirectMemorySize=4g"&#125;</span></span><br></pre></td></tr></table></figure><h3 id="安装内置-Connectors"><a href="#安装内置-Connectors" class="headerlink" title="安装内置 Connectors"></a>安装内置 Connectors</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$PULSAR_HOME</span></span><br><span class="line"></span><br><span class="line">mkdir connectors</span><br><span class="line"></span><br><span class="line">mv pulsar-io-aerospike-2.7.2.nar <span class="variable">$PULSAR_HOME</span>/connectors</span><br></pre></td></tr></table></figure><h3 id="安装-tiered-storage-offloaders"><a href="#安装-tiered-storage-offloaders" class="headerlink" title="安装 tiered storage offloaders"></a>安装 tiered storage offloaders</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$PULSAR_HOME</span></span><br><span class="line"></span><br><span class="line">mkdir offloaders</span><br><span class="line"></span><br><span class="line">tar xvfz apache-pulsar-offloaders-2.7.2-bin.tar.gz</span><br><span class="line"></span><br><span class="line">mv apache-pulsar-offloaders-2.7.2/offloaders <span class="variable">$PULSAR_HOME</span>/offloaders</span><br></pre></td></tr></table></figure><h3 id="启动-pulsar-服务"><a href="#启动-pulsar-服务" class="headerlink" title="启动 pulsar 服务"></a>启动 pulsar 服务</h3><p>前台 standalone 启动 pulsar：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$PULSAR_HOME</span>/bin/pulsar standalone</span><br></pre></td></tr></table></figure><p>后台 standalone 启动 pulsar：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$PULSAR_HOME</span>/bin/pulsar-daemon start standalone</span><br></pre></td></tr></table></figure><p>运行 pulsar 之后会在 pulsar 安装目录下生成以下 3 个目录：</p><ul><li>data： ZooKeeper 和 BookKeeper 的数据存储目录</li><li>instances： pulsar functions 生成的实例</li><li>logs： 安装启动日志文件目录</li></ul><h3 id="使用-pulsar-客户端"><a href="#使用-pulsar-客户端" class="headerlink" title="使用 pulsar 客户端"></a>使用 pulsar 客户端</h3><h4 id="消费数据"><a href="#消费数据" class="headerlink" title="消费数据"></a>消费数据</h4><ul><li>消费 pulsar 数据命令行，topic 名称为 my-topic，订阅名称为 first-subscription：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$PULSAR_HOME</span>/bin/pulsar-client consume my-topic -s <span class="string">"first-subscription"</span></span><br><span class="line"></span><br><span class="line">18:43:27.492 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [my-topic][first-subscription] Subscribing to topic on cnx [id: 0xb385e78b, L:/127.0.0.1:58963 - R:localhost/127.0.0.1:6650], consumerId 0</span><br></pre></td></tr></table></figure><h4 id="生产数据"><a href="#生产数据" class="headerlink" title="生产数据"></a>生产数据</h4><ul><li>向 my-topic 中生产一条数据为 “hello-pulsar”：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$PULSAR_HOME</span>/bin/pulsar-client produce my-topic --messages <span class="string">"hello-pulsar"</span></span><br><span class="line"></span><br><span class="line">18:43:53.354 [main] INFO  org.apache.pulsar.client.cli.PulsarClientTool - 1 messages successfully produced</span><br></pre></td></tr></table></figure><p>此时，pulsar consume 命令窗口会消费到数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">----- got message -----</span><br><span class="line">key:[null], properties:[], content:hello-pulsar</span><br></pre></td></tr></table></figure><h3 id="关闭-pulsar-服务"><a href="#关闭-pulsar-服务" class="headerlink" title="关闭 pulsar 服务"></a>关闭 pulsar 服务</h3><p>前台 standalone 启动的 pulsar：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ctrl + C</span><br></pre></td></tr></table></figure><p>后台 standalone 启动的 pulsar：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$PULSAR_HOME</span>/bin/pulsar-daemon stop standalone</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://pulsar.apache.org/" target="_blank" rel="noopener">Pulsar 官网</a><br><a href="https://www.jianshu.com/p/728d07918f49?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation" target="_blank" rel="noopener">Pulsar-安装部署 简书</a><br><a href="https://archive.apache.org/dist/pulsar/" target="_blank" rel="noopener">Pulsar 安装包下载地址</a><br><a href="https://archive.apache.org/dist/pulsar/pulsar-2.7.2/connectors/" target="_blank" rel="noopener">Pulsar-2.7.2 Connectors 安装包下载地址</a><br><a href="https://archive.apache.org/dist/pulsar/pulsar-2.7.2/" target="_blank" rel="noopener">Pulsar-2.7.2 tiered storage offloaders 安装包下载地址</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录一下 Mac 本机安装 Pulsar 的过程。单机安装 Pulsar， ZooKeeper 和 BookKeeper 会和 Pulsar 运行在同一个 JVM 进程中。&lt;/p&gt;
    
    </summary>
    
      <category term="MQ" scheme="http://yoursite.com/categories/MQ/"/>
    
      <category term="Pulsar" scheme="http://yoursite.com/categories/MQ/Pulsar/"/>
    
    
  </entry>
  
  <entry>
    <title>Hologres</title>
    <link href="http://yoursite.com/2021/05/25/Hologres/"/>
    <id>http://yoursite.com/2021/05/25/Hologres/</id>
    <published>2021-05-25T04:56:14.000Z</published>
    <updated>2021-06-09T18:37:17.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="6d6d7b14a4649e5354ce73346a0232a70b189e00e9a1c47cde82ee091a6eae06">49f7eb9bea6cd6a47e660ee8aad95e5f73cac4e1b520f2f05987d67f416cec10d8c2add8935a4ae2fb7082d916f36e1d3b1ed1fb7bcc4a12e137188f59446abb2793b486bccbe4623c71e641069af1d03b6d7448688f68fd44135d03359ccd28f09c7d65ba42a4d058f4476b96d7f174862b5dc8cec10b8e87ab53a12373a7741c79476ae3c1fd3c456dad1afb3ec00c8777d5797b087436a5537340778a18315f36208c648a0cae72ad59853f82248d9e473db4ca18a536804bb210390995b7043036537b0454d8fad45a3641d00f66a542cea104544daebc48572cf799db379a2580f829a171227e4d9967ed65224ff9f797b18f794c37f4bc6af45bcba76401af3e932c5441d6ea6a548c81198973257ce16d90c9f98f76aaebfb07db5685</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Here&#39;s something encrypted, password is required to continue reading.
    
    </summary>
    
      <category term="OLAP" scheme="http://yoursite.com/categories/OLAP/"/>
    
      <category term="Hologres" scheme="http://yoursite.com/categories/OLAP/Hologres/"/>
    
    
  </entry>
  
  <entry>
    <title>关于大数据发展趋势的思考与总结</title>
    <link href="http://yoursite.com/2021/05/24/%E5%85%B3%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E7%9A%84%E6%80%9D%E8%80%83%E4%B8%8E%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2021/05/24/关于大数据发展趋势的思考与总结/</id>
    <published>2021-05-24T07:30:57.000Z</published>
    <updated>2022-02-08T07:43:55.230Z</updated>
    
    <content type="html"><![CDATA[<p>现代企业的大数据平台大多是基于 Hadoop 构建的”一存多算”的多元化架构，以 HDFS 为统一存储，通过 Spark、HBase、Flink、Presto 等多种计算引擎满足不同场景的处理需求。<br>如今，高弹性和可扩展的计算与存储俨然已经非常成熟了，未来云原生、一体化将是大数据的技术发展趋势，并且依附于小程序IoT等业务载体，以 Saas 带动 IaaS 必将成为大势。</p><a id="more"></a><h2 id="云原生"><a href="#云原生" class="headerlink" title="云原生"></a>云原生</h2><p>上云已经成为企业数字化的共识，上云之后如何用好云是当前大家的重点讨论和思考，如果仍然按照过去线下机房的模式去部署和使用大数据，不仅无法获得云计算的红利，甚至在成本效率上可能面临诸多不适。所以，以”存储计算分离+弹性Serverless”为代表的云原生大数据架构，实现存储的计算伸缩、资源弹性按需使用，大幅提升资源利用率、系统运维灵活性，成为接下来的主要趋势。在阿里云平台，以 Lindorm (兼容HDFS、HBase等多模态的 Serverless 存储) + DLA (提供 Spark、Presto等多模态的 Serverless 计算)向企业提供了云原生的大数据最佳实践。</p><h2 id="一体化"><a href="#一体化" class="headerlink" title="一体化"></a>一体化</h2><p>开源大数据技术在经过十多年的快速发展，在采集、存储、计算、调度、管理等各个方面的整体版图已经相当完善，同时面向各个场景的存储计算引擎也呈现百花齐放的景象，但这也加大了用户的使用门槛和维护复杂度，多种系统的一体化也越来越成为下一个大的发展趋势，比如多种模型数据库的一体化、大数据与数据库的一体化、批计算与流计算的一体化、数据湖与数据仓库的一体化等，这些技术上的整合，可以帮助企业更加经济高效的用好数据。</p><ul><li><p>结果的批流一体<br>用户不需要关心批或者流，在用户提交查询的时候得到的结果就是截止那一刻的统计结果。</p></li><li><p>存储的批流一体：统计场景<br>如 Hologres<br>高性能的实时/批量 append 和 update 的能力，读写互不影响，比如当前的数据湖概念<br>增量订阅读取、批量读取的能力，类似 Apache Pulsar<br>和 OLAP 引擎（impala、presto、clickhouse）对接的能力，列式存储具备较强 SCAN 和 filter 的能力</p></li><li><p>计算引擎的批流一体<br>一套代码搞定批流统计场景，降低开发运维成本</p></li></ul><h2 id="SaaS-带动-IaaS"><a href="#SaaS-带动-IaaS" class="headerlink" title="SaaS 带动 IaaS"></a>SaaS 带动 IaaS</h2><p>与美国相比，中国的云计算市场是”本末倒置”的。美国是以 SaaS 为主，中国现在还是以 IaaS 为主，处于大建数据中心阶段。</p><p>但数据中心里躺着的那些服务器是需要用户买单的，而 SaaS 应用是消耗这些服务器的不二法门。用 SaaS 带动 IaaS，是主流云厂商非常重要的竞争策略。阿里云力推的”云钉一体”就是一个典型案例，可以预见，腾讯云、华为云都将走这一条路。</p><p>要有足够繁荣的生态，才能消耗那些躺在数据中心里的上百万台云服务器。不然，服务器利用率上不去，盲目上规模无异于自杀。</p><p>虽然业内对阿里云+钉钉、腾讯云+企业微信的组合介绍已经很多了，但他们的手里还有第二张王牌。支付宝是阿里云手里的第二张王牌，微信是腾讯云手里的第二张王牌。</p><p>在 PC 时代，大一点的企业以及政府、学校等机构都有一个官方网站，是这些机构对外进行品牌展示、办理业务的窗口。在移动互联网时代，手机替换了电脑。人们越来越多用手机来了解信息和办理业务，网页端的官方网站日渐式微。</p><p>随着 5G 网络的建设普及，小程序将成为 PC 时代”官网”一样的存在。并且，小程序的互动性、及时性会更强。大量的企业和政府单位，将通过小程序的方式来进行信息传递、业务办理，并与用户进行直接、实时的沟通。</p><p>一旦小程序更多地承载业务场景，那其承载的信息流和业务流将出现指数级增长，这需要消耗大量的计算、网络资源，也将成为消耗云计算的关键渠道。</p><p>届时，得小程序者得云计算天下。</p><p>谁会是小程序的赢家，第一是微信，第二是支付宝。百度、华为、字节跳动都得靠边站，UCloud、青云这些云计算小巨头更没戏，而支付宝将成为政府服务的主要渠道，越来越多的政府事务将会可以通过支付宝办理。</p><p>通过手机办理政府业务，将成为大势所趋。并且，没必要每个政府部门都开发一款 App。比如个人所得税 APP，一年也就用一两次。像这类应用，本身业务场景不复杂，通过小程序来实现完全可以。随着 5G 网络的成熟，小程序的流畅度会进一步提升，能够承载的业务场景也会更多。</p><p>支付宝很可能会垄断政府对外业务，随之而来的，阿里云则可能会垄断政务云市场。政府不仅要关注内部系统，更重要的是要与公民进行交互，而这需要一个国民级的 APP 来进行承载。<br>这个国民级 APP，主要是支付宝，其次是微信。</p><p>前端通过支付宝来作为政府服务窗口，后端通过阿里云承载政务云系统，并且实现不同政府部门业务系统和数据的打通，将是未来的发展趋势。因此，阿里云在政务云的市场份额会进一步扩大，这对 UCloud 这样的小巨头而言不是个好消息。</p><p>另一方面，微信小程序的发展空间则更大，并且不局限于政务领域。将来，大部分企业对外服务的主阵地都将由官网转移到微信小程序上。与之配套的，腾讯云很可能会成为最大的赢家。</p><p>以这个角度来看，阿里云可以通过钉钉+支付宝来构建应用生态，腾讯云可以借助微信+企业微信来构建应用生态，以 SaaS 带动 IaaS 消耗。这是横跨C端和B端的顶级巨头玩法，UCloud完全没有可能建立这样的生态系统。未来，UCloud面临阿里云、腾讯云的生态战压力会越来越大。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.infoq.cn/article/0vipwxcltrcc85xojnxe" target="_blank" rel="noopener">阿里为什么要做多模数据库？</a><br><a href="http://www.woshipm.com/it/2795621.html" target="_blank" rel="noopener">小程序生态之路：阿里向左，腾讯向右 – 行业深度战略分析报告</a><br><a href="https://mp.weixin.qq.com/s/1sn6zkNUa7lkR4H_Cij0yw" target="_blank" rel="noopener">UCloud，创业公司死磕公有云的悲壮</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现代企业的大数据平台大多是基于 Hadoop 构建的”一存多算”的多元化架构，以 HDFS 为统一存储，通过 Spark、HBase、Flink、Presto 等多种计算引擎满足不同场景的处理需求。&lt;br&gt;如今，高弹性和可扩展的计算与存储俨然已经非常成熟了，未来云原生、一体化将是大数据的技术发展趋势，并且依附于小程序IoT等业务载体，以 Saas 带动 IaaS 必将成为大势。&lt;/p&gt;
    
    </summary>
    
      <category term="思考总结" scheme="http://yoursite.com/categories/%E6%80%9D%E8%80%83%E6%80%BB%E7%BB%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Presto技术内幕</title>
    <link href="http://yoursite.com/2021/03/04/Presto%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95/"/>
    <id>http://yoursite.com/2021/03/04/Presto技术内幕/</id>
    <published>2021-03-03T17:18:43.000Z</published>
    <updated>2021-06-14T12:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="42deb763f239dd8df9f2339ff653c170bb58b531ec6dd5de8597ce3580d3e56d">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Here&#39;s something encrypted, password is required to continue reading.
    
    </summary>
    
      <category term="OLAP" scheme="http://yoursite.com/categories/OLAP/"/>
    
      <category term="Presto" scheme="http://yoursite.com/categories/OLAP/Presto/"/>
    
    
  </entry>
  
  <entry>
    <title>初识Presto(Trino)</title>
    <link href="http://yoursite.com/2021/03/04/%E5%88%9D%E8%AF%86Presto(Trino)/"/>
    <id>http://yoursite.com/2021/03/04/初识Presto(Trino)/</id>
    <published>2021-03-03T17:17:43.000Z</published>
    <updated>2021-06-07T16:25:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>  本文为 Presto 的入门学习笔记。 Presto 是一个开源的分布式 SQL 查询引擎，它是为了高效查询不同系统和各种规模的数据源而从头开始设计和编写的一套系统。<br>  由于开源纷争，Presto 现已更名为 Trino。</p><a id="more"></a><h2 id="Presto-简介"><a href="#Presto-简介" class="headerlink" title="Presto 简介"></a>Presto 简介</h2><h3 id="背景及发展"><a href="#背景及发展" class="headerlink" title="背景及发展"></a>背景及发展</h3><p>Hadoop 提供的大数据解决方案使用的是 MR 计算框架，这种计算框架适用于大数据的离线和批量计算，因为该计算框架强调的是吞吐率而不是计算效率，所以其不能满足大数据快速实时 Ad-Hoc 查询计算的性能要求。</p><p>因此，开源社区和各大互联网公司纷纷进行大数据实时 Ad-Hoc 查询计算产品的研发 ， Facebook 于2012年秋季开始开发 Presto，目前该产品已经在超过 1000 名 Facebook 雇员中使用，每天运行超过 30000 个查询，每日查询数据量在 1PB 级别。Facebook 称 Presto 的性能比 Hive 要好上 10 倍还多，2013年 Facebook 正式宣布开源 Presto。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li><p>多数据源<br>目前 Presto 可以支持 Mysql、PostgreSql、Cassandra、Hive、Kafka、JMX、Iceberg 等多种 Connector，并且可以支持分库分表以及快速读取的功能。</p></li><li><p>支持 SQL<br>Presto 已经可以完全支持 ANSI SQL，并提供了一个 SQL Shell 给用户，用户可以直接使用 ANSI SQL 进行数据查询和计算。</p></li><li><p>扩展性<br>开发人员可以很容易的开发出适用于自己特定数据源的 Connector，并且可以使用 SQL 语句查询和分析自定义 Connector 中的数据。</p></li><li><p>混合计算<br>每种类型的数据源都对应于一种特定类型的 Connector，用户可以根据业务需要在 Presto 中针对于一种类型的 Connector 配置一个或多个 Catalog 并查询其中的数据，用户可以混合多个 Catalog 进行 join 查询和计算。</p></li><li><p>高性能<br>经过 Facebook 和 京东商城的测试，Presto 的查询平均性能是 Hive 的10倍以上。</p></li><li><p>流水线<br>由于 Presto 是基于 pipeline 进行设计的，因此在进行海量数据处理的过程中，终端用户不用等到所有的数据都处理完毕才能看到结果，而是可以像自来水管道一样，一旦开始计算，就可以立即产生一部分结果数据，并且结果数据会一部分接一部分地呈现在终端客户面前。</p></li></ul><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h4 id="Presto-服务进程"><a href="#Presto-服务进程" class="headerlink" title="Presto 服务进程"></a>Presto 服务进程</h4><ul><li><p>Coordinator<br>Coordinator 服务进程部署于集群中一个单独的节点上，是整个 Presto 集群的管理节点。主要用于接收客户端提交的查询，查询语句解析，生成查询执行计划、Stage、Task，对生成的 Task 进行调度。<br>此外，Coordinator 还对集群中的所有 Worker 进行管理，是整个 Presto 集群的 Master 进程，该进程既与 Worker 进行通信从而获得最新的 Worker 信息，又与 Client 进行通信，从而接收查询请求。</p></li><li><p>Worker<br>在每个 Worker 节点上都存在一个 Worker 服务进程，主要进行数据的处理以及 Task 的执行。Worker 进程每隔一定的时间会向 Coordinator 上的 restful<br>服务发送心跳。当客户端提交一个查询时，Coordinator 则会从当前存活的 Worker 列表中选择出合适的 Worker 节点去运行 Task。Worker 在执行每个 Task 时会进一步对当前 Task 读入的每个 Split 进行一系列的操作和处理。</p></li></ul><h4 id="Presto-模型"><a href="#Presto-模型" class="headerlink" title="Presto 模型"></a>Presto 模型</h4><ul><li>Connector</li></ul><p>使 Presto 适配一个数据源，每一个 Catalog 对应于一个特定的连接器。</p><ul><li>Catalog</li></ul><p>定义连接到一个数据源的细节，它包含了 Schema 并配置了一个连接器来使用。</p><ul><li>Schema</li></ul><p>组织表的一种方式。Catalog 和 Schema 一起定义了一个集合的表，这些表可以查询。</p><ul><li>Table</li></ul><p>表是无序的行的集合。这些行内容被组织成带有数据类型的有名称的列。</p><h4 id="Presto-查询执行模型"><a href="#Presto-查询执行模型" class="headerlink" title="Presto 查询执行模型"></a>Presto 查询执行模型</h4><p>在 Presto 中一次查询执行会被分解为多个 Stage ，Stage 与 Stage 之间是有前后依赖关系的。每个 Stage 内部又会被分解为多个 Task，属于每个 Stage 的 Task 被均分在每个 Worker 上并行执行。在每个 Task 内部又会被分解为多个 Driver ，每个 Driver 负责处理一个 Split ，而且每个 Driver 由一系列前后相连的 Operator 组成，这里的每个 Operator 都代表针对于一个 Split 的操作。</p><ul><li><p>Statement<br>Statement 语句，指的是终端用户输入的用文字表示的 SQL 语句，由子句（Clause）、表达式（Expression）和断言（Predicate）组成。</p></li><li><p>Query<br>Query 即查询执行。当 Presto 接收一个 SQL 语句并执行时，会解析该 SQL 语句，将其转变成一个查询执行和相关的查询执行计划。一个查询执行代表可以在 Presto 集群中运行的查询，是由运行在各个 Worker 上且各自之间相互关联的阶段（Stage）组成的。<br>查询执行是为了完成 SQL 语句所表述的查询而实例化的配置信息、组件、查询执行计划和优化信息等。一个查询执行由 Stage、Task、Driver、Split、Operator 和 DataSource 组成，</p></li><li><p>Stage<br>Stage 即查询执行阶段。当 Presto 运行 Query 时，Presto 会将一个 Query 拆分成具有层级关系的多个 Stage，一个 Stage 就代表查询计划的一部分。</p></li><li><p>Exchange<br>Presto 的 Stage 是通过 Exchange 来连接另一个 Stage 的，Exchange 用于完成有上下游关系的 Stage 之间的数据交换。</p></li><li><p>Task<br>Stage 并不会在 Presto 集群中实际运行，仅代表针对于一个 SQL 语句查询执行计划中的一部分查询的执行过程，只是用来对查询执行计划进行管理和建模。Stage 在逻辑上又被分为一系列的 Task，这些 Task 则需要实际运行在 Presto 的各个 Worker 节点上。</p></li><li><p>Driver<br>一个 Task 包含一个或多个 Driver。一个 Driver 其实就是作用于一个 Split 的一系列 Operator 的集合。因此一个 Driver 用于处理一个 Split，并且生成相应的输出，这些输出由 Task 收集并传送给下游 Stage 中的一个 Task。一个 Driver 拥有一个输入和一个输出。</p></li><li><p>Operator<br>一个 Operator 代表一个 Split 的一种操作，例如过滤、加权、转换等。一个 Operator 依次读取一个 Split 中的数据，将 Operator 所代表的计算和操作作用于 Split 的数据上，并产生输出。每个 Operator 均会以 Page 为最小处理单位分别读取输入数据和产生输出数据。Operator 每次只会读取一个 Page 对象，相应地，每次也只会产生一个 Page 对象。</p></li><li><p>Split<br>Split 即分片。一个分片是一个大的数据集中的一个小的子集。而 Driver 则是作用于一个分片上的一系列操作的集合，而每个节点上运行的 Task，又包含多个 Driver，从而一个 Task 可以处理多个 Split。</p></li><li><p>Page<br>Page 是 Presto 中处理的最小数据单元。一个 Page 对象包含多个 Block 对象，每个 Block 对象是一个字节数组，存储一个字段的若干行。多个 Block 横切的一行是真实的一行数据。一个 Page 最大为 1MB ，最多 16 * 1024 行数据。</p></li></ul><h3 id="Presto-使用场景"><a href="#Presto-使用场景" class="headerlink" title="Presto 使用场景"></a>Presto 使用场景</h3><h4 id="单一的-SQL-分析访问点"><a href="#单一的-SQL-分析访问点" class="headerlink" title="单一的 SQL 分析访问点"></a>单一的 SQL 分析访问点</h4><p>作为一个消费者和分析师，你可能会遇到数不清的问题：</p><ul><li>有时你甚至不知道数据在哪儿，只有凭借公司某个部门的内部知识或者组织内多年的工作经验，你才能找到正确的数据。</li><li>为了查询多个数据库，你需要使用不同的连接和运行多种 SQL 方言的不同查询。这些查询看起来相似，行为上却不同。</li><li>若不使用数据仓库，就无法使用查询合并来自不同系统的数据。</li></ul><p>可以使用 Presto 对接这些数据库，使用一个 SQL 标准来查询所有的系统。所有的仪表盘和分析工具以及其他商业智能系统都可以指向一个系统 – Presto，并访问组织当中的所有数据。</p><h4 id="数据仓库和数据源系统的访问点"><a href="#数据仓库和数据源系统的访问点" class="headerlink" title="数据仓库和数据源系统的访问点"></a>数据仓库和数据源系统的访问点</h4><p>当一个组织需要更好的理解和分析存放在无数 RDBMS 中的数据时，就可以创建和维护数据仓库系统。从多个系统中抽取的数据通过一个复杂的 ETL 过程，最终进入一个严格受控的、巨大的数据仓库。</p><p>尽管数据仓库在很多情况下非常有用，但作为一个数据分析师，你会面临很多新问题：</p><ul><li>除了原来的那些数据库，你的工具和查询现在又多了一个数据接入点。</li><li>你今天就要用的数据还没放入数据仓库。加载数据的过程痛苦、昂贵又困难重重重。</li></ul><p>Presto 允许你添加任何数据仓库作为数据源，就像其他关系数据库一样。如果想深入研究数据仓库的查询，可以在 Presto 里直接完成，也可以在这里访问数据仓库及其源数据库系统，甚至可以编写将它们组合在一起查询。</p><h4 id="提供对任何内容的-SQL-访问"><a href="#提供对任何内容的-SQL-访问" class="headerlink" title="提供对任何内容的 SQL 访问"></a>提供对任何内容的 SQL 访问</h4><p>Presto 允许将所有支持的系统作为数据源进行连接。它使用标准的 ANSI SQL 和使用 SQL 的所有工具对外暴露要查询的数据。</p><h4 id="联邦查询"><a href="#联邦查询" class="headerlink" title="联邦查询"></a>联邦查询</h4><p>将所有的数据孤岛都暴露给 Presto 是向理解数据迈出的一大步。可以使用 SQL 和标准工具来联邦查询所有内容。 在一个语句中引用并使用不同数据库和模式的 SQL 查询，这些数据库和 Schema 来自于完全不同的系统。在同一条 SQL 查询中，可以查询 Presto 中可用的所有数据源。</p><h4 id="虚拟数据仓库的语义层"><a href="#虚拟数据仓库的语义层" class="headerlink" title="虚拟数据仓库的语义层"></a>虚拟数据仓库的语义层</h4><p>数据仓库系统为用户创造了巨大的价值，对组织来说确实一个负担。</p><ul><li>运行和维护数据仓库是一个巨大且昂贵的项目。</li><li>需要专门的团队运行与管理数据仓库和相关的 ETL 过程。</li><li>将数据导入数据仓库需要用户执行繁琐的操作，并且通常非常耗时。</li></ul><p>Presto 可用作虚拟仓库。使用这一工具和标准的 ANSI SQL ，就可以定义语义层。一旦所有的数据库都设置成 Presto 的数据源，就可以直接查询它们。Presto 提供了查询这些数据库所需的计算能力。使用 SQL 和 Presto 支持的函数和运算符，可以直接从数据源获得想要的数据。在使用数据进行分析之前，无需复制、移动或转换它们。</p><h4 id="数据湖查询引擎"><a href="#数据湖查询引擎" class="headerlink" title="数据湖查询引擎"></a>数据湖查询引擎</h4><p>在数据被存储到数据湖的存储系统时，并没有特别考虑接下来应该如何访问它们，Presto 可以使它们成为有用的数据仓库。现代数据湖通常使用 HDFS 以外的其他对象存储系统，这些系统来自云供应商或其他开源项目。 Presto 能使用 Hive 连接器连接它们，无论数据在哪里、如何存储，都可以在数据湖上使用基于 SQL 的数据分析。</p><h4 id="SQL-转换和-ETL"><a href="#SQL-转换和-ETL" class="headerlink" title="SQL 转换和 ETL"></a>SQL 转换和 ETL</h4><p>Presto 也可用于迁移数据，它所提供的丰富的 SQL 函数，可以查询数据，转换数据，并将数据写入同一个数据源或任何其他数据源。</p><h4 id="更快的响应带来更好的数据见解"><a href="#更快的响应带来更好的数据见解" class="headerlink" title="更快的响应带来更好的数据见解"></a>更快的响应带来更好的数据见解</h4><p>复杂的问题和海量数据集带来了诸多限制。将数据复制并加载到数据仓库并在其中分析它们的整个过程会过于昂贵。计算可能消耗太多的计算资源而无法处理全部数据，或者要消耗数天才能得到答案。</p><p>Presto 一开始就避免了数据复制。Presto 的并行计算和重度优化通常能为数据分析带来性能提升。</p><p>如果原来需要 3 天的查询现在只需要 15 分钟就可以完成，那么执行这个查询便是有价值的。从这些结果中获得的知识可以执行更多的查询。</p><h2 id="安装和配置-Presto"><a href="#安装和配置-Presto" class="headerlink" title="安装和配置 Presto"></a>安装和配置 Presto</h2><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn -T2C install -DskipTests</span><br></pre></td></tr></table></figure><h3 id="服务端部署"><a href="#服务端部署" class="headerlink" title="服务端部署"></a>服务端部署</h3><ul><li>从编译后的源码中拷贝jar和配置文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/<span class="built_in">local</span>/presto</span><br><span class="line"></span><br><span class="line">cp -r ～workspace/presto/presto-server/target/presto-server-0.255-SNAPSHOT /usr/<span class="built_in">local</span>/presto</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/presto/presto-server-0.255-SNAPSHOT</span><br><span class="line"></span><br><span class="line">cp ～workspace/presto/presto-server/target/presto-main/etc ./presto-server-0.255-SNAPSHOT/</span><br><span class="line"></span><br><span class="line">ln -s presto-server-0.255-SNAPSHOT server</span><br></pre></td></tr></table></figure><ul><li>设置环境变量</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PRESTO_SERVER_HOME=/usr/<span class="built_in">local</span>/presto/server</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$PRESTO_SERVER_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><ul><li>修改 config.properties</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">coordinator=true</span><br><span class="line">node-scheduler.include-coordinator=true</span><br><span class="line">http-server.http.port=8086</span><br><span class="line">discovery-server.enabled=true</span><br><span class="line">discovery.uri=http://localhost:8086</span><br></pre></td></tr></table></figure><ul><li>修改 node.properties</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">node.id=562e42e2-e874-431f-8da5-cb779744cf7c</span><br><span class="line">node.data-dir=/usr/local/presto/data</span><br><span class="line">catalog.config-dir=/usr/local/presto/server/etc/catalog</span><br><span class="line">plugin.dir=/usr/local/presto/server/plugin</span><br><span class="line">node.server-log-file=/usr/local/presto/server/var/log/server.log</span><br><span class="line">node.launcher-log-file=/usr/local/presto/server/var/log/launcher.log</span><br></pre></td></tr></table></figure><ul><li>修改 jvm.config</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xmx4G</span><br><span class="line">-XX:-UseBiasedLocking</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExitOnOutOfMemoryError</span><br><span class="line">-XX:ReservedCodeCacheSize=512M</span><br></pre></td></tr></table></figure><ul><li>修改 log.properties</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.facebook.presto=INFO</span><br></pre></td></tr></table></figure><ul><li>后台启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$PRESTO_SERVER_HOME</span>/bin/launcher start</span><br></pre></td></tr></table></figure><ul><li>前台启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$PRESTO_SERVER_HOME</span>/bin/launcher run</span><br></pre></td></tr></table></figure><p>[Presto Web UI] <code>http://localhost:8086/ui/)</code></p><p><img src="Presto%E6%9C%AC%E5%9C%B0%E5%90%AF%E5%8A%A8%E7%9A%84Web_UI.png" alt></p><h3 id="Presto-CLI"><a href="#Presto-CLI" class="headerlink" title="Presto CLI"></a>Presto CLI</h3><ul><li>从编译后的源码中拷贝jar</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/presto</span><br><span class="line"></span><br><span class="line">mkdir -p cli/lib</span><br><span class="line"></span><br><span class="line">cp ～workspace/presto/presto-cli/target/presto-cli-0.255-SNAPSHOT-executable.jar /usr/<span class="built_in">local</span>/presto/cli/lib</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/presto/cli/lib</span><br><span class="line"></span><br><span class="line">mv presto-cli-0.255-SNAPSHOT-executable.jar presto</span><br><span class="line"></span><br><span class="line">chmod +x presto</span><br></pre></td></tr></table></figure><ul><li>设置环境变量</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PRESTO_CLI_HOME=/usr/<span class="built_in">local</span>/presto/cli</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$PRESTO_CLI_HOME</span>/lib</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><ul><li>运行 cli 并查看其版本</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">presto --version</span><br><span class="line">Presto CLI 0.255-SNAPSHOT-9095346</span><br></pre></td></tr></table></figure><ul><li>启动 cli</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">presto --server localhost:8086</span><br></pre></td></tr></table></figure><ul><li>额外诊断，打印调试信息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">presto --debug</span><br></pre></td></tr></table></figure><ul><li>执行查询</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">presto --server localhost:8086 --catalog tpch --schema sf1 --execute <span class="string">'select nationkey,name,regionkey from nation limit 5'</span></span><br><span class="line"><span class="string">"0"</span>,<span class="string">"ALGERIA"</span>,<span class="string">"0"</span></span><br><span class="line"><span class="string">"1"</span>,<span class="string">"ARGENTINA"</span>,<span class="string">"1"</span></span><br><span class="line"><span class="string">"2"</span>,<span class="string">"BRAZIL"</span>,<span class="string">"1"</span></span><br><span class="line"><span class="string">"3"</span>,<span class="string">"CANADA"</span>,<span class="string">"1"</span></span><br><span class="line"><span class="string">"4"</span>,<span class="string">"EGYPT"</span>,<span class="string">"4"</span></span><br></pre></td></tr></table></figure><h3 id="简单-SQL-语法"><a href="#简单-SQL-语法" class="headerlink" title="简单 SQL 语法"></a>简单 SQL 语法</h3><ul><li>查看 catalogs</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">presto&gt; show catalogs;</span><br><span class="line">  Catalog   </span><br><span class="line">------------</span><br><span class="line"> blackhole  </span><br><span class="line"> druid      </span><br><span class="line"> example    </span><br><span class="line"> hive       </span><br><span class="line"> jmx        </span><br><span class="line"> localfile  </span><br><span class="line"> memory     </span><br><span class="line"> mysql      </span><br><span class="line"> pinot      </span><br><span class="line"> postgresql </span><br><span class="line"> raptor     </span><br><span class="line"> sqlserver  </span><br><span class="line"> system     </span><br><span class="line"> tpcds      </span><br><span class="line"> tpch       </span><br><span class="line">(15 rows)</span><br><span class="line"></span><br><span class="line">Query 20210606_141818_00009_4qtix, FINISHED, 1 node</span><br><span class="line">Splits: 19 total, 19 <span class="keyword">done</span> (100.00%)</span><br><span class="line">0:00 [0 rows, 0B] [0 rows/s, 0B/s]</span><br></pre></td></tr></table></figure><ul><li>查看 tpch Connector 的 schemas</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">presto&gt; show schemas from tpch;</span><br><span class="line">       Schema       </span><br><span class="line">--------------------</span><br><span class="line"> information_schema </span><br><span class="line"> sf1                </span><br><span class="line"> sf100              </span><br><span class="line"> sf1000             </span><br><span class="line"> sf10000            </span><br><span class="line"> sf100000           </span><br><span class="line"> sf300              </span><br><span class="line"> sf3000             </span><br><span class="line"> sf30000            </span><br><span class="line"> tiny               </span><br><span class="line">(10 rows)</span><br><span class="line"></span><br><span class="line">Query 20210606_142008_00010_4qtix, FINISHED, 1 node</span><br><span class="line">Splits: 19 total, 19 <span class="keyword">done</span> (100.00%)</span><br><span class="line">0:00 [10 rows, 119B] [141 rows/s, 1.65KB/s]</span><br></pre></td></tr></table></figure><ul><li>查看 tpch.sf1 的 tables</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">presto&gt; show tables from tpch.sf1;</span><br><span class="line">  Table   </span><br><span class="line">----------</span><br><span class="line"> customer </span><br><span class="line"> lineitem </span><br><span class="line"> nation   </span><br><span class="line"> orders   </span><br><span class="line"> part     </span><br><span class="line"> partsupp </span><br><span class="line"> region   </span><br><span class="line"> supplier </span><br><span class="line">(8 rows)</span><br><span class="line"></span><br><span class="line">Query 20210606_142111_00011_4qtix, FINISHED, 1 node</span><br><span class="line">Splits: 19 total, 19 <span class="keyword">done</span> (100.00%)</span><br><span class="line">0:00 [8 rows, 158B] [85 rows/s, 1.66KB/s]</span><br></pre></td></tr></table></figure><ul><li>查看 tpch.sf1.nation 表中的实际数据</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">presto&gt; select count(name) from tpch.sf1.nation;</span><br><span class="line"> _col0 </span><br><span class="line">-------</span><br><span class="line">    25 </span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">Query 20210606_142214_00012_4qtix, FINISHED, 1 node</span><br><span class="line">Splits: 21 total, 21 <span class="keyword">done</span> (100.00%)</span><br><span class="line">0:00 [25 rows, 0B] [358 rows/s, 0B/s]</span><br></pre></td></tr></table></figure><ul><li>选择使用特定 schema</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">presto&gt; use tpch.sf1;</span><br><span class="line">USE</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>《Presto实战》<br>《Presto技术内幕》</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  本文为 Presto 的入门学习笔记。 Presto 是一个开源的分布式 SQL 查询引擎，它是为了高效查询不同系统和各种规模的数据源而从头开始设计和编写的一套系统。&lt;br&gt;  由于开源纷争，Presto 现已更名为 Trino。&lt;/p&gt;
    
    </summary>
    
      <category term="OLAP" scheme="http://yoursite.com/categories/OLAP/"/>
    
      <category term="Presto" scheme="http://yoursite.com/categories/OLAP/Presto/"/>
    
    
  </entry>
  
  <entry>
    <title>2020年终个人总结</title>
    <link href="http://yoursite.com/2021/02/01/2020%E5%B9%B4%E7%BB%88%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2021/02/01/2020年终个人总结/</id>
    <published>2021-02-01T06:00:40.000Z</published>
    <updated>2021-05-24T06:03:41.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="e8cb66f1889d332bb7f8280f899753257c5a0c67d437076bd79091327dff4dee">49f7eb9bea6cd6a47e660ee8aad95e5f339e339538ece7d1e61ba075caa7780d</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Here&#39;s something encrypted, password is required to continue reading.
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Apache Iceberg</title>
    <link href="http://yoursite.com/2021/01/20/Apache-Iceberg/"/>
    <id>http://yoursite.com/2021/01/20/Apache-Iceberg/</id>
    <published>2021-01-20T08:19:22.000Z</published>
    <updated>2021-06-03T17:41:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文为 Apache Iceberg 的入门学习笔记。</p><a id="more"></a><h2 id="Apache-Iceberg-简介"><a href="#Apache-Iceberg-简介" class="headerlink" title="Apache Iceberg 简介"></a>Apache Iceberg 简介</h2><p>官网定义：</p><p>Apache Iceberg is an open table format for huge analytic datasets. Iceberg delivers high query performance for tables with tens of petabytes of data, along with atomic commits, concurrent writes, and SQL-compatible table evolution.</p><ul><li><p>在文件 Format（Parquet/Avro/Orc）之上实现 table 语义<br>支持定义和变更Schema<br>支持 Hidden partition 和 Partition 变更<br>ACID 语义<br>历史版本回溯</p></li><li><p>特点<br>借助 partition 和 columns 统计信息实现分区裁剪<br>不绑定 HDFS，可拓展到 S3/OSS 等<br>容许多个 Writer 并发写入，乐观锁机制解决冲突</p></li></ul><h2 id="数据布局Layout"><a href="#数据布局Layout" class="headerlink" title="数据布局Layout"></a>数据布局Layout</h2><p><img src="Iceberg%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.png" alt></p><p>Layout 分为数据文件和元数据文件两部分。</p><p><img src="Iceberg_layout.png" alt></p><ul><li><p>数据文件（data files）<br>Iceberg 表真实存储数据的文件，一般存储在 data 目录下，以 “.parquet” 结尾。</p></li><li><p>清单文件（manifest file）<br>每行都是每个数据文件的详细描述，包括数据文件的状态、文件路径、分区信息、列级别的统计信息（比如每列的最大最小值、空值数等）。通过该文件，可以过滤掉无关数据，提高检索速度。</p></li><li><p>快照（snapshot）<br>快照代表一张表在某个时刻的状态。每个快照版本包含某个时刻的所有数据文件列表。data files 存储在不同的 manifest files 里， manifest files 存储在一个<br> manifest list 文件里面，而一个 manifest list 文件代表一个快照。</p></li></ul><h2 id="读写原理"><a href="#读写原理" class="headerlink" title="读写原理"></a>读写原理</h2><p><img src="%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86_1.png" alt></p><p>绿色表示数据文件，蓝色表示 manifest，黄色表示快照。一次 manifest 可以认为是一次 transaction 的写入，m0 对应写了两个文件，分别落到了 Partiiotn-0 、Partition-1，m1 对应也写了两个文件，分别落到了 Partiiotn-1 、Partition-2。对于 S1 来说，是由 m0 和 m1 两个 manifest 组成的。</p><p><img src="%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86_2.png" alt></p><p>基于 1 开始一次新的写入，写了一个 m2 的 transaction，写了一个数据文件 2 到 Partition-1，此时会新增一个 manifest m2 和 Snapshot S2，S2 会引用之前所有的 transaction m0 和 m1。</p><p>此时，如果要进行批量读取，就可以从 S1、S2、S3 中选择一个快照读取，比如选择了 S2，就是会把 m0、m1、m2 对应的数据全部 load 出来进行读取。</p><p>如果要进行增量读取，从 S0 开始把数据 load 出来，到 S1 的时候，会拿着 S1 的数据减去 S0 的数据得到增量的数据，然后交给计算引擎去做增量处理。</p><p><img src="%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86_3.png" alt></p><h3 id="查询计划"><a href="#查询计划" class="headerlink" title="查询计划"></a>查询计划</h3><p>查询计划是指在表中“查询所需文件”的过程。</p><ul><li><p>元数据过滤<br>清单文件包括分区数据元组和每个数据文件的列级统计信息。在计划期内，查询谓词会自动转换为分区数据上的谓词，并首先应用于过滤数据文件。接下来，使用列级值计数，<br>空计数，下限和上限来消除与查询谓词不匹配的文件。</p></li><li><p>snapshot id<br>每个 snapshot id 会关联到一组 manifest files，而每一组 manifest files 包含很多 manifest file 。</p></li><li><p>manifest files 文件列表<br>每个 manifest file 文件又记录了当前 data 数据块的元数据信息，其中就包含了文件列的最大值和最小值，然后根据这个元数据信息，索引到具体的文件块，从而更快的查询到数据。</p></li></ul><h2 id="Flink-Iceberg-流式入湖"><a href="#Flink-Iceberg-流式入湖" class="headerlink" title="Flink + Iceberg 流式入湖"></a>Flink + Iceberg 流式入湖</h2><h3 id="入门实践"><a href="#入门实践" class="headerlink" title="入门实践"></a>入门实践</h3><p><a href="https://github.com/apache/iceberg/pull/1464/files" target="_blank" rel="noopener">详细参考文档</a></p><h4 id="本机启动-Hadoop-和-Hive"><a href="#本机启动-Hadoop-和-Hive" class="headerlink" title="本机启动 Hadoop 和 Hive"></a>本机启动 Hadoop 和 Hive</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Hadoop</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line">./start-all.sh</span><br><span class="line">hdfs dfs -ls /</span><br><span class="line">drwx-wx-wx   - miaowenting staff          0 2021-05-30 23:27 /tmp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hive</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line">./hive --service metastore &amp;</span><br><span class="line">./hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h4 id="本机启动-Flink"><a href="#本机启动-Flink" class="headerlink" title="本机启动 Flink"></a>本机启动 Flink</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 Flink</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$FLINK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">./start-cluster.sh</span><br></pre></td></tr></table></figure><h4 id="准备-Flink-SQL-Client-环境"><a href="#准备-Flink-SQL-Client-环境" class="headerlink" title="准备 Flink SQL Client 环境"></a>准备 Flink SQL Client 环境</h4><ul><li>配置 HADOOP_CLASSPATH 环境变量：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`<span class="variable">$HADOOP_HOME</span>/bin/hadoop classpath`</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><ul><li>下载运行时jar：<br><a href="https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-flink-runtime/" target="_blank" rel="noopener">iceberg-flink-runtime.jar下载地址</a><br><a href="https://repo.maven.apache.org/maven2/org/apache/flink/" target="_blank" rel="noopener">flink-sql-connector-hive.jar下载地址</a></li></ul><ul><li>启动命令行：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$FLINK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">./sql-client.sh embedded \</span><br><span class="line">    -j /usr/<span class="built_in">local</span>/external/iceberg-flink-runtime-0.10.0.jar \</span><br><span class="line">    -j /usr/<span class="built_in">local</span>/external/flink-sql-connector-hive-2.2.0_2.11-1.11.2.jar \</span><br><span class="line">    shell</span><br></pre></td></tr></table></figure><p><img src="%E6%89%93%E5%BC%80SQL_Client.png" alt></p><h4 id="创建-Iceberg-的-Catalog"><a href="#创建-Iceberg-的-Catalog" class="headerlink" title="创建 Iceberg 的 Catalog"></a>创建 Iceberg 的 Catalog</h4><h5 id="创建-Hive-Catalog"><a href="#创建-Hive-Catalog" class="headerlink" title="创建 Hive Catalog"></a>创建 Hive Catalog</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">CATALOG</span> hive_catalog <span class="keyword">WITH</span>(</span><br><span class="line"><span class="string">'type'</span>=<span class="string">'iceberg'</span>,</span><br><span class="line"><span class="string">'catalog-type'</span>=<span class="string">'hive'</span>,</span><br><span class="line"><span class="string">'uri'</span>=<span class="string">'thrift://localhost:9083'</span>,</span><br><span class="line"><span class="string">'clients'</span>=<span class="string">'5'</span>,</span><br><span class="line"><span class="string">'property-version'</span>=<span class="string">'1'</span>,</span><br><span class="line"><span class="string">'warehouse'</span>=<span class="string">'hdfs://localhost:9000/user/hive/warehouse'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>可能出现以下报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.</span><br><span class="line">at org.apache.flink.table.client.SqlClient.main(SqlClient.java:213)</span><br><span class="line">Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:870)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:227)</span><br><span class="line">at org.apache.flink.table.client.SqlClient.start(SqlClient.java:108)</span><br><span class="line">at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)</span><br><span class="line">Caused by: java.lang.VerifyError: Stack map does not match the one at exception handler 69</span><br><span class="line">Exception Details:</span><br><span class="line">  Location:</span><br><span class="line">    org/apache/iceberg/hive/HiveCatalog.loadNamespaceMetadata(Lorg/apache/iceberg/catalog/Namespace;)Ljava/util/Map; @69: astore_2</span><br><span class="line">  Reason:</span><br><span class="line">    Type &apos;org/apache/hadoop/hive/metastore/api/NoSuchObjectException&apos; (current frame, stack[0]) is not assignable to &apos;org/apache/thrift/TException&apos; (stack map, stack[0])</span><br><span class="line">  Current Frame:</span><br><span class="line">    bci: @26</span><br><span class="line">    flags: &#123; &#125;</span><br><span class="line">    locals: &#123; &apos;org/apache/iceberg/hive/HiveCatalog&apos;, &apos;org/apache/iceberg/catalog/Namespace&apos; &#125;</span><br><span class="line">    stack: &#123; &apos;org/apache/hadoop/hive/metastore/api/NoSuchObjectException&apos; &#125;</span><br><span class="line">  Stackmap Frame:</span><br><span class="line">    bci: @69</span><br><span class="line">    flags: &#123; &#125;</span><br><span class="line">    locals: &#123; &apos;org/apache/iceberg/hive/HiveCatalog&apos;, &apos;org/apache/iceberg/catalog/Namespace&apos; &#125;</span><br><span class="line">    stack: &#123; &apos;org/apache/thrift/TException&apos; &#125;</span><br><span class="line">  Bytecode:</span><br><span class="line">    0x0000000: 2a2b b700 759a 0015 bb00 c759 12c9 04bd</span><br><span class="line">    0x0000010: 00cb 5903 2b53 b700 cebf 2ab4 0038 2bba</span><br><span class="line">    0x0000020: 0236 0000 b600 9ac0 0238 4d2a 2cb7 023c</span><br><span class="line">    0x0000030: 4eb2 00bd 1302 3e2b 2db9 0204 0100 b900</span><br><span class="line">    0x0000040: c504 002d b04d bb00 c759 2c12 c904 bd00</span><br><span class="line">    0x0000050: cb59 032b 53b7 0229 bf4d bb00 d059 bb00</span><br><span class="line">    0x0000060: d259 b700 d313 022b b600 d92b b600 dc13</span><br><span class="line">    0x0000070: 01a4 b600 d9b6 00e0 2cb7 00e3 bf4d b800</span><br><span class="line">    0x0000080: 40b6 00e6 bb00 d059 bb00 d259 b700 d313</span><br><span class="line">    0x0000090: 022d b600 d92b b600 dc13 01a4 b600 d9b6</span><br><span class="line">    0x00000a0: 00e0 2cb7 00e3 bf                      </span><br><span class="line">  Exception Handler Table:</span><br><span class="line">    bci [26, 68] =&gt; handler: 69</span><br><span class="line">    bci [26, 68] =&gt; handler: 69</span><br><span class="line">    bci [26, 68] =&gt; handler: 89</span><br><span class="line">    bci [26, 68] =&gt; handler: 125</span><br><span class="line">  Stackmap Table:</span><br><span class="line">    same_frame(@26)</span><br><span class="line">    same_locals_1_stack_item_frame(@69,Object[#111])</span><br><span class="line">    same_locals_1_stack_item_frame(@89,Object[#111])</span><br><span class="line">    same_locals_1_stack_item_frame(@125,Object[#113])</span><br><span class="line"></span><br><span class="line">at org.apache.iceberg.flink.CatalogLoader$HiveCatalogLoader.loadCatalog(CatalogLoader.java:95)</span><br><span class="line">at org.apache.iceberg.flink.FlinkCatalog.&lt;init&gt;(FlinkCatalog.java:104)</span><br><span class="line">at org.apache.iceberg.flink.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:132)</span><br><span class="line">at org.apache.iceberg.flink.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:122)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.createCatalog(ExecutionContext.java:378)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$null$5(ExecutionContext.java:626)</span><br><span class="line">at java.util.HashMap.forEach(HashMap.java:1289)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:625)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:264)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:624)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:523)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:183)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:136)</span><br><span class="line">at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:859)</span><br></pre></td></tr></table></figure><p>解决办法：</p><ol><li><p>iceberg-flink-runtime-0.10.0.jar、flink-sql-connector-hive-2.2.0_2.11-1.11.2.jar 不要放在 $FLINK_HOME/lib 目录下</p></li><li><p>打开 sql-client.sh 文件，在 jar 包启动的地方加上 -noverify 跳过字节码校验。<br><img src="%E5%88%9B%E5%BB%BAhive_catalog%E6%8A%A5%E9%94%99%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%951.png" alt></p></li></ol><p>type：表示 flink-connector 的连接类型<br>catalog-type：表示创建 catalog 的类型。目前支持 hive 和 hadoop 两种类型<br>uri：Hive metastore 的 thrift URI 地址<br>clients：Hive metastore 客户端线程池大小，默认值为2<br>property-version：Connector 的属性值版本号，当前版本为1，这个值主要用来确保 iceberg connector 的 property 向后兼容</p><p><img src="%E5%88%9B%E5%BB%BAhive_catalog%E6%88%90%E5%8A%9F.png" alt></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">USE</span> <span class="keyword">CATALOG</span> hive_catalog;</span><br></pre></td></tr></table></figure><h5 id="创建-Hadoop-Catalog"><a href="#创建-Hadoop-Catalog" class="headerlink" title="创建 Hadoop Catalog"></a>创建 Hadoop Catalog</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">CATALOG</span> hadoop_catalog <span class="keyword">WITH</span>(</span><br><span class="line"><span class="string">'type'</span>=<span class="string">'iceberg'</span>,</span><br><span class="line"><span class="string">'catalog-type'</span>=<span class="string">'hadoop'</span>,</span><br><span class="line"><span class="string">'warehouse'</span>=<span class="string">'hdfs://localhost:9000/warehouse/path'</span>,</span><br><span class="line"><span class="string">'property-version'</span>=<span class="string">'1'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">USE</span> <span class="keyword">CATALOG</span> hadoop_catalog;</span><br></pre></td></tr></table></figure><p>warehouse：表示 iceberg 的 metadata 的 data 数据存放的文件路径。</p><p><img src="%E5%88%9B%E5%BB%BAhadoop_catalog%E6%88%90%E5%8A%9F.png" alt></p><h5 id="创建-Custom-Catalog"><a href="#创建-Custom-Catalog" class="headerlink" title="创建 Custom Catalog"></a>创建 Custom Catalog</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">CATALOG</span> my_catalog <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'type'</span>=<span class="string">'iceberg'</span>,</span><br><span class="line">  <span class="string">'catalog-impl'</span>=<span class="string">'com.my.custom.CatalogImpl'</span>,</span><br><span class="line">  <span class="string">'my-additional-catalog-config'</span>=<span class="string">'my-value'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><h5 id="通过-YAML-统一新增-Catalog"><a href="#通过-YAML-统一新增-Catalog" class="headerlink" title="通过 YAML 统一新增 Catalog"></a>通过 YAML 统一新增 Catalog</h5><p>修改 sql-client-defaults.yaml 文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">catalogs:</span>  <span class="comment"># empty list</span></span><br><span class="line"><span class="attr">   - name:</span> <span class="string">hadoop_catalog</span></span><br><span class="line"><span class="attr">     type:</span> <span class="string">iceberg</span></span><br><span class="line"><span class="attr">     catalog-type:</span> <span class="string">hadoop</span></span><br><span class="line"><span class="attr">     warehouse:</span> <span class="attr">hdfs://localhost:9000/warehouse/path</span></span><br><span class="line"><span class="attr">     property-version:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">   - name:</span> <span class="string">hive_catalog</span></span><br><span class="line"><span class="attr">     type:</span> <span class="string">iceberg</span></span><br><span class="line"><span class="attr">     catalog-type:</span> <span class="string">hive</span></span><br><span class="line"><span class="attr">     uri:</span> <span class="attr">thrift://localhost:9083</span></span><br><span class="line"><span class="attr">     clients:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">     warehouse:</span> <span class="attr">hdfs://localhost:9000/user/hive/warehouse</span></span><br><span class="line"><span class="attr">     property-version:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> iceberg_db;</span><br><span class="line"></span><br><span class="line"><span class="keyword">USE</span> <span class="string">`iceberg_db`</span>;</span><br></pre></td></tr></table></figure><p><img src="%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%88%90%E5%8A%9F.png" alt></p><h4 id="创建-Iceberg-表"><a href="#创建-Iceberg-表" class="headerlink" title="创建 Iceberg 表"></a>创建 Iceberg 表</h4><ul><li>首先要进入到指定的 catalog 和 database 目录下</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">USE</span> <span class="keyword">CATALOG</span> hive_catalog;</span><br><span class="line"></span><br><span class="line"><span class="keyword">USE</span> iceberg_db;</span><br></pre></td></tr></table></figure><ul><li>创建非分区表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建 Iceberg 非分区表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sample_iceberg (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">'unique id'</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="keyword">STRING</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入测试数据1</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sample_iceberg <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="string">'test1'</span>);</span><br><span class="line"></span><br><span class="line">[INFO] Submitting SQL <span class="keyword">update</span> <span class="keyword">statement</span> <span class="keyword">to</span> the cluster...</span><br><span class="line">[INFO] <span class="keyword">Table</span> <span class="keyword">update</span> <span class="keyword">statement</span> has been successfully submitted <span class="keyword">to</span> the cluster:</span><br><span class="line">Job <span class="keyword">ID</span>: <span class="number">79</span>f18eee231895abd2ab15fbd0641ade</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入测试数据2</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sample_iceberg <span class="keyword">VALUES</span> (<span class="number">2</span>,<span class="string">'test2'</span>);</span><br><span class="line"></span><br><span class="line">[INFO] Submitting SQL <span class="keyword">update</span> <span class="keyword">statement</span> <span class="keyword">to</span> the cluster...</span><br><span class="line">[INFO] <span class="keyword">Table</span> <span class="keyword">update</span> <span class="keyword">statement</span> has been successfully submitted <span class="keyword">to</span> the cluster:</span><br><span class="line">Job <span class="keyword">ID</span>: c0afa452656f42de5df6bc745fbaf022</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询已插入的数据</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> sample_iceberg;</span><br></pre></td></tr></table></figure><p><img src="%E6%9F%A5%E8%AF%A2Iceberg%E9%9D%9E%E5%88%86%E5%8C%BA%E8%A1%A8%E6%95%B0%E6%8D%AE.png" alt></p><ul><li>创建分区表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建 Iceberg 分区表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sample_iceberg_partition (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">'unique id'</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="keyword">STRING</span></span><br><span class="line">) </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">data</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入测试数据1</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sample_iceberg_partition <span class="keyword">PARTITION</span>(<span class="keyword">data</span>=<span class="string">'city'</span>) <span class="keyword">SELECT</span> <span class="number">86</span>;</span><br><span class="line"></span><br><span class="line">[INFO] Submitting SQL <span class="keyword">update</span> <span class="keyword">statement</span> <span class="keyword">to</span> the cluster...</span><br><span class="line">[INFO] <span class="keyword">Table</span> <span class="keyword">update</span> <span class="keyword">statement</span> has been successfully submitted <span class="keyword">to</span> the cluster:</span><br><span class="line">Job <span class="keyword">ID</span>: <span class="number">9</span>ff0581b9b16ee01c0c849c50608f1b2</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询已插入的数据</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> sample_iceberg_partition;</span><br></pre></td></tr></table></figure><p><img src="%E6%9F%A5%E8%AF%A2Iceberg%E5%88%86%E5%8C%BA%E8%A1%A8%E6%95%B0%E6%8D%AE.png" alt></p><ul><li>创建带 WITH 参数的表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sample_iceberg_connector (</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">'unique id'</span>,</span><br><span class="line"><span class="keyword">data</span> <span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span>=<span class="string">'iceberg'</span>,</span><br><span class="line">  <span class="string">'catalog-type'</span>=<span class="string">'hive'</span>,</span><br><span class="line">  <span class="string">'uri'</span>=<span class="string">'thrift://localhost:9083'</span>,</span><br><span class="line">  <span class="string">'clients'</span>=<span class="string">'5'</span>,</span><br><span class="line">  <span class="string">'property-version'</span>=<span class="string">'1'</span>,</span><br><span class="line">  <span class="string">'warehouse'</span>=<span class="string">'hdfs://localhost:9000/warehouse/path'</span>,</span><br><span class="line"><span class="string">'write.format.default'</span>=<span class="string">'ORC'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><img src="%E5%88%9B%E5%BB%BAIceberg%E8%A1%A8%E6%88%90%E5%8A%9F.png" alt></p><p>支持 PARTITION BY 子句，用来设置分区字段<br>支持 COMMENT ‘table comment’ 子句<br>支持 WITH(‘key’=’value’,…) 子句，用来设置 table 级别的配置属性</p><p>暂不支持 computed column<br>暂不支持 primary key<br>暂不支持定义 watermark</p><h4 id="Flink-SQL-写入"><a href="#Flink-SQL-写入" class="headerlink" title="Flink SQL 写入"></a>Flink SQL 写入</h4><ul><li>修改 flink-conf.yaml 配置文件，配置 checkpoint<br>因为 flink 提交 Iceberg 的信息是在每次 checkpoint 时提交的，所以需要开启 checkpoint</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启 checkpoint</span></span><br><span class="line"><span class="string">state.backend:</span> <span class="string">filesystem</span></span><br><span class="line"><span class="string">state.backend.async:</span> <span class="literal">true</span></span><br><span class="line"><span class="string">state.backend.fs.memory-threshold:</span> <span class="number">1024</span></span><br><span class="line"><span class="string">state.checkpoints.dir:</span> <span class="attr">hdfs://localhost:9000/flink-checkpoints</span></span><br><span class="line"><span class="string">state.savepoints.dir:</span> <span class="attr">hdfs://localhost:9000/flink-checkpoints</span></span><br><span class="line"><span class="string">state.backend.incremental:</span> <span class="literal">false</span></span><br><span class="line"><span class="string">state.backend.local-recovery:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># state.checkpoints.num-retained: 1</span></span><br><span class="line"><span class="string">taskmanager.state.local.root-dirs:</span> <span class="string">none</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># checkpoint 间隔时间</span></span><br><span class="line"><span class="string">execution.checkpointing.interval:</span> <span class="number">10</span><span class="string">s</span></span><br><span class="line"><span class="comment"># checkpoint 失败容忍次数</span></span><br><span class="line"><span class="string">execution.checkpointing.tolerable-failed-checkpoints:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure><ul><li>首先要进入到指定的 catalog 和 database 目录下</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">USE</span> <span class="keyword">CATALOG</span> hive_catalog;</span><br><span class="line"></span><br><span class="line"><span class="keyword">USE</span> iceberg_db;</span><br></pre></td></tr></table></figure><ul><li>创建一个 iceberg 的 sink connector</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_sink_iceberg (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">'unique id'</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span>=<span class="string">'iceberg'</span>,</span><br><span class="line">  <span class="string">'catalog-type'</span>=<span class="string">'hive'</span>,</span><br><span class="line">  <span class="string">'uri'</span>=<span class="string">'thrift://localhost:9083'</span>,</span><br><span class="line">  <span class="string">'clients'</span>=<span class="string">'5'</span>,</span><br><span class="line">  <span class="string">'property-version'</span>=<span class="string">'1'</span>,</span><br><span class="line">  <span class="string">'warehouse'</span>=<span class="string">'hdfs://localhost:9000/warehouse/path'</span>,</span><br><span class="line">  <span class="string">'write.format.default'</span>=<span class="string">'ORC'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li>创建一个 datagen 的 source connector</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_source_datagen (</span><br><span class="line"> userid <span class="built_in">INT</span>,</span><br><span class="line"> f_random_str <span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">'connector'</span>=<span class="string">'datagen'</span>,</span><br><span class="line"> <span class="string">'rows-per-second'</span>=<span class="string">'5'</span>,</span><br><span class="line"> <span class="string">'fields.userid.kind'</span>=<span class="string">'random'</span>,</span><br><span class="line"> <span class="string">'fields.userid.min'</span>=<span class="string">'1'</span>,</span><br><span class="line"> <span class="string">'fields.userid.max'</span>=<span class="string">'1000'</span>,</span><br><span class="line"> <span class="string">'fields.f_random_str.length'</span>=<span class="string">'10'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询 source 数据，会有源源不断的数据输出</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> table_source_datagen;</span><br></pre></td></tr></table></figure><ul><li>通过设置 execution.type 来切换提交流作业和批作业</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 流式</span></span><br><span class="line"><span class="keyword">SET</span> execution.type = streaming;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 批</span></span><br><span class="line"><span class="keyword">SET</span> execution.type = batch;</span><br></pre></td></tr></table></figure><ul><li>借助流式作业（默认）写入数据到 Apache Iceberg 表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> table_sink_iceberg <span class="keyword">SELECT</span> userid,f_random_str <span class="keyword">FROM</span> table_source_datagen;</span><br><span class="line"></span><br><span class="line">[INFO] Submitting SQL <span class="keyword">update</span> <span class="keyword">statement</span> <span class="keyword">to</span> the cluster...</span><br><span class="line">[INFO] <span class="keyword">Table</span> <span class="keyword">update</span> <span class="keyword">statement</span> has been successfully submitted <span class="keyword">to</span> the cluster:</span><br><span class="line">Job <span class="keyword">ID</span>: <span class="number">52</span>c57f566aa6da61d46ec2fc73b74b59</span><br></pre></td></tr></table></figure><ul><li>查询 sink 到 iceberg 表里的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> table_sink_iceberg;</span><br></pre></td></tr></table></figure><p><img src="%E6%9F%A5%E8%AF%A2Iceberg%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5%E7%9A%84%E6%95%B0%E6%8D%AE.png" alt></p><h4 id="DataStream-写入"><a href="#DataStream-写入" class="headerlink" title="DataStream 写入"></a>DataStream 写入</h4><h5 id="批量读取"><a href="#批量读取" class="headerlink" title="批量读取"></a>批量读取</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadooptable(<span class="string">"hdfs://nn:8020/warehouse/path"</span>);</span><br><span class="line">DataStream&lt;RowData&gt; batch = FlinkSource.forRowData()</span><br><span class="line">     .env(env)</span><br><span class="line">     .tableLoader(loader)</span><br><span class="line">     .streaming(<span class="keyword">false</span>)</span><br><span class="line">     .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print all records to stdout.</span></span><br><span class="line">batch.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Submit and execute this batch read job.</span></span><br><span class="line">env.execute(<span class="string">"Test Iceberg Batch Read"</span>);</span><br></pre></td></tr></table></figure><h5 id="流式读取"><a href="#流式读取" class="headerlink" title="流式读取"></a>流式读取</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadoopTable(<span class="string">"hdfs://nn:8020/warehouse/path"</span>);</span><br><span class="line">DataStream&lt;RowData&gt; stream = FlinkSource.forRowData()</span><br><span class="line">     .env(env)</span><br><span class="line">     .tableLoader(loader)</span><br><span class="line">     .streaming(<span class="keyword">true</span>)</span><br><span class="line">     .startSnapshotId(<span class="number">3821550127947089987L</span>)</span><br><span class="line">     .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print all records to stdout.</span></span><br><span class="line">stream.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Submit and execute this streaming read job.</span></span><br><span class="line">env.execute(<span class="string">"Test Iceberg streaming Read"</span>);</span><br></pre></td></tr></table></figure><h5 id="追加写入"><a href="#追加写入" class="headerlink" title="追加写入"></a>追加写入</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;RowData&gt; input = ... ;</span><br><span class="line">Configuration hadoopConf = <span class="keyword">new</span> Configuration();</span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadoopTable(<span class="string">"hdfs://nn:8020/warehouse/path"</span>, hadoopConf);</span><br><span class="line"></span><br><span class="line">FlinkSink.forRowData(input)</span><br><span class="line">    .tableLoader(tableLoader)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"Test Iceberg DataStream"</span>);</span><br></pre></td></tr></table></figure><h5 id="覆盖写入"><a href="#覆盖写入" class="headerlink" title="覆盖写入"></a>覆盖写入</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;RowData&gt; input = ... ;</span><br><span class="line">Configuration hadoopConf = <span class="keyword">new</span> Configuration();</span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadoopTable(<span class="string">"hdfs://nn:8020/warehouse/path"</span>, hadoopConf);</span><br><span class="line"></span><br><span class="line">FlinkSink.forRowData(input)</span><br><span class="line">    .tableLoader(tableLoader)</span><br><span class="line">    .overwrite(<span class="keyword">true</span>)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"Test Iceberg DataStream"</span>);</span><br></pre></td></tr></table></figure><h4 id="修改表属性"><a href="#修改表属性" class="headerlink" title="修改表属性"></a>修改表属性</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="keyword">sample</span> <span class="keyword">SET</span> (<span class="string">'write.format.default'</span>=<span class="string">'avro'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="keyword">sample</span> <span class="keyword">RENAME</span> <span class="keyword">TO</span> <span class="string">`hive_catalog.default.new_sample`</span>;</span><br></pre></td></tr></table></figure><ul><li>Flink SQL 目前支持修改 iceberg 表的相关属性</li><li>Flink SQL 暂不支持添加列、修改列、删除列的操作，但可以通过 Iceberg Java API 来完成</li></ul><h4 id="删除表、库和-Catalog"><a href="#删除表、库和-Catalog" class="headerlink" title="删除表、库和 Catalog"></a>删除表、库和 Catalog</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">--  删除表</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">sample</span>；</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除库</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> test_db;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除 catalog</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">CATALOG</span> hive_catalog;</span><br></pre></td></tr></table></figure><h3 id="Flink-Sink-写入原理"><a href="#Flink-Sink-写入原理" class="headerlink" title="Flink Sink 写入原理"></a>Flink Sink 写入原理</h3><p><img src="Flink_Sink%E5%8E%9F%E7%90%86.png" alt></p><ul><li><p>IcebergStreamWriter<br>主要用来写入记录到对应的 avro、parquet、orc 文件，生成一个对应的 Iceberg datafile，并发送给下游算子。</p></li><li><p>IcebergFilesCommitter<br>为每个 checkpointId 维护了一个 datafile 文件列表，即 Map&lt;Long,List<datafile>&gt;，这样即使中间有某个 checkpoint 的 transaction 提交失败了，<br>它的 datafile 文件仍然维护在 State 中，依然可以通过后续的 checkpoint 来提交数据到 Iceberg 表中。</datafile></p></li></ul><p><img src="flink_sink_state%E8%AE%BE%E8%AE%A1.png" alt></p><p>Flink Sink State 改进：</p><ol><li><p>多个不同的 Flink Job 写入同一个 Iceberg 表时，如何保证写入数据的正确性？</p></li><li><p>在云端环境下，metastore 所在的数据中心和日志数据中心是两个数据中心。这时候，可能导致 Commit Transaction 到 Iceberg 经常失败的现象，长期失败会导致 State 膨胀。<br>如何解决这个问题。</p></li></ol><h3 id="Flink-Sink-小文件处理"><a href="#Flink-Sink-小文件处理" class="headerlink" title="Flink Sink 小文件处理"></a>Flink Sink 小文件处理</h3><ul><li><p>小数据量的合并<br>在 IcebergFilesCommitter 之后添加一个 Compactor 算子，用来实现少量小文件的频繁合并。</p></li><li><p>大数据量的合并<br>设计 Flink Batch 作业，对接 Iceberg 的 RewriteDataFilesAction 来实现表内的大数据合并。</p></li></ul><h2 id="社区规划"><a href="#社区规划" class="headerlink" title="社区规划"></a>社区规划</h2><ol><li><p>预计将在下个 Apache Iceberg Release 中支持：<br>a. Flink Sink 流式入湖和批量入湖<br>b. Flink Streaming Reader<br>c. Flink Batch Reader</p></li><li><p>Flink + Iceberg 对小文件的处理<br>a. Committer 任务之后添加 Compactor 算子，专门处理少量数据的 compaction<br>b. 设计 Flink 批任务来处理大数据量的 compaction</p></li><li><p>对接 Iceberg 的 row-level delete 功能<br>a. 通过 Flink 实现 CDC 日志的实时写入和分析<br>b. 通过 Flink 实现 CDC 日志的增量拉取<br>c. Flink + Iceberg 支持批量的数据更新</p></li><li><p>更加完善的 Flink SQL 支持<br>a. 更富的 Flink DDL 支持，例如支持增删改 column<br>b. 往 Flink 社区讨论支持 hidden partition</p></li><li><p>通过 SQL extension 来完成日常数据管理<br>a. 在 Icebeg 内实现 compaction 语法和命令<br>b. SQL 查看 history、snapshot、manifest、files 文件</p></li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.51cto.com/u_15023245/2619826" target="_blank" rel="noopener">基于 Flink+Iceberg 构建企业级实时数据湖 - 文章</a><br><a href="https://www.bilibili.com/video/BV14A411J7e6?p=4" target="_blank" rel="noopener">基于 Flink+Iceberg 构建企业级实时数据湖 - 视频</a><br><a href="https://www.yuque.com/deadwind/fusion/iqc65z?language=zh-cn" target="_blank" rel="noopener">Apache Iceberg调研</a><br><a href="https://www.bilibili.com/video/av929015385/" target="_blank" rel="noopener">Apache Iceberg 0.11.0 最新解读视频，胡争</a><br><a href="https://www.cnblogs.com/swordfall/p/14548574.html#auto_id_0" target="_blank" rel="noopener">Flink集成Iceberg简介</a><br><a href="https://github.com/apache/iceberg/blob/master/site/docs/flink.md" target="_blank" rel="noopener">Flink+Iceberg 社区使用文档</a><br><a href="https://blog.csdn.net/joniers/article/details/116590366" target="_blank" rel="noopener">Flink+iceberg 环境搭建及问题处理</a><br><a href="https://cloud.tencent.com/developer/article/1727735" target="_blank" rel="noopener">Flink集成数据湖之实时数据写入 Iceberg</a><br><a href="https://mp.weixin.qq.com/s/2MxyxOYrHi5HWmkKtXYjBg" target="_blank" rel="noopener">Flink + Iceberg 在去哪儿的实时数仓实践</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文为 Apache Iceberg 的入门学习笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="DataLake" scheme="http://yoursite.com/categories/DataLake/"/>
    
      <category term="Iceberg" scheme="http://yoursite.com/categories/DataLake/Iceberg/"/>
    
    
  </entry>
  
  <entry>
    <title>初识数据湖</title>
    <link href="http://yoursite.com/2021/01/20/%E5%88%9D%E8%AF%86%E6%95%B0%E6%8D%AE%E6%B9%96/"/>
    <id>http://yoursite.com/2021/01/20/初识数据湖/</id>
    <published>2021-01-20T08:19:22.000Z</published>
    <updated>2021-06-03T17:41:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>对于数据湖的入门者，本文记录一下对数据湖的初步认识。</p><a id="more"></a><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>数据湖是个什么概念呢？一般来说把一家企业产生的数据维护在一个平台内，这个平台我们称之为“数据湖”。</p><p>个人认为数据湖应该是一种不断演进中、可扩展的大数据存储、处理、分析的基础设施。以数据为导向，实现任意来源、任意规模、任意类型数据的全量获取、全量存储、多模式处理与全生命周期管理；并通过与各类外部异构数据源的交互集成，支持各类企业级应用。</p><p><img src="%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E6%9C%AC%E8%83%BD%E5%8A%9B%E7%A4%BA%E6%84%8F.png" alt></p><p>看下面这幅图，这个湖的数据来源多种多样，有的可能是结构化数据，有的可能是非结构化数据，有的甚至是二进制数据。有一拨人站在湖的入口，用设备在检测水质，这对应着数据湖上的流处理作业；有一拨抽水机从湖里抽水，这对应着数据湖的批处理作业；还有一批人在船头钓鱼或者在岸上捕鱼，这对应着数据科学家从数据湖中通过机器学习的手段提取价值。<br><img src="%E5%88%9D%E8%AF%86%E6%95%B0%E6%8D%AE%E6%B9%96.png" alt></p><p>总结起来，数据湖主要有 4 个方面的特点：</p><ul><li><p>存储原始数据，这些原始数据的来源非常丰富<br>结构化数据<br>半结构化数据<br>非结构化数据<br>二进制数据（图片等）</p></li><li><p>支持多种计算模型<br>批处理<br>流计算<br>交互式分析<br>机器学习</p></li><li><p>有完善的数据管理能力<br>能做到多种数据源接入<br>时间不同数据之间的连接<br>支持 Schema 管理<br>支持权限管理</p></li><li><p>灵活的底层存储<br>一般用 S3/OSS/HDFS 这种廉价的分布式文件系统<br>支持 Parquet/Avro/Orc 文件格式<br>支持数据缓存加速<br>满足对应场景的数据分析需求</p></li></ul><p>那么，开源的数据湖架构一般是什么样的呢？一般分为四层：</p><ul><li>最底层是廉价、弹性可扩展的分布式文件系统，云上用户 S3 和 OSS 这种对象存储用的更多一些，毕竟价格便宜很多；非云上用户一般采用自己维护的 HDFS。</li><li>第二层是数据加速层。提供本地数据缓存（多块 SSD）和元数据加速服务。数据湖架构是一个存储计算彻底分离的架构，如果所有的数据访问都远程读取文件系统上的数据，那么性能和成本开销很大。如果能把经常访问到的一些热点数据缓存在计算节点本地，这就非常自然的实现了冷热分离，一方面能获取到不错的本地读取性能，另一方面还节省了远程访问的带宽。这一层里边，通常会选择开源的 alluxio，或者选择阿里云上的 Jindofs。</li><li>第三层就是 Table Format 层，提供面向用户的主表级语义。要是把一些数据文件封装成一个有业务语义的 table，提供 ACID、snapshot、schema、partition 等表级别的语义。一般对应着开源的 Delta、Iceberg、Hudi 等项目。对一些用户来说，他们认为 Delta、Iceberg、Hudi 这些就是数据湖，其实这几个项目只是数据湖这个架构里边的一环，只是因为它们离用户最近，屏蔽了底层的很多细节，所以才会造成这样的误解。数据湖和数据中台一样，是一种架构理念，而不是专指某一个技术。</li><li>最上层就是不同计算场景的计算引擎了，满足不同的分析需求。开源的一般有 Spark、Flink、Hive、Presto、Hive MR 等，这一批计算引擎可以同时访问同一张数据湖的表。</li></ul><p><img src="%E6%95%B0%E6%8D%AE%E6%B9%96%E7%9A%84%E4%B8%80%E8%88%AC4%E5%B1%82%E6%9E%B6%E6%9E%84.png" alt></p><h3 id="数据湖-vs-数据仓库"><a href="#数据湖-vs-数据仓库" class="headerlink" title="数据湖 vs 数据仓库"></a>数据湖 vs 数据仓库</h3><p>比较数据来源于 AWS。</p><table><thead><tr><th>特性</th><th>数据仓库</th><th>数据湖</th></tr></thead><tbody><tr><td>数据</td><td>来自事务系统、运营数据库和业务线应用程序的关系数据</td><td>来自 IoT设备、网站、移动应用程序、社交媒体和企业应用程序的非关系和关系数据</td></tr><tr><td>Schema</td><td>设计在数据仓库实施之前（写入型Schema）</td><td>写入在分析时（读取型Schema）</td></tr><tr><td>性价比</td><td>更快查询结果会带来较高存储成本</td><td>更快查询结果只需较低存储成本</td></tr><tr><td>数据质量</td><td>可作为重要事实依据的高度监管数据</td><td>任何可以或无法进行监管的数据（例如原始数据）</td></tr><tr><td>用户</td><td>业务分析师</td><td>数据科学家、数据开发人员和业务分析师（使用监管数据）</td></tr><tr><td>分析</td><td>批处理报告、BI和可视化</td><td>机器学习、预测分析、数据发现和分析</td></tr></tbody></table><h3 id="Flink数据湖业务场景"><a href="#Flink数据湖业务场景" class="headerlink" title="Flink数据湖业务场景"></a>Flink数据湖业务场景</h3><h4 id="场景一-构建实时Data-Pipeline"><a href="#场景一-构建实时Data-Pipeline" class="headerlink" title="场景一: 构建实时Data Pipeline"></a>场景一: 构建实时Data Pipeline</h4><p><img src="%E5%9C%BA%E6%99%AF1_%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6Data_pipeline.png" alt></p><p>首先，Flink+Iceberg 最经典的一个应用场景就是构建实时的 Data Pipeline。业务端产生大量的日志数据，被导入到 Kafka 这样的消息队列，运用 Flink 流计算引擎执行 ETL 后，导入到 Apache Iceberg 原始表中。有一些业务场景需要直接跑分析作业来分析原始表的数据，而另外一些业务需要对数据做进一步的提纯，那么我们可以再起一个 Flink 作业从 Apache Iceberg 表中消费增量数据，经过处理之后写到到提纯之后的 Iceberg 表中。此时，可能还有业务需要对数据做进一步的聚合，那么我们继续在 Iceberg 表上启动增量 Flink 作业，将聚合之后的数据结果写入到聚合表中。</p><p>有人可能会提出质疑，这个场景好像通过 Flink+Hive 也能实现。Flink+Hive 的确可以实现，但写入到 Hive 的数据更多的是为了实现数仓的数据分析，而不是为了做增量拉取。一般来说，Hive 的增量写入是以 partition 为单位，时间是 15min 以上，Flink 长期高频率地写入会造成 partition 膨胀。而 Iceberg 允许实现 1min 甚至 30s 的增量写入，这样就可以大大提高了端到端数据的实时性，上层的分析作业可以看到更新的数据，下游的增量作业也可以读取到更新的数据。</p><ul><li><p>核心优势：<br>可以借助 Flink 实现数据 exactly-once 语义地入湖和出湖<br>新写入数据可以在 checkpoint 周期内可见<br>可以方便地构建 data pipeline，满足不同业务层的数据加工和分析需求</p></li><li><p>对比 Hive 方案：<br>hive 的增量写入以 partition 为单位，长期高频率的 checkpoint 写入，会导致 hive partition 的膨胀<br>本质上 hive 的增量写入和消费粒度都太大，实时性无法比肩 iceberg</p></li></ul><h4 id="场景二-CDC数据实时摄入摄出"><a href="#场景二-CDC数据实时摄入摄出" class="headerlink" title="场景二: CDC数据实时摄入摄出"></a>场景二: CDC数据实时摄入摄出</h4><p><img src="%E5%9C%BA%E6%99%AF%E4%BA%8C_CDC%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%91%84%E5%85%A5%E5%B0%84%E5%87%BA.png" alt></p><p>第二个经典的场景，就是可以用 Flink+Iceberg 分析来自 MySQL 等关系型数据库的 binlog 等。</p><p>一方面，Flink 已经原生的支持 CDC 数据解析，一条 binlog 数据通过 flink-cdc-connector 拉取之后，自动转换成 Flink runtime 能识别的 insert、delete、update_before、update_after 四种消息，供用户进一步的实时计算。</p><p>另一方面，Iceberg 已经较为完善的实现了 equality delete 功能，也就是用户定义好到删除的 record，直接写到 Iceberg 表内就可以删除对应的行，本身就是为了实现数据湖的流式删除。在 Iceberg 未来的版本中，用户将不需要设计任何额外的业务字段，不用写几行代码就可以完成 binlog 流式入湖到 Iceberg （社区 PR 已经提供了一个 Flink 写入 CDC 数据的原型）。</p><p>此外，CDC 数据成功入湖 Iceberg 之后，常见的计算引擎 Presto、Spark、Hive 等，都可以实时的读取到 Iceberg 表中最新的数据。</p><h3 id="场景三-近实时场景的流批统一"><a href="#场景三-近实时场景的流批统一" class="headerlink" title="场景三: 近实时场景的流批统一"></a>场景三: 近实时场景的流批统一</h3><p><img src="%E5%9C%BA%E6%99%AF%E4%B8%89_%E8%BF%91%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E7%9A%84%E6%89%B9%E6%B5%81%E7%BB%9F%E4%B8%80.png" alt></p><p>第三个经典场景是近实时场景的批流统一。在常用的 lambda 架构中，有一条实时链路和一条离线链路。实时链路一般由 Flink、Kafka、HBase 这些组件构建而成，离线链路一般会用到 Parquet、Spark等组件。这里边涉及到的计算组件和存储组件非常多，系统维护成本和业务开发成本都非常高。有很多场景，它们的实时性要求并没有那么苛刻，例如可以放松到分钟级别，这种场景我们称之为近实时场景。那么，我们是不是可以通过 Flink+Iceberg 来优化我们常用的 lambda 架构呢？</p><p><img src="%E5%9C%BA%E6%99%AF%E4%B8%89_%E8%BF%91%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E7%9A%84%E6%89%B9%E6%B5%81%E7%BB%9F%E4%B8%802.png" alt></p><p>我们可以用 Flink+Iceberg 把整个架构优化成上图所示。实时的数据通过 Flink 写入到 Iceberg 表中，近实时链路可以通过 Flink 计算增量数据，离线链路也可以通过 Flink 批计算读取整个快照做全局分析，得到对应的分析结果，供不同场景下的用户读取和分析。经过这种改进之后，我们把计算引擎统一成了 Flink，把存储组件统一成了 Iceberg，整个系统的维护开发成本大大降低。</p><h4 id="场景四-从-Iceberg-历史数据启动-Flink-任务"><a href="#场景四-从-Iceberg-历史数据启动-Flink-任务" class="headerlink" title="场景四: 从 Iceberg 历史数据启动 Flink 任务"></a>场景四: 从 Iceberg 历史数据启动 Flink 任务</h4><p><img src="%E5%9C%BA%E6%99%AF%E5%9B%9B_%E4%BB%8EIceberg%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE%E5%90%AF%E5%8A%A8Flink%E4%BB%BB%E5%8A%A1.png" alt></p><p>第四个场景，是采用 Iceberg 全量数据和 Kafka 的增量数据来 Bootstrap 新的 Flink 作业。我们现有的流作业在线上跑着，突然有一天某个业务方跑过来说，他们遇到一个新的计算场景，需要设计一个新的 Flink 作业，跑一遍去年一年的历史数据，跑完之后再对接到正在产生的 Kafka 增量数据。那么，这时候应该怎么办呢？</p><p>我们依然可以采用常见的 lamnda 架构，实时链路通过 Kafka -&gt; Flink -&gt; Iceberg 实时写入到数据湖，由于 Kafka 成本较高，保留最近 7 天数据即可，Iceberg 存储成本较低，可以存储全量的历史数据（按照 checkpoint 拆分成多个数据区间）。启动解析 Flink 作业的时候，只需要去拉取 Iceberg 的数据，跑完之后平滑的对接到 Kafka 数据即可。</p><h4 id="场景五-通过-Iceberg-数据来订正实时聚合结果"><a href="#场景五-通过-Iceberg-数据来订正实时聚合结果" class="headerlink" title="场景五: 通过 Iceberg 数据来订正实时聚合结果"></a>场景五: 通过 Iceberg 数据来订正实时聚合结果</h4><p><img src="%E5%9C%BA%E6%99%AF%E4%BA%94_%E9%80%9A%E8%BF%87Iceberg%E6%95%B0%E6%8D%AE%E6%9D%A5%E8%AE%A2%E6%AD%A3%E5%AE%9E%E6%97%B6%E8%81%9A%E5%90%88%E7%BB%93%E6%9E%9C.png" alt></p><p>第五个场景，和第四个场景类似。同样是在 lambda 架构下，实时链路由于事件丢失或者到达顺序的问题，可能导致流计算结果不一定完全准确，这时候一般都需要全量的历史数据来订正实时计算的结果。而 Iceberg 可以很好的充当这个角色，因为它可以高性价比的管理好历史数据。</p><h3 id="Why-Iceberg"><a href="#Why-Iceberg" class="headerlink" title="Why Iceberg"></a>Why Iceberg</h3><h4 id="Delta-vs-Hudi-vs-Iceberg"><a href="#Delta-vs-Hudi-vs-Iceberg" class="headerlink" title="Delta vs Hudi vs Iceberg"></a>Delta vs Hudi vs Iceberg</h4><p><img src="Delta_Hudi_Iceberg%E4%B8%89%E5%A4%A7%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93%E5%9B%BE.png" alt></p><table><thead><tr><th>Items</th><th>Open Source Delta</th><th>Apache Iceberg</th><th>Apache Hudi</th></tr></thead><tbody><tr><td>Open Source Time</td><td>2019/04/12</td><td>2018/11/06(incubation)</td><td>2019/01/17(incubation)</td></tr><tr><td>Github Star</td><td>2800+</td><td>692</td><td>1400+</td></tr><tr><td>Releases</td><td>5</td><td>5</td><td>48</td></tr><tr><td>ACID</td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td>Isolation Level</td><td>Write/Snapshot serialization</td><td>Write serialization</td><td>Snapshot serialization</td></tr><tr><td>Time Travel</td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td>Row-level DELETE(batch)</td><td>Yes</td><td>Ongoing</td><td>No</td></tr><tr><td>Row-level DELETE(streaming)</td><td>No</td><td>Ongoing</td><td>Yes</td></tr><tr><td>Abstracted Schema</td><td>No</td><td>Yes</td><td>No</td></tr><tr><td>Engine Pluggable</td><td>No</td><td>Yes</td><td>No</td></tr><tr><td>Open File Format</td><td>Yes</td><td>Yes</td><td>Yes(Data) + No(Log)</td></tr><tr><td>Filter push down</td><td>No</td><td>Yes</td><td>No</td></tr><tr><td>Auto-Compaction</td><td>No</td><td>Ongoing</td><td>Yes</td></tr><tr><td>Python support</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>File Encryption</td><td>No</td><td>Yes</td><td>No</td></tr></tbody></table><h4 id="Flink-为何选择-Apache-Iceberg"><a href="#Flink-为何选择-Apache-Iceberg" class="headerlink" title="Flink: 为何选择 Apache Iceberg"></a>Flink: 为何选择 Apache Iceberg</h4><ol><li>Iceberg 的设计和 Flink 数据湖的需求最匹配</li></ol><ul><li>完美解耦计算引擎和文件格式两层，便于接入多样化的计算引擎和文件格式</li><li>正确的完成了 Table Format这一层的功能需求</li><li>更容易成为 Table Format 层的事实标准</li></ul><ol start="2"><li>两个项目的长远规划相似</li></ol><ul><li>Apache Iceberg：打造流批一体的数据湖存储层</li><li>Apache Flink：打造流批一体的计算引擎</li><li>两者合力打造流批一体的数据湖架构</li></ul><ol start="3"><li>强大的社区资源</li></ol><ul><li>Apache Iceberg 始自 Netflix。Netflix 最早是 All In Cloud 的互联网巨头之一，也是最早在线上生产环境运行 Flink/Spark+Iceberg 这套数据湖方案的公司</li><li>支撑着 Apple、LinkedIn、Adobe、腾讯、网易等多家互联网巨头 PB 级的生产数据</li><li>严苛的文档审核、代码审核及测试设计。拥有来自其他 Apache项目的1个VP、7个PMC、4个Committer</li></ul><p>Delta 和 Hudi 跟 Spark 的代码路径绑定太深，尤其是写入路径。毕竟当时这两个项目设计之初，都多多少少把 Spark 作为它们默认的计算引擎了。而 Apache Iceberg 非常坚定，总值就是要做一个通用化设计的 Table Format。因此它完美的解耦了计算引擎和底下的存储系统，便于多样化计算引擎和文件格式，可以说正确的完成了数据湖架构中的 Table Format 这一层的实现。我们认为它也更容易成为 Table Format 层的开源事实标准。</p><p>另一方面，Apache Iceberg 正在朝着批流一体的数据湖存储层发展， manifest 和 snapshot 的设计，有效的隔离不同 transaction 的变更，非常方便批处理和增量计算。而我们知道 Apache Flink 已经是流批一体的计算引擎，可以说这二者的长远规划完美匹配，未来二者将合力打造流批一体的数据湖架构。</p><p>最后，我们还发现 Apache Iceberg 这个项目背后的社区资源非常丰富。在国外，Netflix、Apple、Linkedin、Adobe 等公司都有 PB 级别的生产数据运行在 Apache Iceberg 上；在国内，腾讯这样的巨头也有非常庞大的数据跑在 Apache Iceberg 之上，他们最大的一个业务每天有几十T的增量数据写入到 Iceberg。社区成员同样非常资深和多样化，拥有来自其他项目的 7 位 Apache PMC，1位 VP。在代码和设计的 review 上，也非常苛刻，一个稍微大点的 PR 涉及 100+ 的 comment 很常见，这些都使得 Apache Iceberg 的设计+代码质量比较高。</p><p>正是基于以上考虑，Apache Flink 最终选择了 Apache Iceberg 作为第一个数据湖接入项目。</p><h2 id="数据湖基本架构"><a href="#数据湖基本架构" class="headerlink" title="数据湖基本架构"></a>数据湖基本架构</h2><p><img src="%E6%95%B0%E6%8D%AE%E6%B9%96%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84%E7%A4%BA%E6%84%8F.png" alt></p><p>上图所示是一个数据湖系统的参考架构。对于一个典型的数据湖而言，它与大数据平台相同的地方在于它也具备超大规模数据所需的存储和计算能力，能提供多模式的数据处理能力；增强点在于数据湖提供了更为完善的数据管理能力，具体体现在：</p><ul><li><p>更强大的数据接入能力<br>对各类异构数据源的定义管理能力，以及对于外部数据源相关数据的抽取迁移能力，抽取迁移的数据包括外部数据源的元数据与实际存储的数据。</p></li><li><p>更强大的数据管理能力<br>可分为基本管理能力和扩展管理能力。基本管理能力包括对各类元数据的管理、数据访问控制、数据资产管理，是一个数据湖系统所必须的。扩展管理能力包括任务管理、流程编排以及数据质量、数据治理相关的能力。任务管理和流程编排主要用来管理、编排、调度、监测在数据湖系统中处理数据的各类任务，通常情况下，数据湖构建者会通过读取数据湖的相关元数据，来实现与数据湖系统的融合。而数据质量和数据治理则是更为复杂的问题，一般情况下，数据湖系统不会直接提供相关功能，但是会开放各类接口或元数据，供有能力的企业/组织与已有的数据治理软件集成或者做定制开发。</p></li><li><p>可共享的元数据<br>数据湖中的各类计算引擎会与数据湖中的数据深度融合，而融合的基础就是数据湖的元数据。好的数据湖系统，计算引擎在处理数据时，能从元数据中直接获取数据存储位置、数据格式、数据模式、数据分布等信息，然后直接进行数据处理，而无需进行人工/编程干预。更进一步，好的数据湖系统还可以对数据湖中的数据进行访问控制，控制的力度可以做到“库表列行”等不同级别。</p></li></ul><p> “集中式存储”更多的是业务概念上的集中，本质上希望一个企业组织内部能在一个明确统一的地方进行沉淀。事实上，数据湖的存储应该是一类可按需扩展的分布式文件系统，大多数数据湖实践中也是推荐采用 S3/OSS/HDFS 等分布式文件系统作为数据湖的统一存储。</p><h2 id="各个厂商的数据湖解决方案"><a href="#各个厂商的数据湖解决方案" class="headerlink" title="各个厂商的数据湖解决方案"></a>各个厂商的数据湖解决方案</h2><h3 id="AWS"><a href="#AWS" class="headerlink" title="AWS"></a>AWS</h3><p><img src="AWS%E6%95%B0%E6%8D%AE%E6%B9%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png" alt></p><h3 id="华为"><a href="#华为" class="headerlink" title="华为"></a>华为</h3><p><img src="%E5%8D%8E%E4%B8%BA%E6%95%B0%E6%8D%AE%E6%B9%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png" alt></p><h3 id="阿里云"><a href="#阿里云" class="headerlink" title="阿里云"></a>阿里云</h3><p><img src="%E9%98%BF%E9%87%8C%E4%BA%91%E6%95%B0%E6%8D%AE%E6%B9%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png" alt></p><h3 id="Azure"><a href="#Azure" class="headerlink" title="Azure"></a>Azure</h3><p><img src="Azure%E6%95%B0%E6%8D%AE%E6%B9%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/142756094" target="_blank" rel="noopener">“新晋网红”数据湖到底该如何理解？</a><br><a href="https://blog.51cto.com/u_15023245/2619826" target="_blank" rel="noopener">基于 Flink+Iceberg 构建企业级实时数据湖 - 文章</a><br><a href="https://www.bilibili.com/video/BV14A411J7e6?p=4" target="_blank" rel="noopener">基于 Flink+Iceberg 构建企业级实时数据湖 - 视频</a><br><a href="https://help.aliyun.com/document_detail/70378.html?spm=5176.21620736.J_5253785160.6.731041d2lYoMlv" target="_blank" rel="noopener">阿里云 云原生数据湖分析 DLA 帮助文档</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于数据湖的入门者，本文记录一下对数据湖的初步认识。&lt;/p&gt;
    
    </summary>
    
      <category term="DataLake" scheme="http://yoursite.com/categories/DataLake/"/>
    
    
  </entry>
  
  <entry>
    <title>晚熟的人</title>
    <link href="http://yoursite.com/2021/01/10/%E6%99%9A%E7%86%9F%E7%9A%84%E4%BA%BA/"/>
    <id>http://yoursite.com/2021/01/10/晚熟的人/</id>
    <published>2021-01-10T05:58:40.000Z</published>
    <updated>2021-05-25T03:58:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>就一直晚熟下去… 没有什么不好</p><a id="more"></a><p>于创作来说，不能过早地固步自封，不能过早的使自己的风格固化，<br>就是要不断地求新求变，努力试图突破自己，要不断的成长。<br>工作中，不必急功急利，保持学习热情，所有好运都会如期而至。<br>你成熟的越晚，说明创作创新的过程延续的越长。</p><p>于人性来说，本性善良的人都晚熟，并且是被”劣人”催熟的，后来虽然开窍了，<br>但也仍然善良与赤诚，不断地寻找同类，最后却成了最孤独的一个。<br>开窍了，就不能啥事都往外喷了，积蓄能量，厚积薄发。<br>你成熟的越晚，善良延续的越长，孤独而自省。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;就一直晚熟下去… 没有什么不好&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Apache Hudi</title>
    <link href="http://yoursite.com/2021/01/04/Apache-Hudi/"/>
    <id>http://yoursite.com/2021/01/04/Apache-Hudi/</id>
    <published>2021-01-03T17:17:31.000Z</published>
    <updated>2021-06-03T17:20:29.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="7a13309d1b18da83d67f5e813bed9016874b53ac96444e0b19c070fd7d3e3d3a">e3562aa68b223e1c03dd60eecfbb4ec09217adc47939c9407bb5553f1a72086f64fcd5c1b773522b005f8a6763d76e309f4c1ee334b15d045936244f7d81e4378a898228740a8f021aca889558b5fcf250e987f1d87bc7598458b2c26c14e64519492805f4dcb9d5de0b8407389e58af8749934a2d92e9049c61fa40228742e2f30b5136ed25d014a22b58798c9bdbb7fda6505bbf327818b5335b53972af3435eb718167710ebe128e7d22c09f0e40d</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Here&#39;s something encrypted, password is required to continue reading.
    
    </summary>
    
      <category term="DataLake" scheme="http://yoursite.com/categories/DataLake/"/>
    
      <category term="Hudi" scheme="http://yoursite.com/categories/DataLake/Hudi/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink源码剖析-flink-table-runtime-blink_TopN</title>
    <link href="http://yoursite.com/2020/09/30/Flink%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-flink-table-runtime-blink-TopN/"/>
    <id>http://yoursite.com/2020/09/30/Flink源码剖析-flink-table-runtime-blink-TopN/</id>
    <published>2020-09-30T08:35:54.000Z</published>
    <updated>2020-10-10T05:58:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文将基于 flink <code>release-1.11</code> 源码，简单分析下 TopN function 的实现。</p><a id="more"></a><p><img src="TopNFunction%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt></p><h2 id="AbstractTopNFunction"><a href="#AbstractTopNFunction" class="headerlink" title="AbstractTopNFunction"></a>AbstractTopNFunction</h2><p>AbstractTopNFunction 中有如下属性，定义 sortKey selector 和 comparator，rankEnd 相关参数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// we set default topN size to 100</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> DEFAULT_TOPN_SIZE = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The util to compare two sortKey equals to each other.</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 生成 sortKey 比较器实例类的工具类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> GeneratedRecordComparator generatedSortKeyComparator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * sortKey 比较器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> Comparator&lt;RowData&gt; sortKeyComparator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> generateUpdateBefore;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 是否输出排序序号</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> outputRankNumber;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 输入的数据类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> RowDataTypeInfo inputRowType;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * key selector，选择 RowData 中的哪一个字段来排序</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> KeySelector&lt;RowData, RowData&gt; sortKeySelector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * key 上下文，获取当前处理数据的 key</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> KeyContext keyContext;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 是否是固定的 TopN 集合大小</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> isConstantRankEnd;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rankStart 值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> rankStart;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rankEnd 在 RowData 中的下标</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> rankEndIndex;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rankEnd 值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">long</span> rankEnd;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * java.util.Function，从 RowData 的某一个位置获取 rankEnd</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> Function&lt;RowData, Long&gt; rankEndFetcher;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 记录 rankEnd，可能随着输入数据动态变化</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> ValueState&lt;Long&gt; rankEndState;</span><br><span class="line"><span class="keyword">private</span> Counter invalidCounter;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 当 TopN 需要输出排位序号时，会用到这个对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> JoinedRowData outputRow;</span><br><span class="line"></span><br><span class="line"><span class="comment">// metrics</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">long</span> hitCount = <span class="number">0L</span>;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">long</span> requestCount = <span class="number">0L</span>;</span><br></pre></td></tr></table></figure><p>AbstractTopNFunction 的 <code>open()</code> 方法主要从状态后端获取 rankEndState，并初始化类属性： </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">super</span>.open(parameters);</span><br><span class="line">initCleanupTimeState(<span class="string">"RankFunctionCleanupTime"</span>);</span><br><span class="line">outputRow = <span class="keyword">new</span> JoinedRowData();</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!isConstantRankEnd) &#123;</span><br><span class="line"><span class="comment">// 从状态后端读取当前 rankEnd 值</span></span><br><span class="line">ValueStateDescriptor&lt;Long&gt; rankStateDesc = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"rankEnd"</span>, Types.LONG);</span><br><span class="line">rankEndState = getRuntimeContext().getState(rankStateDesc);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// compile comparator</span></span><br><span class="line"><span class="comment">// classLoader 加载 key comparator 类</span></span><br><span class="line">sortKeyComparator = generatedSortKeyComparator.newInstance(getRuntimeContext().getUserCodeClassLoader());</span><br><span class="line"><span class="comment">// 把确定不需要的对象直接赋值为 null</span></span><br><span class="line">generatedSortKeyComparator = <span class="keyword">null</span>;</span><br><span class="line">invalidCounter = getRuntimeContext().getMetricGroup().counter(<span class="string">"topn.invalidTopSize"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// initialize rankEndFetcher</span></span><br><span class="line"><span class="keyword">if</span> (!isConstantRankEnd) &#123;</span><br><span class="line">LogicalType rankEndIdxType = inputRowType.getLogicalTypes()[rankEndIndex];</span><br><span class="line"><span class="keyword">switch</span> (rankEndIdxType.getTypeRoot()) &#123;</span><br><span class="line"><span class="keyword">case</span> BIGINT:</span><br><span class="line">rankEndFetcher = (RowData row) -&gt; row.getLong(rankEndIndex);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> INTEGER:</span><br><span class="line">rankEndFetcher = (RowData row) -&gt; (<span class="keyword">long</span>) row.getInt(rankEndIndex);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> SMALLINT:</span><br><span class="line">rankEndFetcher = (RowData row) -&gt; (<span class="keyword">long</span>) row.getShort(rankEndIndex);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">LOG.error(<span class="string">"variable rank index column must be long, short or int type, while input type is &#123;&#125;"</span>,</span><br><span class="line">rankEndIdxType.getClass().getName());</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(</span><br><span class="line"><span class="string">"variable rank index column must be long type, while input type is "</span> +</span><br><span class="line">rankEndIdxType.getClass().getName());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AbstractTopNFunction 的 <code>initRankEnd()</code> 方法根据 input row 来动态获取 rankEnd ：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Initialize rank end.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> row input record</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> rank end</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">initRankEnd</span><span class="params">(RowData row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (isConstantRankEnd) &#123;</span><br><span class="line"><span class="keyword">return</span> rankEnd;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">Long rankEndValue = rankEndState.value();</span><br><span class="line"><span class="keyword">long</span> curRankEnd = rankEndFetcher.apply(row);</span><br><span class="line"><span class="keyword">if</span> (rankEndValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">rankEnd = curRankEnd;</span><br><span class="line"><span class="comment">// 同步更新到状态后端</span></span><br><span class="line">rankEndState.update(rankEnd);</span><br><span class="line"><span class="keyword">return</span> rankEnd;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">rankEnd = rankEndValue;</span><br><span class="line"><span class="keyword">if</span> (rankEnd != curRankEnd) &#123;</span><br><span class="line"><span class="comment">// increment the invalid counter when the current rank end not equal to previous rank end</span></span><br><span class="line">invalidCounter.inc();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> rankEnd;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AbstractTopNFunction 的 <code>checkSortKeyInBufferRange()</code> 方法来判断 input row 是否应该被放到其 key 对应的 TopBuffer 中：</p><ol><li>将 input row 与 TopBuffer 中的最后一个 entry 比较，comparator 返回 true 则将 input row 丢到 TopBuffer 中；</li><li>comparator 返回 false，当前 TopBuffer 中的 entry 个数还没有达到默认的 TopN size，也将 input row 丢到 TopBuffer 中。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Checks whether the record should be put into the buffer.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> sortKey sortKey to test</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> buffer  buffer to add</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if the record should be put into the buffer.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">checkSortKeyInBufferRange</span><span class="params">(RowData sortKey, TopNBuffer buffer)</span> </span>&#123;</span><br><span class="line">Comparator&lt;RowData&gt; comparator = buffer.getSortKeyComparator();</span><br><span class="line">Map.Entry&lt;RowData, Collection&lt;RowData&gt;&gt; worstEntry = buffer.lastEntry();</span><br><span class="line"><span class="keyword">if</span> (worstEntry == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// return true if the buffer is empty. TopNBuffer 是空的，直接返回 true</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">RowData worstKey = worstEntry.getKey();</span><br><span class="line"><span class="comment">//执行 TopN 比较器</span></span><br><span class="line"><span class="keyword">int</span> compare = comparator.compare(sortKey, worstKey);</span><br><span class="line"><span class="keyword">if</span> (compare &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 如果满足条件，可以放到 TopNBuffer 中</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 到达的数据条数还没有达到默认的 TopN 大小 100，也可以放到 TopNBuffer 中</span></span><br><span class="line"><span class="keyword">return</span> buffer.getCurrentTopNum() &lt; getDefaultTopNSize();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>AbstractTopNFunction 的 <code>createOutputRow()</code> 方法用于构建 output row，区分带不带 rank 序号：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 构建 output row</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> inputRow input row</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rank     排位序号</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rowKind  描述一行 changelog 的行为种类</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> &#123;<span class="doctag">@link</span> RowData&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RowData <span class="title">createOutputRow</span><span class="params">(RowData inputRow, <span class="keyword">long</span> rank, RowKind rowKind)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (outputRankNumber) &#123;</span><br><span class="line"><span class="comment">// 需要输出 rank number</span></span><br><span class="line">GenericRowData rankRow = <span class="keyword">new</span> GenericRowData(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 第 0 个字段设置为排位序号，将 rank 专门放置在一个 RowData 中</span></span><br><span class="line">rankRow.setField(<span class="number">0</span>, rank);</span><br><span class="line"></span><br><span class="line">outputRow.replace(inputRow, rankRow);</span><br><span class="line">outputRow.setRowKind(rowKind);</span><br><span class="line"><span class="keyword">return</span> outputRow;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">inputRow.setRowKind(rowKind);</span><br><span class="line"><span class="keyword">return</span> inputRow;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="AppendOnlyTopNFunction"><a href="#AppendOnlyTopNFunction" class="headerlink" title="AppendOnlyTopNFunction"></a>AppendOnlyTopNFunction</h2><p>AppendOnlyTopNFunction 中有如下属性，状态后端 MapState 和本地堆内存 TopNBuffer 结合使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * sortKey 字段类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> RowDataTypeInfo sortKeyType;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * input row 的序列化类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> TypeSerializer&lt;RowData&gt; inputRowSer;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> cacheSize;</span><br><span class="line"></span><br><span class="line"><span class="comment">// a map state stores mapping from sort key to records list which is in topN</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * sortKey &lt;-&gt; 在 TopN 中的 RowData list</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> MapState&lt;RowData, List&lt;RowData&gt;&gt; dataState;</span><br><span class="line"></span><br><span class="line"><span class="comment">// the buffer stores mapping from sort key to records list, a heap mirror to dataState</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 当前 sortKey 对应的 TopNBuffer</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> TopNBuffer buffer;</span><br><span class="line"></span><br><span class="line"><span class="comment">// the kvSortedMap stores mapping from partition key to it's buffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * sortKey &lt;-&gt; TopNBuffer</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> Map&lt;RowData, TopNBuffer&gt; kvSortedMap;</span><br></pre></td></tr></table></figure><p>AppendOnlyTopNFunction 的 <code>open()</code> 方法中从状态后端中获取当前 key 的 TopN list：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">super</span>.open(parameters);</span><br><span class="line"><span class="comment">// LRU的缓存大小=总的缓存大小/topN的缓存大小</span></span><br><span class="line"><span class="keyword">int</span> lruCacheSize = Math.max(<span class="number">1</span>, (<span class="keyword">int</span>) (cacheSize / getDefaultTopNSize()));</span><br><span class="line"><span class="comment">// 根据 key 缓存 LRU list</span></span><br><span class="line">kvSortedMap = <span class="keyword">new</span> LRUMap&lt;&gt;(lruCacheSize);</span><br><span class="line">LOG.info(<span class="string">"Top&#123;&#125; operator is using LRU caches key-size: &#123;&#125;"</span>, getDefaultTopNSize(), lruCacheSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据 key 记录当前的 TopN list</span></span><br><span class="line"><span class="comment">// RowDataTypeInfo</span></span><br><span class="line">ListTypeInfo&lt;RowData&gt; valueTypeInfo = <span class="keyword">new</span> ListTypeInfo&lt;&gt;(inputRowType);</span><br><span class="line">MapStateDescriptor&lt;RowData, List&lt;RowData&gt;&gt; mapStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line"><span class="string">"data-state-with-append"</span>, sortKeyType, valueTypeInfo);</span><br><span class="line">dataState = getRuntimeContext().getMapState(mapStateDescriptor);</span><br><span class="line"></span><br><span class="line"><span class="comment">// metrics</span></span><br><span class="line">registerMetric(kvSortedMap.size() * getDefaultTopNSize());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AppendOnlyTopNFunction 的 <code>processElement()</code> 方法处理数据，判断当前 input row 是否可以丢到 TopNBuffer 中：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(RowData input, Context context, Collector&lt;RowData&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 获取当前时间，记录在上下文的计时器中</span></span><br><span class="line"><span class="keyword">long</span> currentTime = context.timerService().currentProcessingTime();</span><br><span class="line"><span class="comment">// register state-cleanup timer</span></span><br><span class="line">registerProcessingCleanupTimer(context, currentTime);</span><br><span class="line"></span><br><span class="line">initHeapStates();</span><br><span class="line">initRankEnd(input);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从输入的数据中抽取 sortKey</span></span><br><span class="line">RowData sortKey = sortKeySelector.getKey(input);</span><br><span class="line"><span class="comment">// check whether the sortKey is in the topN range</span></span><br><span class="line"><span class="comment">// 根据 sortKey 判断当前数据是否应该被放到 TopNBuffer 中</span></span><br><span class="line"><span class="keyword">if</span> (checkSortKeyInBufferRange(sortKey, buffer)) &#123;</span><br><span class="line"><span class="comment">// insert sort key into buffer</span></span><br><span class="line">buffer.put(sortKey, inputRowSer.copy(input));</span><br><span class="line">Collection&lt;RowData&gt; inputs = buffer.get(sortKey);</span><br><span class="line"><span class="comment">// update data state</span></span><br><span class="line"><span class="comment">// copy a new collection to avoid mutating state values, see CopyOnWriteStateMap,</span></span><br><span class="line"><span class="comment">// otherwise, the result might be corrupt.</span></span><br><span class="line"><span class="comment">// don't need to perform a deep copy, because RowData elements will not be updated</span></span><br><span class="line"><span class="comment">// 同步记录到 MapState 中</span></span><br><span class="line">dataState.put(sortKey, <span class="keyword">new</span> ArrayList&lt;&gt;(inputs));</span><br><span class="line"><span class="keyword">if</span> (outputRankNumber || hasOffset()) &#123;</span><br><span class="line"><span class="comment">// the without-number-algorithm can't handle topN with offset,</span></span><br><span class="line"><span class="comment">// so use the with-number-algorithm to handle offset</span></span><br><span class="line">processElementWithRowNumber(sortKey, input, out);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">processElementWithoutRowNumber(input, out);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AppendOnlyTopNFunction 的 <code>initHeapStates()</code> 是在处理 input row 之前，在堆内存中初始化 TopNBuffer，并将状态后端存储的 TopN list 设置到 TopNBuffer 中：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initHeapStates</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">requestCount += <span class="number">1</span>;</span><br><span class="line"><span class="comment">// 从 KeyContext 中获取当前的key</span></span><br><span class="line">RowData currentKey = (RowData) keyContext.getCurrentKey();</span><br><span class="line"><span class="comment">// 取出 key 对应的 TopNBuffer</span></span><br><span class="line">buffer = kvSortedMap.get(currentKey);</span><br><span class="line"><span class="keyword">if</span> (buffer == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// buffer 为 null，则为此 key 构建 TopNBuffer，为其设置 key comparator</span></span><br><span class="line">buffer = <span class="keyword">new</span> TopNBuffer(sortKeyComparator, ArrayList::<span class="keyword">new</span>);</span><br><span class="line">kvSortedMap.put(currentKey, buffer);</span><br><span class="line"><span class="comment">// restore buffer</span></span><br><span class="line"><span class="comment">// 读取 state 中记录的 TopN list，塞到这个 TopNBuffer 里</span></span><br><span class="line">Iterator&lt;Map.Entry&lt;RowData, List&lt;RowData&gt;&gt;&gt; iter = dataState.iterator();</span><br><span class="line"><span class="keyword">if</span> (iter != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">Map.Entry&lt;RowData, List&lt;RowData&gt;&gt; entry = iter.next();</span><br><span class="line">RowData sortKey = entry.getKey();</span><br><span class="line">List&lt;RowData&gt; values = entry.getValue();</span><br><span class="line"><span class="comment">// the order is preserved</span></span><br><span class="line">buffer.putAll(sortKey, values);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// buffer 不为 null，记录命中一次 TopNBuffer 缓存</span></span><br><span class="line">hitCount += <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AppendOnlyTopNFunction 的 <code>processElementWithoutRowNumber()</code> 方法是处理丢到 TopNBuffer 中的 input row，决定这条数据是否被 Delete ： </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">processElementWithoutRowNumber</span><span class="params">(RowData input, Collector&lt;RowData&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// remove retired element</span></span><br><span class="line"><span class="comment">// 当前 TopNBuffer 中缓存的数据条数大于 TopN 的 N</span></span><br><span class="line"><span class="keyword">if</span> (buffer.getCurrentTopNum() &gt; rankEnd) &#123;</span><br><span class="line">Map.Entry&lt;RowData, Collection&lt;RowData&gt;&gt; lastEntry = buffer.lastEntry();</span><br><span class="line">RowData lastKey = lastEntry.getKey();</span><br><span class="line">Collection&lt;RowData&gt; lastList = lastEntry.getValue();</span><br><span class="line">RowData lastElement = buffer.lastElement();</span><br><span class="line"><span class="keyword">int</span> size = lastList.size();</span><br><span class="line"><span class="comment">// remove last one</span></span><br><span class="line"><span class="keyword">if</span> (size &lt;= <span class="number">1</span>) &#123;</span><br><span class="line"><span class="comment">// 移除最后一个元素</span></span><br><span class="line">buffer.removeAll(lastKey);</span><br><span class="line">dataState.remove(lastKey);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 移除大于 TopN 的 N 之后的元素</span></span><br><span class="line">buffer.removeLast();</span><br><span class="line"><span class="comment">// last element has been removed from lastList, we have to copy a new collection</span></span><br><span class="line"><span class="comment">// for lastList to avoid mutating state values, see CopyOnWriteStateMap,</span></span><br><span class="line"><span class="comment">// otherwise, the result might be corrupt.</span></span><br><span class="line"><span class="comment">// don't need to perform a deep copy, because RowData elements will not be updated</span></span><br><span class="line"><span class="comment">// 更新状态后端</span></span><br><span class="line">dataState.put(lastKey, <span class="keyword">new</span> ArrayList&lt;&gt;(lastList));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (size == <span class="number">0</span> || input.equals(lastElement)) &#123;</span><br><span class="line"><span class="comment">// input 的数据和 TopNBuffer 中的最后一个元素相同，则直接返回</span></span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// lastElement shouldn't be null</span></span><br><span class="line">collectDelete(out, lastElement);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// it first appears in the TopN, send INSERT message</span></span><br><span class="line">collectInsert(out, input);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="AppendOnlyTopNFunctionTest"><a href="#AppendOnlyTopNFunctionTest" class="headerlink" title="AppendOnlyTopNFunctionTest"></a>AppendOnlyTopNFunctionTest</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Tests for &#123;<span class="doctag">@link</span> AppendOnlyTopNFunction&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AppendOnlyTopNFunctionTest</span> <span class="keyword">extends</span> <span class="title">TopNFunctionTestBase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> AbstractTopNFunction <span class="title">createFunction</span><span class="params">(RankType rankType, RankRange rankRange,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">boolean</span> generateUpdateBefore, <span class="keyword">boolean</span> outputRankNumber)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> AppendOnlyTopNFunction(minTime.toMilliseconds(),</span><br><span class="line">maxTime.toMilliseconds(),</span><br><span class="line">inputRowType,</span><br><span class="line">sortKeyComparator,</span><br><span class="line">sortKeySelector,</span><br><span class="line">rankType,</span><br><span class="line">rankRange,</span><br><span class="line">generateUpdateBefore,</span><br><span class="line">outputRankNumber,</span><br><span class="line">cacheSize);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testVariableRankRange</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER,</span><br><span class="line"><span class="comment">// 指定数据的第2个字段值为 rankEnd，动态指定 TopN 集合的大小</span></span><br><span class="line"><span class="keyword">new</span> VariableRankRange(<span class="number">1</span>),</span><br><span class="line"><span class="keyword">true</span>,</span><br><span class="line"><span class="comment">// 不用输出 topN 的排序序号</span></span><br><span class="line"><span class="keyword">false</span>);</span><br><span class="line"><span class="comment">// 将 TopNFunction 包装进 KeyedProcessOperator</span></span><br><span class="line">OneInputStreamOperatorTestHarness&lt;RowData, RowData&gt; testHarness = createTestHarness(func);</span><br><span class="line"><span class="comment">// 测试类准备工作</span></span><br><span class="line">testHarness.open();</span><br><span class="line"></span><br><span class="line"><span class="comment">// KeyedProcessOperator 作为 input operator 模拟处理数据</span></span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">12</span>)); <span class="comment">// 开始处理(book,2,12)，key 为 book，rankEnd 为 2，加入 TopN 集合</span></span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>)); <span class="comment">// 开始处理(book,2,19)，key 为 book，rankEnd 为 2，加入 TopN 集合</span></span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">11</span>)); <span class="comment">// 开始处理(book,2,11)，key 为 book，rankEnd 为 2，超出 TopN 集合容量，因此需要先删除 (book,2,19)，留下 2 个较小的</span></span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">33</span>)); <span class="comment">// 开始处理(fruit,1,33)，key 为 fruit，rankEnd 为 1，加入 TopN 集合</span></span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">44</span>)); <span class="comment">// 开始处理(fruit,1,44)，key 为 fruit，rankEnd 为 1，44 &gt; 33，直接过滤掉</span></span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">22</span>)); <span class="comment">// 开始处理(fruit,1,22)，key 为 fruit，rankEnd 为 1，超出 TopN 集合容量，因此需要先删除 (fruit,1,33)，留下 1 个较小的</span></span><br><span class="line">testHarness.close();</span><br><span class="line"></span><br><span class="line">ConcurrentLinkedQueue&lt;Object&gt; output = testHarness.getOutput();</span><br><span class="line"><span class="keyword">for</span> (Object o : output) &#123;</span><br><span class="line">StreamRecord streamRecord = (StreamRecord) o;</span><br><span class="line">System.out.println(<span class="string">"Output element -&gt; "</span> + streamRecord.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;Object&gt; expectedOutput = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="comment">// ("book", 2L, 12)</span></span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">12</span>));</span><br><span class="line"><span class="comment">// ("book", 2L, 19)</span></span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>));</span><br><span class="line"><span class="comment">// ("book", 2L, 11)</span></span><br><span class="line">expectedOutput.add(deleteRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>));</span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">11</span>));</span><br><span class="line"><span class="comment">// ("fruit", 1L, 33)</span></span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">33</span>));</span><br><span class="line"><span class="comment">// ("fruit", 1L, 44)</span></span><br><span class="line"><span class="comment">// ("fruit", 1L, 22)</span></span><br><span class="line">expectedOutput.add(deleteRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">33</span>));</span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">22</span>));</span><br><span class="line">assertorWithoutRowNumber</span><br><span class="line">.assertOutputEquals(<span class="string">"output wrong."</span>, expectedOutput, testHarness.getOutput());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNFunctionTestBase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// key 比较器的类加载工具类，生成一个 key 比较器实例</span></span><br><span class="line">    <span class="keyword">static</span> GeneratedRecordComparator sortKeyComparator = <span class="keyword">new</span> GeneratedRecordComparator(<span class="string">""</span>, <span class="string">""</span>, <span class="keyword">new</span> Object[<span class="number">0</span>]) &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1434685115916728955L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordComparator <span class="title">newInstance</span><span class="params">(ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// compare(RowData o1, RowData o2) 方法中比较 o1 和 o2 的第 0 个元素，从小到大比较</span></span><br><span class="line"><span class="keyword">return</span> IntRecordComparator.INSTANCE;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNFunctionTestBase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> sortKeyIdx = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// key 选择器，比较 RowData 中的第 2 位置的元素</span></span><br><span class="line">BinaryRowDataKeySelector sortKeySelector = <span class="keyword">new</span> BinaryRowDataKeySelector(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;sortKeyIdx&#125;,</span><br><span class="line">inputRowType.getLogicalTypes());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Output element -&gt; +I(book,2,12)   </span><br><span class="line">Output element -&gt; +I(book,2,19)   </span><br><span class="line">Output element -&gt; -D(book,2,19)    </span><br><span class="line">Output element -&gt; +I(book,2,11)   </span><br><span class="line">Output element -&gt; +I(fruit,1,33)  </span><br><span class="line">Output element -&gt; -D(fruit,1,33)  </span><br><span class="line">Output element -&gt; +I(fruit,1,22)</span><br></pre></td></tr></table></figure><h2 id="RetractableTopNFunction"><a href="#RetractableTopNFunction" class="headerlink" title="RetractableTopNFunction"></a>RetractableTopNFunction</h2><p>内部使用 TreeMap 进行 TopN 排序，可以对数据执行撤回操作，RowKind.UPDATE_BEFORE（-U）。</p><p>RetractableTopNFunction 中有如下属性，记录相同的 RowData 列表，使用 sortedMap 来进行 TopN 排序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// a map state stores mapping from sort key to records list</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * RowData &lt;-&gt;  相同的 RowData list，状态后端远程维护</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> MapState&lt;RowData, List&lt;RowData&gt;&gt; dataState;</span><br><span class="line"></span><br><span class="line"><span class="comment">// a sorted map stores mapping from sort key to records count</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * RowData &lt;-&gt; 对应的记录个数，ValueState 中记录有序的 RowData</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;SortedMap&lt;RowData, Long&gt;&gt; treeMap;</span><br></pre></td></tr></table></figure><p>RetractableTopNFunction 中的 <code>processElement()</code> 方法，按照数据的 RowKind 分别执行 emit 和 retract 操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(RowData input, Context ctx, Collector&lt;RowData&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">long</span> currentTime = ctx.timerService().currentProcessingTime();</span><br><span class="line"><span class="comment">// register state-cleanup timer</span></span><br><span class="line">registerProcessingCleanupTimer(ctx, currentTime);</span><br><span class="line">initRankEnd(input);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从状态后端中获取有序 RowData 的集合</span></span><br><span class="line">SortedMap&lt;RowData, Long&gt; sortedMap = treeMap.value();</span><br><span class="line"><span class="keyword">if</span> (sortedMap == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 如果为 null，则新建一个，指定 sortKey comparator</span></span><br><span class="line">sortedMap = <span class="keyword">new</span> TreeMap&lt;&gt;(sortKeyComparator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">RowData sortKey = sortKeySelector.getKey(input);</span><br><span class="line"><span class="comment">// RowKind.INSERT 或 RowKind.UPDATE_AFTER</span></span><br><span class="line"><span class="keyword">boolean</span> isAccumulate = RowDataUtil.isAccumulateMsg(input);</span><br><span class="line"><span class="comment">// erase row kind for further state accessing</span></span><br><span class="line">input.setRowKind(RowKind.INSERT);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (isAccumulate) &#123;</span><br><span class="line"><span class="comment">// update sortedMap，记录当前 sortKey 的记录数到状态后端</span></span><br><span class="line"><span class="keyword">if</span> (sortedMap.containsKey(sortKey)) &#123;</span><br><span class="line">sortedMap.put(sortKey, sortedMap.get(sortKey) + <span class="number">1</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">sortedMap.put(sortKey, <span class="number">1L</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// emit</span></span><br><span class="line"><span class="keyword">if</span> (outputRankNumber || hasOffset()) &#123;</span><br><span class="line"><span class="comment">// the without-number-algorithm can't handle topN with offset,</span></span><br><span class="line"><span class="comment">// so use the with-number-algorithm to handle offset</span></span><br><span class="line">emitRecordsWithRowNumber(sortedMap, sortKey, input, out);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">emitRecordsWithoutRowNumber(sortedMap, sortKey, input, out);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同步更新到状态后端</span></span><br><span class="line"><span class="comment">// update data state</span></span><br><span class="line">List&lt;RowData&gt; inputs = dataState.get(sortKey);</span><br><span class="line"><span class="keyword">if</span> (inputs == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// the sort key is never seen</span></span><br><span class="line">inputs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">&#125;</span><br><span class="line">inputs.add(input);</span><br><span class="line">dataState.put(sortKey, inputs);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// emit updates first，先输出 update 操作，-U 代表执行撤回操作</span></span><br><span class="line"><span class="keyword">if</span> (outputRankNumber || hasOffset()) &#123;</span><br><span class="line"><span class="comment">// the without-number-algorithm can't handle topN with offset,</span></span><br><span class="line"><span class="comment">// so use the with-number-algorithm to handle offset</span></span><br><span class="line">retractRecordWithRowNumber(sortedMap, sortKey, input, out);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">retractRecordWithoutRowNumber(sortedMap, sortKey, input, out);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// and then update sortedMap</span></span><br><span class="line"><span class="keyword">if</span> (sortedMap.containsKey(sortKey)) &#123;</span><br><span class="line"><span class="keyword">long</span> count = sortedMap.get(sortKey) - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">sortedMap.remove(sortKey);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">sortedMap.put(sortKey, count);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (sortedMap.isEmpty()) &#123;</span><br><span class="line"><span class="keyword">if</span> (lenient) &#123;</span><br><span class="line">LOG.warn(STATE_CLEARED_WARN_MSG);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(STATE_CLEARED_WARN_MSG);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(</span><br><span class="line"><span class="string">"Can not retract a non-existent record. This should never happen."</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 更新状态后端中记录的 sortedMap</span></span><br><span class="line">treeMap.update(sortedMap);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>RetractableTopNFunction 中的 <code>emitRecordsWithRowNumber()</code> 方法正常输出排序行：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">emitRecordsWithRowNumber</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">SortedMap&lt;RowData, Long&gt; sortedMap, RowData sortKey, RowData inputRow, Collector&lt;RowData&gt; out)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Iterator&lt;Map.Entry&lt;RowData, Long&gt;&gt; iterator = sortedMap.entrySet().iterator();</span><br><span class="line"><span class="keyword">long</span> currentRank = <span class="number">0L</span>;</span><br><span class="line">RowData currentRow = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">boolean</span> findsSortKey = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">while</span> (iterator.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">Map.Entry&lt;RowData, Long&gt; entry = iterator.next();</span><br><span class="line">RowData key = entry.getKey();</span><br><span class="line"><span class="keyword">if</span> (!findsSortKey &amp;&amp; key.equals(sortKey)) &#123;</span><br><span class="line">currentRank += entry.getValue();</span><br><span class="line">currentRow = inputRow;</span><br><span class="line"><span class="comment">// 从 sortedMap 中找到当前的 sortKey</span></span><br><span class="line">findsSortKey = <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (findsSortKey) &#123;</span><br><span class="line">List&lt;RowData&gt; inputs = dataState.get(key);</span><br><span class="line"><span class="keyword">if</span> (inputs == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// Skip the data if it's state is cleared because of state ttl.</span></span><br><span class="line"><span class="keyword">if</span> (lenient) &#123;</span><br><span class="line">LOG.warn(STATE_CLEARED_WARN_MSG);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(STATE_CLEARED_WARN_MSG);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; inputs.size() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">RowData prevRow = inputs.get(i); <span class="comment">// 取出前一个row</span></span><br><span class="line">collectUpdateBefore(out, prevRow, currentRank);</span><br><span class="line">collectUpdateAfter(out, currentRow, currentRank); <span class="comment">//输出当前行</span></span><br><span class="line">currentRow = prevRow; <span class="comment">// 前一行赋给当前行</span></span><br><span class="line">currentRank += <span class="number">1</span>;</span><br><span class="line">i++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">currentRank += entry.getValue();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (isInRankEnd(currentRank)) &#123;</span><br><span class="line"><span class="comment">// there is no enough elements in Top-N, emit INSERT message for the new record.</span></span><br><span class="line">collectInsert(out, currentRow, currentRank);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>RetractableTopNFunction 中的 <code>retractRecordWithRowNumber()</code> 方法将撤回行从 sortedMap 中移除，并更新前一行的排位输出：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">retractRecordWithRowNumber</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">SortedMap&lt;RowData, Long&gt; sortedMap, RowData sortKey, RowData inputRow, Collector&lt;RowData&gt; out)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Iterator&lt;Map.Entry&lt;RowData, Long&gt;&gt; iterator = sortedMap.entrySet().iterator();</span><br><span class="line"><span class="keyword">long</span> currentRank = <span class="number">0L</span>;</span><br><span class="line">RowData prevRow = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">boolean</span> findsSortKey = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">while</span> (iterator.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">Map.Entry&lt;RowData, Long&gt; entry = iterator.next();</span><br><span class="line">RowData key = entry.getKey();</span><br><span class="line"><span class="keyword">if</span> (!findsSortKey &amp;&amp; key.equals(sortKey)) &#123;</span><br><span class="line">List&lt;RowData&gt; inputs = dataState.get(key);</span><br><span class="line"><span class="keyword">if</span> (inputs == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// Skip the data if it's state is cleared because of state ttl.</span></span><br><span class="line"><span class="keyword">if</span> (lenient) &#123;</span><br><span class="line">LOG.warn(STATE_CLEARED_WARN_MSG);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(STATE_CLEARED_WARN_MSG);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">Iterator&lt;RowData&gt; inputIter = inputs.iterator();</span><br><span class="line"><span class="keyword">while</span> (inputIter.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">RowData currentRow = inputIter.next();</span><br><span class="line"><span class="keyword">if</span> (!findsSortKey &amp;&amp; equaliser.equals(currentRow, inputRow)) &#123;</span><br><span class="line">prevRow = currentRow;</span><br><span class="line">findsSortKey = <span class="keyword">true</span>;</span><br><span class="line">inputIter.remove();</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (findsSortKey) &#123;</span><br><span class="line">collectUpdateBefore(out, prevRow, currentRank);</span><br><span class="line">collectUpdateAfter(out, currentRow, currentRank);</span><br><span class="line">prevRow = currentRow;</span><br><span class="line">&#125;</span><br><span class="line">currentRank += <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (inputs.isEmpty()) &#123;</span><br><span class="line">dataState.remove(key); <span class="comment">// 将撤回的行从 sortedMap 中移除</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">dataState.put(key, inputs);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (findsSortKey) &#123;</span><br><span class="line">List&lt;RowData&gt; inputs = dataState.get(key);</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; inputs.size() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">RowData currentRow = inputs.get(i); <span class="comment">// 上一行作为当前行</span></span><br><span class="line"><span class="comment">// 处理上一条数据</span></span><br><span class="line">collectUpdateBefore(out, prevRow, currentRank);</span><br><span class="line"><span class="comment">// 输出当前行</span></span><br><span class="line">collectUpdateAfter(out, currentRow, currentRank);</span><br><span class="line">prevRow = currentRow;</span><br><span class="line">currentRank += <span class="number">1</span>;</span><br><span class="line">i++;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">currentRank += entry.getValue();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (isInRankEnd(currentRank)) &#123;</span><br><span class="line"><span class="comment">// there is no enough elements in Top-N, emit DELETE message for the retract record.</span></span><br><span class="line">collectDelete(out, prevRow, currentRank);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="RetractableTopNFunctionTest"><a href="#RetractableTopNFunctionTest" class="headerlink" title="RetractableTopNFunctionTest"></a>RetractableTopNFunctionTest</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Tests for &#123;<span class="doctag">@link</span> RetractableTopNFunction&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RetractableTopNFunctionTest</span> <span class="keyword">extends</span> <span class="title">TopNFunctionTestBase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> AbstractTopNFunction <span class="title">createFunction</span><span class="params">(RankType rankType, RankRange rankRange,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">boolean</span> generateUpdateBefore, <span class="keyword">boolean</span> outputRankNumber)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> RetractableTopNFunction(</span><br><span class="line">minTime.toMilliseconds(),</span><br><span class="line">maxTime.toMilliseconds(),</span><br><span class="line">inputRowType,</span><br><span class="line">sortKeyComparator,</span><br><span class="line">sortKeySelector,</span><br><span class="line">rankType,</span><br><span class="line">rankRange,</span><br><span class="line">generatedEqualiser,</span><br><span class="line">generateUpdateBefore,</span><br><span class="line">outputRankNumber);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testProcessRetractMessageWithNotGenerateUpdateBefore</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER,</span><br><span class="line"><span class="comment">// 固定的 TopN 集合大小，1～2</span></span><br><span class="line"><span class="keyword">new</span> ConstantRankRange(<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">false</span>,</span><br><span class="line"><span class="comment">// 输出 TopN 的排序序号</span></span><br><span class="line"><span class="keyword">true</span>);</span><br><span class="line">OneInputStreamOperatorTestHarness&lt;RowData, RowData&gt; testHarness = createTestHarness(func);</span><br><span class="line">testHarness.open();</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">1L</span>, <span class="number">12</span>));</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>));</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">4L</span>, <span class="number">11</span>));</span><br><span class="line">testHarness.processElement(updateBeforeRecord(<span class="string">"book"</span>, <span class="number">1L</span>, <span class="number">12</span>));</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">5L</span>, <span class="number">11</span>));</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"fruit"</span>, <span class="number">4L</span>, <span class="number">33</span>));</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"fruit"</span>, <span class="number">3L</span>, <span class="number">44</span>));</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"fruit"</span>, <span class="number">5L</span>, <span class="number">22</span>));</span><br><span class="line">testHarness.close();</span><br><span class="line"></span><br><span class="line">ConcurrentLinkedQueue&lt;Object&gt; output = testHarness.getOutput();</span><br><span class="line"><span class="keyword">for</span> (Object o : output) &#123;</span><br><span class="line">StreamRecord streamRecord = (StreamRecord) o;</span><br><span class="line">System.out.println(<span class="string">"Output element -&gt; "</span> + streamRecord.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;Object&gt; expectedOutput = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="comment">// ("book", 1L, 12)</span></span><br><span class="line"><span class="comment">// sortedMap -&gt; [("book", 1L, 12)]</span></span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"book"</span>, <span class="number">1L</span>, <span class="number">12</span>, <span class="number">1L</span>));</span><br><span class="line"><span class="comment">// ("book", 2L, 19)</span></span><br><span class="line"><span class="comment">// sortedMap -&gt; [("book", 1L, 12)],[("book", 2L, 19)]</span></span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>, <span class="number">2L</span>));</span><br><span class="line"><span class="comment">// ("book", 4L, 11)</span></span><br><span class="line"><span class="comment">// sortedMap -&gt; [("book", 4L, 11)],[("book", 1L, 12)],[("book", 2L, 19)]</span></span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"book"</span>, <span class="number">4L</span>, <span class="number">11</span>, <span class="number">1L</span>));</span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"book"</span>, <span class="number">1L</span>, <span class="number">12</span>, <span class="number">2L</span>));</span><br><span class="line"><span class="comment">// UB ("book", 1L, 12)，撤回即将 ("book", 1L ,12) 从 sortedMap 中移除，("book", 2L, 19)的排序被更新为 2</span></span><br><span class="line"><span class="comment">// sortedMap -&gt; [("book", 4L, 11)],[("book", 2L, 19)]</span></span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>, <span class="number">2L</span>));</span><br><span class="line"><span class="comment">// ("book", 5L, 11)</span></span><br><span class="line"><span class="comment">// sortedMap -&gt; [("book", 4L, 11),("book", 5L, 11)],[("book", 2L, 19)]，("book", 5L, 11) 的排序被更新为 2</span></span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"book"</span>, <span class="number">5L</span>, <span class="number">11</span>, <span class="number">2L</span>));</span><br><span class="line"><span class="comment">// ("fruit", 4L, 33)</span></span><br><span class="line"><span class="comment">// ("fruit", 3L, 44)</span></span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"fruit"</span>, <span class="number">4L</span>, <span class="number">33</span>, <span class="number">1L</span>));</span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"fruit"</span>, <span class="number">3L</span>, <span class="number">44</span>, <span class="number">2L</span>));</span><br><span class="line"><span class="comment">// ("fruit", 5L, 22)</span></span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"fruit"</span>, <span class="number">5L</span>, <span class="number">22</span>, <span class="number">1L</span>));</span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"fruit"</span>, <span class="number">4L</span>, <span class="number">33</span>, <span class="number">2L</span>));</span><br><span class="line">assertorWithRowNumber.assertOutputEquals(<span class="string">"output wrong."</span>, expectedOutput, testHarness.getOutput());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Output element -&gt; +I&#123;row1=+I(book,1,12), row2=+I(1)&#125;</span><br><span class="line">Output element -&gt; +I&#123;row1=+I(book,2,19), row2=+I(2)&#125;</span><br><span class="line">Output element -&gt; +U&#123;row1=+I(book,4,11), row2=+I(1)&#125;</span><br><span class="line">Output element -&gt; +U&#123;row1=+I(book,1,12), row2=+I(2)&#125;</span><br><span class="line">Output element -&gt; +U&#123;row1=+I(book,2,19), row2=+I(2)&#125;</span><br><span class="line">Output element -&gt; +U&#123;row1=+I(book,5,11), row2=+I(2)&#125;</span><br><span class="line">Output element -&gt; +I&#123;row1=+I(fruit,4,33), row2=+I(1)&#125;</span><br><span class="line">Output element -&gt; +I&#123;row1=+I(fruit,3,44), row2=+I(2)&#125;</span><br><span class="line">Output element -&gt; +U&#123;row1=+I(fruit,5,22), row2=+I(1)&#125;</span><br><span class="line">Output element -&gt; +U&#123;row1=+I(fruit,4,33), row2=+I(2)&#125;</span><br></pre></td></tr></table></figure><h2 id="UpdatableTopNFunction"><a href="#UpdatableTopNFunction" class="headerlink" title="UpdatableTopNFunction"></a>UpdatableTopNFunction</h2><p>支持更新流，是 RetractableTopNFunction 的简单实现版本，输入流中不能包含 DELETE 和 UPDATE_BEFORE 操作。</p><h3 id="UpdatableTopNFunctionTest"><a href="#UpdatableTopNFunctionTest" class="headerlink" title="UpdatableTopNFunctionTest"></a>UpdatableTopNFunctionTest</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Tests for &#123;<span class="doctag">@link</span> UpdatableTopNFunction&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UpdatableTopNFunctionTest</span> <span class="keyword">extends</span> <span class="title">TopNFunctionTestBase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> AbstractTopNFunction <span class="title">createFunction</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">RankType rankType,</span></span></span><br><span class="line"><span class="function"><span class="params">RankRange rankRange,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> generateUpdateBefore,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> outputRankNumber)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> UpdatableTopNFunction(</span><br><span class="line">minTime.toMilliseconds(),</span><br><span class="line">maxTime.toMilliseconds(),</span><br><span class="line">inputRowType,</span><br><span class="line">rowKeySelector,</span><br><span class="line">sortKeyComparator,</span><br><span class="line">sortKeySelector,</span><br><span class="line">rankType,</span><br><span class="line">rankRange,</span><br><span class="line">generateUpdateBefore,</span><br><span class="line">outputRankNumber,</span><br><span class="line">cacheSize);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testVariableRankRange</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER,</span><br><span class="line"><span class="comment">// TopN 的集合大小随着数据动态变化</span></span><br><span class="line"><span class="keyword">new</span> VariableRankRange(<span class="number">1</span>),</span><br><span class="line"><span class="keyword">true</span>,</span><br><span class="line"><span class="keyword">false</span>);</span><br><span class="line">OneInputStreamOperatorTestHarness&lt;RowData, RowData&gt; testHarness = createTestHarness(func);</span><br><span class="line">testHarness.open();</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>));</span><br><span class="line">testHarness.processElement(updateAfterRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">18</span>));</span><br><span class="line">testHarness.processElement(insertRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">44</span>));</span><br><span class="line">testHarness.processElement(updateAfterRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">33</span>));</span><br><span class="line">testHarness.processElement(updateAfterRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">22</span>));</span><br><span class="line">testHarness.close();</span><br><span class="line"></span><br><span class="line">ConcurrentLinkedQueue&lt;Object&gt; output = testHarness.getOutput();</span><br><span class="line"><span class="keyword">for</span> (Object o : output) &#123;</span><br><span class="line">StreamRecord streamRecord = (StreamRecord) o;</span><br><span class="line">System.out.println(<span class="string">"Output element -&gt; "</span> + streamRecord.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;Object&gt; expectedOutput = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>));</span><br><span class="line">expectedOutput.add(updateBeforeRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">19</span>));</span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"book"</span>, <span class="number">2L</span>, <span class="number">18</span>));</span><br><span class="line">expectedOutput.add(insertRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">44</span>));</span><br><span class="line">expectedOutput.add(updateBeforeRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">44</span>));</span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">33</span>));</span><br><span class="line">expectedOutput.add(updateBeforeRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">33</span>));</span><br><span class="line">expectedOutput.add(updateAfterRecord(<span class="string">"fruit"</span>, <span class="number">1L</span>, <span class="number">22</span>));</span><br><span class="line">assertorWithoutRowNumber</span><br><span class="line">.assertOutputEquals(<span class="string">"output wrong."</span>, expectedOutput, testHarness.getOutput());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Output element -&gt; +I(book,2,19)</span><br><span class="line">Output element -&gt; -U(book,2,19)</span><br><span class="line">Output element -&gt; +U(book,2,18)</span><br><span class="line">Output element -&gt; +I(fruit,1,44)</span><br><span class="line">Output element -&gt; -U(fruit,1,44)</span><br><span class="line">Output element -&gt; +U(fruit,1,33)</span><br><span class="line">Output element -&gt; -U(fruit,1,33)</span><br><span class="line">Output element -&gt; +U(fruit,1,22)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将基于 flink &lt;code&gt;release-1.11&lt;/code&gt; 源码，简单分析下 TopN function 的实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink部署-flink-on-kubernetes</title>
    <link href="http://yoursite.com/2020/09/29/Flink%E9%83%A8%E7%BD%B2-flink-on-kubernetes/"/>
    <id>http://yoursite.com/2020/09/29/Flink部署-flink-on-kubernetes/</id>
    <published>2020-09-29T07:23:17.000Z</published>
    <updated>2021-06-22T07:45:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>kubernetes 是目前非常流行的容器编排系统，在其之上可以运行 web 服务、大数据处理等各类应用。这些应用被打包在非常轻量的容器中，我们通过声明的方式来告知 kubernetes 要如何部署和扩容这些程序，并对外提供服务。flink on kubernetes 可以得到一个健壮和高可扩的数据处理应用，并且能够更安全的和其他服务共享一个 kubernetes 集群。</p><p>本文将记录使用 kubernetes 部署 flink 应用的步骤。</p><a id="more"></a><h2 id="Mac-安装-Docker"><a href="#Mac-安装-Docker" class="headerlink" title="Mac 安装 Docker"></a>Mac 安装 Docker</h2><p>Docker Desktop 下载地址：<a href="https://www.docker.com/get-started" target="_blank" rel="noopener">Docker 官网</a><br>注册 DockerID 并登录。</p><p>安装 docker 命令行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> brew install docker</span></span><br></pre></td></tr></table></figure><h2 id="Minikube-搭建-Kubernetes-实验环境"><a href="#Minikube-搭建-Kubernetes-实验环境" class="headerlink" title="Minikube 搭建 Kubernetes 实验环境"></a>Minikube 搭建 Kubernetes 实验环境</h2><p>可以参考：<a href="https://kubernetes.io/docs/setup/learning-environment/minikube/#quickstart" target="_blank" rel="noopener">Kubernetes 官网</a></p><h3 id="安装-Minikube"><a href="#安装-Minikube" class="headerlink" title="安装 Minikube"></a>安装 Minikube</h3><ol><li><p>校验 MacOS 是否支持虚拟化，运行如下命令出现 ‘VMX’：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sysctl -a | grep -E --color <span class="string">'machdep.cpu.features|VMX'</span></span></span><br></pre></td></tr></table></figure></li><li><p>安装 kubectl 命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -LO <span class="string">"https://storage.googleapis.com/kubernetes-release/release/<span class="variable">$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)</span>/bin/darwin/amd64/kubectl"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chmod +x ./kubectl</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo mv ./kubectl /usr/<span class="built_in">local</span>/bin/kubectl</span></span><br></pre></td></tr></table></figure><p> 或者直接使用 Homebrew 安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> brew install kubectl </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew install kubernetes-cli</span></span><br></pre></td></tr></table></figure><p> 查看是否安装成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version --client</span></span><br><span class="line">Client Version: version.Info&#123;Major:"1", Minor:"19", GitVersion:"v1.19.2", GitCommit:"f5743093fd1c663cb0cbc89748f730662345d44d", GitTreeState:"clean", BuildDate:"2020-09-16T13:41:02Z", GoVersion:"go1.15", Compiler:"gc", Platform:"darwin/amd64"&#125;</span><br></pre></td></tr></table></figure></li><li><p>安装 VirtualBox<br>VirtualBox 下载地址：<a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox 官网</a></p></li></ol><ol start="4"><li><p>安装 minikube</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 &amp;&amp; chmod +x minikube</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo mv minikube /usr/<span class="built_in">local</span>/bin</span></span><br></pre></td></tr></table></figure><p> 或者直接使用 Homebrew 安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> brew install minikube</span></span><br></pre></td></tr></table></figure></li><li><p>执行 minikube start<br>该命令会下载 kubelet 和 kubeadm 程序，并构建一个完整的 k8s 集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> minikube start</span></span><br></pre></td></tr></table></figure></li><li><p>查看 k8s pods<br>Minikube 已经将命令 kubectl 指向虚拟机中的 k8s 集群了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -A</span></span><br><span class="line">kube-system   coredns-f9fd979d6-xjht6                           1/1     Running   0          5h14m</span><br><span class="line">kube-system   etcd-minikube                                     1/1     Running   0          5h14m</span><br><span class="line">kube-system   kube-apiserver-minikube                           1/1     Running   0          5h14m</span><br><span class="line">kube-system   kube-controller-manager-minikube                  1/1     Running   0          5h14m</span><br><span class="line">kube-system   kube-proxy-ff8m8                                  1/1     Running   0          5h14m</span><br><span class="line">kube-system   kube-scheduler-minikube                           1/1     Running   0          5h14m</span><br><span class="line">kube-system   storage-provisioner                               1/1     Running   0          5h14m</span><br></pre></td></tr></table></figure></li></ol><h2 id="Flink-实时处理-demo"><a href="#Flink-实时处理-demo" class="headerlink" title="Flink 实时处理 demo"></a>Flink 实时处理 demo</h2><p>我们可以编写一个简单的实时处理脚本，该脚本会从某个端口中读取文本，分割为单词，并且每 5 秒钟打印一次每个单词出现的次数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">    .socketTextStream(<span class="string">"192.168.99.1"</span>, <span class="number">9999</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">    .keyBy(<span class="number">0</span>)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">    .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">dataStream.print();</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"Window WordCount"</span>);</span><br></pre></td></tr></table></figure><p>K8s 容器中的程序可以通过 IP 192.168.99.1 来访问 Minikube 宿主机上的服务。</p><p>demo 下载：<a href="flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar">flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar</a></p><h2 id="构建-Docker-容器镜像"><a href="#构建-Docker-容器镜像" class="headerlink" title="构建 Docker 容器镜像"></a>构建 Docker 容器镜像</h2><p>flink 提供了一个官方的容器镜像，可以从 <a href="https://hub.docker.com/_/flink?tab=tags&page=1&name=1.8.1-scala_2.12" target="_blank" rel="noopener">DockerHub</a> 上下载镜像。<br>官方镜像的 <a href="https://github.com/docker-flink/docker-flink/blob/master/1.8/scala_2.12-debian/Dockerfile" target="_blank" rel="noopener">Dockerfile</a>，以 <code>1.8.1-scala_2.12</code> 为例，大致内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FROM openjdk:8-jre</span><br><span class="line">ENV FLINK_HOME=/opt/flink</span><br><span class="line">ENV PATH=$FLINK_HOME/bin:$PATH</span><br><span class="line">RUN groupadd --system --gid=9999 flink &amp;&amp; \</span><br><span class="line">    useradd --system --home-dir $FLINK_HOME --uid=9999 --gid=flink flink</span><br><span class="line">WORKDIR $FLINK_HOME</span><br><span class="line"></span><br><span class="line">RUN useradd flink &amp;&amp; \</span><br><span class="line">  wget -O flink.tgz &quot;$FLINK_TGZ_URL&quot; &amp;&amp; \</span><br><span class="line">  tar -xf flink.tgz</span><br><span class="line">ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]</span><br></pre></td></tr></table></figure><p>主要做了以下几件事情：</p><ul><li>将 OpenJDK 1.8 作为基础镜像</li><li>下载并安装 flink 至 /opt/flink 目录中</li><li>添加 flink 用户和组等</li></ul><p>下面我们以 flink:1.8.1-scala_2.12 作为基础镜像，编写新的 Dockerfile，将打包好的任务 jar 包放置进去。此外，新版 flink 已将 hadoop 依赖从官方发行版本中剥离，因此在打包镜像的时候也要包含进去。<br>Hadoop jar 下载：<a href="flink-shaded-hadoop-2-uber-2.8.3-7.0.jar">flink-shaded-hadoop-2-uber-2.8.3-7.0.jar</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FROM flink:1.8.1-scala_2.12</span><br><span class="line"></span><br><span class="line">ARG hadoop_jar</span><br><span class="line">ARG job_jar</span><br><span class="line"></span><br><span class="line">ENV FLINK_CONF=$FLINK_HOME/conf/flink-conf.yaml</span><br><span class="line"></span><br><span class="line">RUN set -x &amp;&amp; \</span><br><span class="line">  sed -i -e &quot;s/jobmanager\.heap\.size:.*/jobmanager.heap.size: 128m/g&quot; $FLINK_CONF &amp;&amp; \</span><br><span class="line">  sed -i -e &quot;s/taskmanager\.heap\.size:.*/taskmanager.heap.size: 256m/g&quot; $FLINK_CONF</span><br><span class="line"></span><br><span class="line">COPY --chown=flink:flink $hadoop_jar $job_jar $FLINK_HOME/lib/</span><br><span class="line"></span><br><span class="line">USER flink</span><br></pre></td></tr></table></figure><p>将 docker 命令行指向 Minikube 中的 Docker 服务，这样打印出来的镜像才能被 k8s 使用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">eval</span> $(minikube docker-env)</span></span><br></pre></td></tr></table></figure><p>移动到 Dockerfile 所在目录，开始构建镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker build \</span></span><br><span class="line">  --build-arg hadoop_jar=flink-shaded-hadoop-2-uber-2.8.3-7.0.jar \</span><br><span class="line">  --build-arg job_jar=flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar \</span><br><span class="line">  --tag flink-on-kubernetes:0.0.1 .</span><br></pre></td></tr></table></figure><p>镜像打包完毕，可用于部署：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker image ls</span></span><br><span class="line">REPOSITORY                           TAG                                              IMAGE ID            CREATED             SIZE</span><br><span class="line">flink-on-kubernetes                  0.0.1                                            ed4dfaf07cfe        5 hours ago         618MB</span><br></pre></td></tr></table></figure><h2 id="部署-JobManager"><a href="#部署-JobManager" class="headerlink" title="部署 JobManager"></a>部署 JobManager</h2><p><code>jobmanager.yml</code> ：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        instance:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink-on-kubernetes:0.0.1</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">               <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dparallelism.default=1"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dblob.server.port=6124"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dqueryable-state.server.ports=6125"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dstate.savepoints.dir=hdfs://192.168.99.1:9000/flink/savepoints/"</span><span class="string">,</span></span><br><span class="line">               <span class="string">]</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ui</span></span><br></pre></td></tr></table></figure><ul><li>${JOB} 变量可以使用 <code>envsubst</code> 命令来替换</li><li>容器的入口修改为 <code>standalone-job.sh</code></li><li>JobManager 的 rpc 地址修改为了 k8s Service 的名称，集群中的其他组件将通过这个名称来访问 JobManager。</li><li>为 Flink Blob Server &amp; Queryable State Server 指定默认端口号</li></ul><p>使用 <code>kubectl</code> 命令创建 JobManager pod，并查看状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> JOB=flink-on-kubernetes</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> envsubst &lt;jobmanager.yml | kubectl create -f -</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod</span></span><br><span class="line">NAME                                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">flink-on-kubernetes-jobmanager-dzhcs              1/1     Running   0          77m</span><br></pre></td></tr></table></figure><p>创建一个 k8s Service 把 JobManager 的端口开放出来，以便 TaskManager 前来注册。<br><code>service.yml</code>：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    instance:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8081</span></span><br></pre></td></tr></table></figure><p>使用 <code>kubectl</code> 命令创建 JobManager service，并查看状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> envsubst &lt;service.yml | kubectl create -f -</span></span><br><span class="line"><span class="meta">$</span><span class="bash">  kubectl get service</span></span><br><span class="line">NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                       AGE</span><br><span class="line">flink-on-kubernetes-jobmanager   NodePort    10.104.157.70   &lt;none&gt;        6123:30261/TCP,6124:31158/TCP,6125:30509/TCP,8081:30262/TCP   89m</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> minikube service <span class="variable">$JOB</span>-jobmanager --url</span></span><br><span class="line">http://192.168.99.100:30261</span><br><span class="line">http://192.168.99.100:31158</span><br><span class="line">http://192.168.99.100:30509</span><br><span class="line">http://192.168.99.100:30262</span><br></pre></td></tr></table></figure><p><img src="JobManager%E9%80%8F%E5%87%BA%E7%9A%84Dashboard.png" alt></p><h2 id="部署-TaskManager"><a href="#部署-TaskManager" class="headerlink" title="部署 TaskManager"></a>部署 TaskManager</h2><p><code>taskmanager.yml</code> ：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">      instance:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        instance:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink-on-kubernetes:0.0.1</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">["/opt/flink/bin/taskmanager.sh"]</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">["start-foreground",</span> <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">]</span></span><br></pre></td></tr></table></figure><p>使用 <code>kubectl</code> 命令创建 TaskManager pod，并查看状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod</span></span><br><span class="line">NAME                                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">flink-on-kubernetes-jobmanager-dzhcs              1/1     Running   0          77m</span><br><span class="line">flink-on-kubernetes-taskmanager-64b7cc4bf-9t6cr   1/1     Running   2          77m</span><br></pre></td></tr></table></figure><p>至此，Flink 脚本集群已经在运行中了。在监听终端下输入如下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -lk 9999</span></span><br><span class="line">hello world</span><br><span class="line">hello flink</span><br></pre></td></tr></table></figure><p>打开另一个终端，查看 TaskManager 的标准输出日志：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs -f -l instance=<span class="variable">$JOB</span>-taskmanager</span></span><br><span class="line">(hello,2)</span><br><span class="line">(flink,1)</span><br><span class="line">(world,1)</span><br></pre></td></tr></table></figure><p><img src="taskmanager%E7%BB%9F%E8%AE%A1%E5%8D%95%E8%AF%8D%E7%9A%84%E6%A0%87%E5%87%86%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://developer.aliyun.com/article/784822?spm=a2c6h.13148508.0.0.5be54f0eC7Xfxo" target="_blank" rel="noopener">唯品会：在 Flink 容器化与平台化上的建设实践</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;kubernetes 是目前非常流行的容器编排系统，在其之上可以运行 web 服务、大数据处理等各类应用。这些应用被打包在非常轻量的容器中，我们通过声明的方式来告知 kubernetes 要如何部署和扩容这些程序，并对外提供服务。flink on kubernetes 可以得到一个健壮和高可扩的数据处理应用，并且能够更安全的和其他服务共享一个 kubernetes 集群。&lt;/p&gt;
&lt;p&gt;本文将记录使用 kubernetes 部署 flink 应用的步骤。&lt;/p&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
  </entry>
  
</feed>
