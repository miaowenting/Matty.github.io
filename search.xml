<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Apache-Iceberg]]></title>
    <url>%2F2021%2F02%2F20%2FApache-Iceberg%2F</url>
    <content type="text"><![CDATA[Hey, password is required here. 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]]></content>
      <categories>
        <category>DataLake</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2020年终个人总结]]></title>
    <url>%2F2021%2F02%2F01%2F2020%E5%B9%B4%E7%BB%88%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Hey, password is required here. 49f7eb9bea6cd6a47e660ee8aad95e5f339e339538ece7d1e61ba075caa7780d]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[晚熟的人]]></title>
    <url>%2F2021%2F01%2F10%2F%E6%99%9A%E7%86%9F%E7%9A%84%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[Hey, password is required here. 49f7eb9bea6cd6a47e660ee8aad95e5f339e339538ece7d1e61ba075caa7780d]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink源码剖析-flink-table-runtime-blink_TopN]]></title>
    <url>%2F2020%2F09%2F30%2FFlink%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-flink-table-runtime-blink-TopN%2F</url>
    <content type="text"><![CDATA[本文将基于 flink release-1.11 源码，简单分析下 TopN function 的实现。 AbstractTopNFunctionAbstractTopNFunction 中有如下属性，定义 sortKey selector 和 comparator，rankEnd 相关参数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// we set default topN size to 100private static final long DEFAULT_TOPN_SIZE = 100;// The util to compare two sortKey equals to each other./** * 生成 sortKey 比较器实例类的工具类 */private GeneratedRecordComparator generatedSortKeyComparator;/** * sortKey 比较器 */protected Comparator&lt;RowData&gt; sortKeyComparator;private final boolean generateUpdateBefore;/** * 是否输出排序序号 */protected final boolean outputRankNumber;/** * 输入的数据类型 */protected final RowDataTypeInfo inputRowType;/** * key selector，选择 RowData 中的哪一个字段来排序 */protected final KeySelector&lt;RowData, RowData&gt; sortKeySelector;/** * key 上下文，获取当前处理数据的 key */protected KeyContext keyContext;/** * 是否是固定的 TopN 集合大小 */private final boolean isConstantRankEnd;/** * rankStart 值 */private final long rankStart;/** * rankEnd 在 RowData 中的下标 */private final int rankEndIndex;/** * rankEnd 值 */protected long rankEnd;/** * java.util.Function，从 RowData 的某一个位置获取 rankEnd */private transient Function&lt;RowData, Long&gt; rankEndFetcher;/** * 记录 rankEnd，可能随着输入数据动态变化 */private ValueState&lt;Long&gt; rankEndState;private Counter invalidCounter;/** * 当 TopN 需要输出排位序号时，会用到这个对象 */private JoinedRowData outputRow;// metricsprotected long hitCount = 0L;protected long requestCount = 0L; AbstractTopNFunction 的 open() 方法主要从状态后端获取 rankEndState，并初始化类属性： 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic void open(Configuration parameters) throws Exception &#123; super.open(parameters); initCleanupTimeState("RankFunctionCleanupTime"); outputRow = new JoinedRowData(); if (!isConstantRankEnd) &#123; // 从状态后端读取当前 rankEnd 值 ValueStateDescriptor&lt;Long&gt; rankStateDesc = new ValueStateDescriptor&lt;&gt;("rankEnd", Types.LONG); rankEndState = getRuntimeContext().getState(rankStateDesc); &#125; // compile comparator // classLoader 加载 key comparator 类 sortKeyComparator = generatedSortKeyComparator.newInstance(getRuntimeContext().getUserCodeClassLoader()); // 把确定不需要的对象直接赋值为 null generatedSortKeyComparator = null; invalidCounter = getRuntimeContext().getMetricGroup().counter("topn.invalidTopSize"); // initialize rankEndFetcher if (!isConstantRankEnd) &#123; LogicalType rankEndIdxType = inputRowType.getLogicalTypes()[rankEndIndex]; switch (rankEndIdxType.getTypeRoot()) &#123; case BIGINT: rankEndFetcher = (RowData row) -&gt; row.getLong(rankEndIndex); break; case INTEGER: rankEndFetcher = (RowData row) -&gt; (long) row.getInt(rankEndIndex); break; case SMALLINT: rankEndFetcher = (RowData row) -&gt; (long) row.getShort(rankEndIndex); break; default: LOG.error("variable rank index column must be long, short or int type, while input type is &#123;&#125;", rankEndIdxType.getClass().getName()); throw new UnsupportedOperationException( "variable rank index column must be long type, while input type is " + rankEndIdxType.getClass().getName()); &#125; &#125;&#125; AbstractTopNFunction 的 initRankEnd() 方法根据 input row 来动态获取 rankEnd ： 1234567891011121314151617181920212223242526272829 /** * Initialize rank end. * * @param row input record * @return rank end * @throws Exception */protected long initRankEnd(RowData row) throws Exception &#123; if (isConstantRankEnd) &#123; return rankEnd; &#125; else &#123; Long rankEndValue = rankEndState.value(); long curRankEnd = rankEndFetcher.apply(row); if (rankEndValue == null) &#123; rankEnd = curRankEnd; // 同步更新到状态后端 rankEndState.update(rankEnd); return rankEnd; &#125; else &#123; rankEnd = rankEndValue; if (rankEnd != curRankEnd) &#123; // increment the invalid counter when the current rank end not equal to previous rank end invalidCounter.inc(); &#125; return rankEnd; &#125; &#125;&#125; AbstractTopNFunction 的 checkSortKeyInBufferRange() 方法来判断 input row 是否应该被放到其 key 对应的 TopBuffer 中： 将 input row 与 TopBuffer 中的最后一个 entry 比较，comparator 返回 true 则将 input row 丢到 TopBuffer 中； comparator 返回 false，当前 TopBuffer 中的 entry 个数还没有达到默认的 TopN size，也将 input row 丢到 TopBuffer 中。1234567891011121314151617181920212223242526/** * Checks whether the record should be put into the buffer. * * @param sortKey sortKey to test * @param buffer buffer to add * @return true if the record should be put into the buffer. */protected boolean checkSortKeyInBufferRange(RowData sortKey, TopNBuffer buffer) &#123; Comparator&lt;RowData&gt; comparator = buffer.getSortKeyComparator(); Map.Entry&lt;RowData, Collection&lt;RowData&gt;&gt; worstEntry = buffer.lastEntry(); if (worstEntry == null) &#123; // return true if the buffer is empty. TopNBuffer 是空的，直接返回 true return true; &#125; else &#123; RowData worstKey = worstEntry.getKey(); //执行 TopN 比较器 int compare = comparator.compare(sortKey, worstKey); if (compare &lt; 0) &#123; // 如果满足条件，可以放到 TopNBuffer 中 return true; &#125; else &#123; // 到达的数据条数还没有达到默认的 TopN 大小 100，也可以放到 TopNBuffer 中 return buffer.getCurrentTopNum() &lt; getDefaultTopNSize(); &#125; &#125;&#125; AbstractTopNFunction 的 createOutputRow() 方法用于构建 output row，区分带不带 rank 序号： 1234567891011121314151617181920212223 /** * 构建 output row * * @param inputRow input row * @param rank 排位序号 * @param rowKind 描述一行 changelog 的行为种类 * @return &#123;@link RowData&#125; */private RowData createOutputRow(RowData inputRow, long rank, RowKind rowKind) &#123; if (outputRankNumber) &#123; // 需要输出 rank number GenericRowData rankRow = new GenericRowData(1); // 第 0 个字段设置为排位序号，将 rank 专门放置在一个 RowData 中 rankRow.setField(0, rank); outputRow.replace(inputRow, rankRow); outputRow.setRowKind(rowKind); return outputRow; &#125; else &#123; inputRow.setRowKind(rowKind); return inputRow; &#125;&#125; AppendOnlyTopNFunctionAppendOnlyTopNFunction 中有如下属性，状态后端 MapState 和本地堆内存 TopNBuffer 结合使用： 123456789101112131415161718192021222324252627 /** * sortKey 字段类型 */private final RowDataTypeInfo sortKeyType;/** * input row 的序列化类 */private final TypeSerializer&lt;RowData&gt; inputRowSer;private final long cacheSize;// a map state stores mapping from sort key to records list which is in topN/** * sortKey &lt;-&gt; 在 TopN 中的 RowData list */private transient MapState&lt;RowData, List&lt;RowData&gt;&gt; dataState;// the buffer stores mapping from sort key to records list, a heap mirror to dataState/** * 当前 sortKey 对应的 TopNBuffer */private transient TopNBuffer buffer;// the kvSortedMap stores mapping from partition key to it's buffer/** * sortKey &lt;-&gt; TopNBuffer */private transient Map&lt;RowData, TopNBuffer&gt; kvSortedMap; AppendOnlyTopNFunction 的 open() 方法中从状态后端中获取当前 key 的 TopN list： 123456789101112131415161718public void open(Configuration parameters) throws Exception &#123; super.open(parameters); // LRU的缓存大小=总的缓存大小/topN的缓存大小 int lruCacheSize = Math.max(1, (int) (cacheSize / getDefaultTopNSize())); // 根据 key 缓存 LRU list kvSortedMap = new LRUMap&lt;&gt;(lruCacheSize); LOG.info("Top&#123;&#125; operator is using LRU caches key-size: &#123;&#125;", getDefaultTopNSize(), lruCacheSize); // 根据 key 记录当前的 TopN list // RowDataTypeInfo ListTypeInfo&lt;RowData&gt; valueTypeInfo = new ListTypeInfo&lt;&gt;(inputRowType); MapStateDescriptor&lt;RowData, List&lt;RowData&gt;&gt; mapStateDescriptor = new MapStateDescriptor&lt;&gt;( "data-state-with-append", sortKeyType, valueTypeInfo); dataState = getRuntimeContext().getMapState(mapStateDescriptor); // metrics registerMetric(kvSortedMap.size() * getDefaultTopNSize());&#125; AppendOnlyTopNFunction 的 processElement() 方法处理数据，判断当前 input row 是否可以丢到 TopNBuffer 中： 123456789101112131415161718192021222324252627282930313233@Overridepublic void processElement(RowData input, Context context, Collector&lt;RowData&gt; out) throws Exception &#123; // 获取当前时间，记录在上下文的计时器中 long currentTime = context.timerService().currentProcessingTime(); // register state-cleanup timer registerProcessingCleanupTimer(context, currentTime); initHeapStates(); initRankEnd(input); // 从输入的数据中抽取 sortKey RowData sortKey = sortKeySelector.getKey(input); // check whether the sortKey is in the topN range // 根据 sortKey 判断当前数据是否应该被放到 TopNBuffer 中 if (checkSortKeyInBufferRange(sortKey, buffer)) &#123; // insert sort key into buffer buffer.put(sortKey, inputRowSer.copy(input)); Collection&lt;RowData&gt; inputs = buffer.get(sortKey); // update data state // copy a new collection to avoid mutating state values, see CopyOnWriteStateMap, // otherwise, the result might be corrupt. // don't need to perform a deep copy, because RowData elements will not be updated // 同步记录到 MapState 中 dataState.put(sortKey, new ArrayList&lt;&gt;(inputs)); if (outputRankNumber || hasOffset()) &#123; // the without-number-algorithm can't handle topN with offset, // so use the with-number-algorithm to handle offset processElementWithRowNumber(sortKey, input, out); &#125; else &#123; processElementWithoutRowNumber(input, out); &#125; &#125;&#125; AppendOnlyTopNFunction 的 initHeapStates() 是在处理 input row 之前，在堆内存中初始化 TopNBuffer，并将状态后端存储的 TopN list 设置到 TopNBuffer 中： 123456789101112131415161718192021222324252627private void initHeapStates() throws Exception &#123; requestCount += 1; // 从 KeyContext 中获取当前的key RowData currentKey = (RowData) keyContext.getCurrentKey(); // 取出 key 对应的 TopNBuffer buffer = kvSortedMap.get(currentKey); if (buffer == null) &#123; // buffer 为 null，则为此 key 构建 TopNBuffer，为其设置 key comparator buffer = new TopNBuffer(sortKeyComparator, ArrayList::new); kvSortedMap.put(currentKey, buffer); // restore buffer // 读取 state 中记录的 TopN list，塞到这个 TopNBuffer 里 Iterator&lt;Map.Entry&lt;RowData, List&lt;RowData&gt;&gt;&gt; iter = dataState.iterator(); if (iter != null) &#123; while (iter.hasNext()) &#123; Map.Entry&lt;RowData, List&lt;RowData&gt;&gt; entry = iter.next(); RowData sortKey = entry.getKey(); List&lt;RowData&gt; values = entry.getValue(); // the order is preserved buffer.putAll(sortKey, values); &#125; &#125; &#125; else &#123; // buffer 不为 null，记录命中一次 TopNBuffer 缓存 hitCount += 1; &#125;&#125; AppendOnlyTopNFunction 的 processElementWithoutRowNumber() 方法是处理丢到 TopNBuffer 中的 input row，决定这条数据是否被 Delete ： 1234567891011121314151617181920212223242526272829303132333435private void processElementWithoutRowNumber(RowData input, Collector&lt;RowData&gt; out) throws Exception &#123; // remove retired element // 当前 TopNBuffer 中缓存的数据条数大于 TopN 的 N if (buffer.getCurrentTopNum() &gt; rankEnd) &#123; Map.Entry&lt;RowData, Collection&lt;RowData&gt;&gt; lastEntry = buffer.lastEntry(); RowData lastKey = lastEntry.getKey(); Collection&lt;RowData&gt; lastList = lastEntry.getValue(); RowData lastElement = buffer.lastElement(); int size = lastList.size(); // remove last one if (size &lt;= 1) &#123; // 移除最后一个元素 buffer.removeAll(lastKey); dataState.remove(lastKey); &#125; else &#123; // 移除大于 TopN 的 N 之后的元素 buffer.removeLast(); // last element has been removed from lastList, we have to copy a new collection // for lastList to avoid mutating state values, see CopyOnWriteStateMap, // otherwise, the result might be corrupt. // don't need to perform a deep copy, because RowData elements will not be updated // 更新状态后端 dataState.put(lastKey, new ArrayList&lt;&gt;(lastList)); &#125; if (size == 0 || input.equals(lastElement)) &#123; // input 的数据和 TopNBuffer 中的最后一个元素相同，则直接返回 return; &#125; else &#123; // lastElement shouldn't be null collectDelete(out, lastElement); &#125; &#125; // it first appears in the TopN, send INSERT message collectInsert(out, input);&#125; AppendOnlyTopNFunctionTest12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * Tests for &#123;@link AppendOnlyTopNFunction&#125;. */public class AppendOnlyTopNFunctionTest extends TopNFunctionTestBase &#123; @Override protected AbstractTopNFunction createFunction(RankType rankType, RankRange rankRange, boolean generateUpdateBefore, boolean outputRankNumber) &#123; return new AppendOnlyTopNFunction(minTime.toMilliseconds(), maxTime.toMilliseconds(), inputRowType, sortKeyComparator, sortKeySelector, rankType, rankRange, generateUpdateBefore, outputRankNumber, cacheSize); &#125; @Test public void testVariableRankRange() throws Exception &#123; AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER, // 指定数据的第2个字段值为 rankEnd，动态指定 TopN 集合的大小 new VariableRankRange(1), true, // 不用输出 topN 的排序序号 false); // 将 TopNFunction 包装进 KeyedProcessOperator OneInputStreamOperatorTestHarness&lt;RowData, RowData&gt; testHarness = createTestHarness(func); // 测试类准备工作 testHarness.open(); // KeyedProcessOperator 作为 input operator 模拟处理数据 testHarness.processElement(insertRecord("book", 2L, 12)); // 开始处理(book,2,12)，key 为 book，rankEnd 为 2，加入 TopN 集合 testHarness.processElement(insertRecord("book", 2L, 19)); // 开始处理(book,2,19)，key 为 book，rankEnd 为 2，加入 TopN 集合 testHarness.processElement(insertRecord("book", 2L, 11)); // 开始处理(book,2,11)，key 为 book，rankEnd 为 2，超出 TopN 集合容量，因此需要先删除 (book,2,19)，留下 2 个较小的 testHarness.processElement(insertRecord("fruit", 1L, 33)); // 开始处理(fruit,1,33)，key 为 fruit，rankEnd 为 1，加入 TopN 集合 testHarness.processElement(insertRecord("fruit", 1L, 44)); // 开始处理(fruit,1,44)，key 为 fruit，rankEnd 为 1，44 &gt; 33，直接过滤掉 testHarness.processElement(insertRecord("fruit", 1L, 22)); // 开始处理(fruit,1,22)，key 为 fruit，rankEnd 为 1，超出 TopN 集合容量，因此需要先删除 (fruit,1,33)，留下 1 个较小的 testHarness.close(); ConcurrentLinkedQueue&lt;Object&gt; output = testHarness.getOutput(); for (Object o : output) &#123; StreamRecord streamRecord = (StreamRecord) o; System.out.println("Output element -&gt; " + streamRecord.getValue()); &#125; List&lt;Object&gt; expectedOutput = new ArrayList&lt;&gt;(); // ("book", 2L, 12) expectedOutput.add(insertRecord("book", 2L, 12)); // ("book", 2L, 19) expectedOutput.add(insertRecord("book", 2L, 19)); // ("book", 2L, 11) expectedOutput.add(deleteRecord("book", 2L, 19)); expectedOutput.add(insertRecord("book", 2L, 11)); // ("fruit", 1L, 33) expectedOutput.add(insertRecord("fruit", 1L, 33)); // ("fruit", 1L, 44) // ("fruit", 1L, 22) expectedOutput.add(deleteRecord("fruit", 1L, 33)); expectedOutput.add(insertRecord("fruit", 1L, 22)); assertorWithoutRowNumber .assertOutputEquals("output wrong.", expectedOutput, testHarness.getOutput()); &#125;&#125; 123456789101112131415abstract class TopNFunctionTestBase &#123; // key 比较器的类加载工具类，生成一个 key 比较器实例 static GeneratedRecordComparator sortKeyComparator = new GeneratedRecordComparator("", "", new Object[0]) &#123; private static final long serialVersionUID = 1434685115916728955L; @Override public RecordComparator newInstance(ClassLoader classLoader) &#123; // compare(RowData o1, RowData o2) 方法中比较 o1 和 o2 的第 0 个元素，从小到大比较 return IntRecordComparator.INSTANCE; &#125; &#125;;&#125; 12345678abstract class TopNFunctionTestBase &#123; private int sortKeyIdx = 2; // key 选择器，比较 RowData 中的第 2 位置的元素 BinaryRowDataKeySelector sortKeySelector = new BinaryRowDataKeySelector(new int[]&#123;sortKeyIdx&#125;, inputRowType.getLogicalTypes());&#125; 输出结果为： 1234567Output element -&gt; +I(book,2,12) Output element -&gt; +I(book,2,19) Output element -&gt; -D(book,2,19) Output element -&gt; +I(book,2,11) Output element -&gt; +I(fruit,1,33) Output element -&gt; -D(fruit,1,33) Output element -&gt; +I(fruit,1,22) RetractableTopNFunction内部使用 TreeMap 进行 TopN 排序，可以对数据执行撤回操作，RowKind.UPDATE_BEFORE（-U）。 RetractableTopNFunction 中有如下属性，记录相同的 RowData 列表，使用 sortedMap 来进行 TopN 排序： 1234567891011// a map state stores mapping from sort key to records list/** * RowData &lt;-&gt; 相同的 RowData list，状态后端远程维护 */private transient MapState&lt;RowData, List&lt;RowData&gt;&gt; dataState;// a sorted map stores mapping from sort key to records count/** * RowData &lt;-&gt; 对应的记录个数，ValueState 中记录有序的 RowData */private transient ValueState&lt;SortedMap&lt;RowData, Long&gt;&gt; treeMap; RetractableTopNFunction 中的 processElement() 方法，按照数据的 RowKind 分别执行 emit 和 retract 操作： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081@Overridepublic void processElement(RowData input, Context ctx, Collector&lt;RowData&gt; out) throws Exception &#123; long currentTime = ctx.timerService().currentProcessingTime(); // register state-cleanup timer registerProcessingCleanupTimer(ctx, currentTime); initRankEnd(input); // 从状态后端中获取有序 RowData 的集合 SortedMap&lt;RowData, Long&gt; sortedMap = treeMap.value(); if (sortedMap == null) &#123; // 如果为 null，则新建一个，指定 sortKey comparator sortedMap = new TreeMap&lt;&gt;(sortKeyComparator); &#125; RowData sortKey = sortKeySelector.getKey(input); // RowKind.INSERT 或 RowKind.UPDATE_AFTER boolean isAccumulate = RowDataUtil.isAccumulateMsg(input); // erase row kind for further state accessing input.setRowKind(RowKind.INSERT); if (isAccumulate) &#123; // update sortedMap，记录当前 sortKey 的记录数到状态后端 if (sortedMap.containsKey(sortKey)) &#123; sortedMap.put(sortKey, sortedMap.get(sortKey) + 1); &#125; else &#123; sortedMap.put(sortKey, 1L); &#125; // emit if (outputRankNumber || hasOffset()) &#123; // the without-number-algorithm can't handle topN with offset, // so use the with-number-algorithm to handle offset emitRecordsWithRowNumber(sortedMap, sortKey, input, out); &#125; else &#123; emitRecordsWithoutRowNumber(sortedMap, sortKey, input, out); &#125; // 同步更新到状态后端 // update data state List&lt;RowData&gt; inputs = dataState.get(sortKey); if (inputs == null) &#123; // the sort key is never seen inputs = new ArrayList&lt;&gt;(); &#125; inputs.add(input); dataState.put(sortKey, inputs); &#125; else &#123; // emit updates first，先输出 update 操作，-U 代表执行撤回操作 if (outputRankNumber || hasOffset()) &#123; // the without-number-algorithm can't handle topN with offset, // so use the with-number-algorithm to handle offset retractRecordWithRowNumber(sortedMap, sortKey, input, out); &#125; else &#123; retractRecordWithoutRowNumber(sortedMap, sortKey, input, out); &#125; // and then update sortedMap if (sortedMap.containsKey(sortKey)) &#123; long count = sortedMap.get(sortKey) - 1; if (count == 0) &#123; sortedMap.remove(sortKey); &#125; else &#123; sortedMap.put(sortKey, count); &#125; &#125; else &#123; if (sortedMap.isEmpty()) &#123; if (lenient) &#123; LOG.warn(STATE_CLEARED_WARN_MSG); &#125; else &#123; throw new RuntimeException(STATE_CLEARED_WARN_MSG); &#125; &#125; else &#123; throw new RuntimeException( "Can not retract a non-existent record. This should never happen."); &#125; &#125; &#125; // 更新状态后端中记录的 sortedMap treeMap.update(sortedMap);&#125; RetractableTopNFunction 中的 emitRecordsWithRowNumber() 方法正常输出排序行： 1234567891011121314151617181920212223242526272829303132333435363738394041424344private void emitRecordsWithRowNumber( SortedMap&lt;RowData, Long&gt; sortedMap, RowData sortKey, RowData inputRow, Collector&lt;RowData&gt; out) throws Exception &#123; Iterator&lt;Map.Entry&lt;RowData, Long&gt;&gt; iterator = sortedMap.entrySet().iterator(); long currentRank = 0L; RowData currentRow = null; boolean findsSortKey = false; while (iterator.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123; Map.Entry&lt;RowData, Long&gt; entry = iterator.next(); RowData key = entry.getKey(); if (!findsSortKey &amp;&amp; key.equals(sortKey)) &#123; currentRank += entry.getValue(); currentRow = inputRow; // 从 sortedMap 中找到当前的 sortKey findsSortKey = true; &#125; else if (findsSortKey) &#123; List&lt;RowData&gt; inputs = dataState.get(key); if (inputs == null) &#123; // Skip the data if it's state is cleared because of state ttl. if (lenient) &#123; LOG.warn(STATE_CLEARED_WARN_MSG); &#125; else &#123; throw new RuntimeException(STATE_CLEARED_WARN_MSG); &#125; &#125; else &#123; int i = 0; while (i &lt; inputs.size() &amp;&amp; isInRankEnd(currentRank)) &#123; RowData prevRow = inputs.get(i); // 取出前一个row collectUpdateBefore(out, prevRow, currentRank); collectUpdateAfter(out, currentRow, currentRank); //输出当前行 currentRow = prevRow; // 前一行赋给当前行 currentRank += 1; i++; &#125; &#125; &#125; else &#123; currentRank += entry.getValue(); &#125; &#125; if (isInRankEnd(currentRank)) &#123; // there is no enough elements in Top-N, emit INSERT message for the new record. collectInsert(out, currentRow, currentRank); &#125;&#125; RetractableTopNFunction 中的 retractRecordWithRowNumber() 方法将撤回行从 sortedMap 中移除，并更新前一行的排位输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263private void retractRecordWithRowNumber( SortedMap&lt;RowData, Long&gt; sortedMap, RowData sortKey, RowData inputRow, Collector&lt;RowData&gt; out) throws Exception &#123; Iterator&lt;Map.Entry&lt;RowData, Long&gt;&gt; iterator = sortedMap.entrySet().iterator(); long currentRank = 0L; RowData prevRow = null; boolean findsSortKey = false; while (iterator.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123; Map.Entry&lt;RowData, Long&gt; entry = iterator.next(); RowData key = entry.getKey(); if (!findsSortKey &amp;&amp; key.equals(sortKey)) &#123; List&lt;RowData&gt; inputs = dataState.get(key); if (inputs == null) &#123; // Skip the data if it's state is cleared because of state ttl. if (lenient) &#123; LOG.warn(STATE_CLEARED_WARN_MSG); &#125; else &#123; throw new RuntimeException(STATE_CLEARED_WARN_MSG); &#125; &#125; else &#123; Iterator&lt;RowData&gt; inputIter = inputs.iterator(); while (inputIter.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123; RowData currentRow = inputIter.next(); if (!findsSortKey &amp;&amp; equaliser.equals(currentRow, inputRow)) &#123; prevRow = currentRow; findsSortKey = true; inputIter.remove(); &#125; else if (findsSortKey) &#123; collectUpdateBefore(out, prevRow, currentRank); collectUpdateAfter(out, currentRow, currentRank); prevRow = currentRow; &#125; currentRank += 1; &#125; if (inputs.isEmpty()) &#123; dataState.remove(key); // 将撤回的行从 sortedMap 中移除 &#125; else &#123; dataState.put(key, inputs); &#125; &#125; &#125; else if (findsSortKey) &#123; List&lt;RowData&gt; inputs = dataState.get(key); int i = 0; while (i &lt; inputs.size() &amp;&amp; isInRankEnd(currentRank)) &#123; RowData currentRow = inputs.get(i); // 上一行作为当前行 // 处理上一条数据 collectUpdateBefore(out, prevRow, currentRank); // 输出当前行 collectUpdateAfter(out, currentRow, currentRank); prevRow = currentRow; currentRank += 1; i++; &#125; &#125; else &#123; currentRank += entry.getValue(); &#125; &#125; if (isInRankEnd(currentRank)) &#123; // there is no enough elements in Top-N, emit DELETE message for the retract record. collectDelete(out, prevRow, currentRank); &#125;&#125; RetractableTopNFunctionTest12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Tests for &#123;@link RetractableTopNFunction&#125;. */public class RetractableTopNFunctionTest extends TopNFunctionTestBase &#123; @Override protected AbstractTopNFunction createFunction(RankType rankType, RankRange rankRange, boolean generateUpdateBefore, boolean outputRankNumber) &#123; return new RetractableTopNFunction( minTime.toMilliseconds(), maxTime.toMilliseconds(), inputRowType, sortKeyComparator, sortKeySelector, rankType, rankRange, generatedEqualiser, generateUpdateBefore, outputRankNumber); &#125; @Test public void testProcessRetractMessageWithNotGenerateUpdateBefore() throws Exception &#123; AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER, // 固定的 TopN 集合大小，1～2 new ConstantRankRange(1, 2), false, // 输出 TopN 的排序序号 true); OneInputStreamOperatorTestHarness&lt;RowData, RowData&gt; testHarness = createTestHarness(func); testHarness.open(); testHarness.processElement(insertRecord("book", 1L, 12)); testHarness.processElement(insertRecord("book", 2L, 19)); testHarness.processElement(insertRecord("book", 4L, 11)); testHarness.processElement(updateBeforeRecord("book", 1L, 12)); testHarness.processElement(insertRecord("book", 5L, 11)); testHarness.processElement(insertRecord("fruit", 4L, 33)); testHarness.processElement(insertRecord("fruit", 3L, 44)); testHarness.processElement(insertRecord("fruit", 5L, 22)); testHarness.close(); ConcurrentLinkedQueue&lt;Object&gt; output = testHarness.getOutput(); for (Object o : output) &#123; StreamRecord streamRecord = (StreamRecord) o; System.out.println("Output element -&gt; " + streamRecord.getValue()); &#125; List&lt;Object&gt; expectedOutput = new ArrayList&lt;&gt;(); // ("book", 1L, 12) // sortedMap -&gt; [("book", 1L, 12)] expectedOutput.add(insertRecord("book", 1L, 12, 1L)); // ("book", 2L, 19) // sortedMap -&gt; [("book", 1L, 12)],[("book", 2L, 19)] expectedOutput.add(insertRecord("book", 2L, 19, 2L)); // ("book", 4L, 11) // sortedMap -&gt; [("book", 4L, 11)],[("book", 1L, 12)],[("book", 2L, 19)] expectedOutput.add(updateAfterRecord("book", 4L, 11, 1L)); expectedOutput.add(updateAfterRecord("book", 1L, 12, 2L)); // UB ("book", 1L, 12)，撤回即将 ("book", 1L ,12) 从 sortedMap 中移除，("book", 2L, 19)的排序被更新为 2 // sortedMap -&gt; [("book", 4L, 11)],[("book", 2L, 19)] expectedOutput.add(updateAfterRecord("book", 2L, 19, 2L)); // ("book", 5L, 11) // sortedMap -&gt; [("book", 4L, 11),("book", 5L, 11)],[("book", 2L, 19)]，("book", 5L, 11) 的排序被更新为 2 expectedOutput.add(updateAfterRecord("book", 5L, 11, 2L)); // ("fruit", 4L, 33) // ("fruit", 3L, 44) expectedOutput.add(insertRecord("fruit", 4L, 33, 1L)); expectedOutput.add(insertRecord("fruit", 3L, 44, 2L)); // ("fruit", 5L, 22) expectedOutput.add(updateAfterRecord("fruit", 5L, 22, 1L)); expectedOutput.add(updateAfterRecord("fruit", 4L, 33, 2L)); assertorWithRowNumber.assertOutputEquals("output wrong.", expectedOutput, testHarness.getOutput()); &#125;&#125; 输出结果如下： 12345678910Output element -&gt; +I&#123;row1=+I(book,1,12), row2=+I(1)&#125;Output element -&gt; +I&#123;row1=+I(book,2,19), row2=+I(2)&#125;Output element -&gt; +U&#123;row1=+I(book,4,11), row2=+I(1)&#125;Output element -&gt; +U&#123;row1=+I(book,1,12), row2=+I(2)&#125;Output element -&gt; +U&#123;row1=+I(book,2,19), row2=+I(2)&#125;Output element -&gt; +U&#123;row1=+I(book,5,11), row2=+I(2)&#125;Output element -&gt; +I&#123;row1=+I(fruit,4,33), row2=+I(1)&#125;Output element -&gt; +I&#123;row1=+I(fruit,3,44), row2=+I(2)&#125;Output element -&gt; +U&#123;row1=+I(fruit,5,22), row2=+I(1)&#125;Output element -&gt; +U&#123;row1=+I(fruit,4,33), row2=+I(2)&#125; UpdatableTopNFunction支持更新流，是 RetractableTopNFunction 的简单实现版本，输入流中不能包含 DELETE 和 UPDATE_BEFORE 操作。 UpdatableTopNFunctionTest12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Tests for &#123;@link UpdatableTopNFunction&#125;. */public class UpdatableTopNFunctionTest extends TopNFunctionTestBase &#123; @Override protected AbstractTopNFunction createFunction( RankType rankType, RankRange rankRange, boolean generateUpdateBefore, boolean outputRankNumber) &#123; return new UpdatableTopNFunction( minTime.toMilliseconds(), maxTime.toMilliseconds(), inputRowType, rowKeySelector, sortKeyComparator, sortKeySelector, rankType, rankRange, generateUpdateBefore, outputRankNumber, cacheSize); &#125; @Test public void testVariableRankRange() throws Exception &#123; AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER, // TopN 的集合大小随着数据动态变化 new VariableRankRange(1), true, false); OneInputStreamOperatorTestHarness&lt;RowData, RowData&gt; testHarness = createTestHarness(func); testHarness.open(); testHarness.processElement(insertRecord("book", 2L, 19)); testHarness.processElement(updateAfterRecord("book", 2L, 18)); testHarness.processElement(insertRecord("fruit", 1L, 44)); testHarness.processElement(updateAfterRecord("fruit", 1L, 33)); testHarness.processElement(updateAfterRecord("fruit", 1L, 22)); testHarness.close(); ConcurrentLinkedQueue&lt;Object&gt; output = testHarness.getOutput(); for (Object o : output) &#123; StreamRecord streamRecord = (StreamRecord) o; System.out.println("Output element -&gt; " + streamRecord.getValue()); &#125; List&lt;Object&gt; expectedOutput = new ArrayList&lt;&gt;(); expectedOutput.add(insertRecord("book", 2L, 19)); expectedOutput.add(updateBeforeRecord("book", 2L, 19)); expectedOutput.add(updateAfterRecord("book", 2L, 18)); expectedOutput.add(insertRecord("fruit", 1L, 44)); expectedOutput.add(updateBeforeRecord("fruit", 1L, 44)); expectedOutput.add(updateAfterRecord("fruit", 1L, 33)); expectedOutput.add(updateBeforeRecord("fruit", 1L, 33)); expectedOutput.add(updateAfterRecord("fruit", 1L, 22)); assertorWithoutRowNumber .assertOutputEquals("output wrong.", expectedOutput, testHarness.getOutput()); &#125;&#125; 输出结果如下： 12345678Output element -&gt; +I(book,2,19)Output element -&gt; -U(book,2,19)Output element -&gt; +U(book,2,18)Output element -&gt; +I(fruit,1,44)Output element -&gt; -U(fruit,1,44)Output element -&gt; +U(fruit,1,33)Output element -&gt; -U(fruit,1,33)Output element -&gt; +U(fruit,1,22)]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink部署-flink-on-kubernetes]]></title>
    <url>%2F2020%2F09%2F29%2FFlink%E9%83%A8%E7%BD%B2-flink-on-kubernetes%2F</url>
    <content type="text"><![CDATA[kubernetes 是目前非常流行的容器编排系统，在其之上可以运行 web 服务、大数据处理等各类应用。这些应用被打包在非常轻量的容器中，我们通过声明的方式来告知 kubernetes 要如何部署和扩容这些程序，并对外提供服务。flink on kubernetes 可以得到一个健壮和高可扩的数据处理应用，并且能够更安全的和其他服务共享一个 kubernetes 集群。 本文将记录使用 kubernetes 部署 flink 应用的步骤。 Mac 安装 DockerDocker Desktop 下载地址：Docker 官网注册 DockerID 并登录。 安装 docker 命令行： 1$ brew install docker Minikube 搭建 Kubernetes 实验环境可以参考：Kubernetes 官网 安装 Minikube 校验 MacOS 是否支持虚拟化，运行如下命令出现 ‘VMX’： 1$ sysctl -a | grep -E --color 'machdep.cpu.features|VMX' 安装 kubectl 命令 123$ curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"$ chmod +x ./kubectl$ sudo mv ./kubectl /usr/local/bin/kubectl 或者直接使用 Homebrew 安装： 12$ brew install kubectl $ brew install kubernetes-cli 查看是否安装成功： 12$ kubectl version --clientClient Version: version.Info&#123;Major:"1", Minor:"19", GitVersion:"v1.19.2", GitCommit:"f5743093fd1c663cb0cbc89748f730662345d44d", GitTreeState:"clean", BuildDate:"2020-09-16T13:41:02Z", GoVersion:"go1.15", Compiler:"gc", Platform:"darwin/amd64"&#125; 安装 VirtualBoxVirtualBox 下载地址：VirtualBox 官网 安装 minikube 12$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 &amp;&amp; chmod +x minikube$ sudo mv minikube /usr/local/bin 或者直接使用 Homebrew 安装： 1$ brew install minikube 执行 minikube start该命令会下载 kubelet 和 kubeadm 程序，并构建一个完整的 k8s 集群。 1$ minikube start 查看 k8s podsMinikube 已经将命令 kubectl 指向虚拟机中的 k8s 集群了。 12345678$ kubectl get pods -Akube-system coredns-f9fd979d6-xjht6 1/1 Running 0 5h14mkube-system etcd-minikube 1/1 Running 0 5h14mkube-system kube-apiserver-minikube 1/1 Running 0 5h14mkube-system kube-controller-manager-minikube 1/1 Running 0 5h14mkube-system kube-proxy-ff8m8 1/1 Running 0 5h14mkube-system kube-scheduler-minikube 1/1 Running 0 5h14mkube-system storage-provisioner 1/1 Running 0 5h14m Flink 实时处理 demo我们可以编写一个简单的实时处理脚本，该脚本会从某个端口中读取文本，分割为单词，并且每 5 秒钟打印一次每个单词出现的次数。 12345678910DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env .socketTextStream("192.168.99.1", 9999) .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1);dataStream.print();env.execute("Window WordCount"); K8s 容器中的程序可以通过 IP 192.168.99.1 来访问 Minikube 宿主机上的服务。 demo 下载：flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar 构建 Docker 容器镜像flink 提供了一个官方的容器镜像，可以从 DockerHub 上下载镜像。官方镜像的 Dockerfile，以 1.8.1-scala_2.12 为例，大致内容如下： 1234567891011FROM openjdk:8-jreENV FLINK_HOME=/opt/flinkENV PATH=$FLINK_HOME/bin:$PATHRUN groupadd --system --gid=9999 flink &amp;&amp; \ useradd --system --home-dir $FLINK_HOME --uid=9999 --gid=flink flinkWORKDIR $FLINK_HOMERUN useradd flink &amp;&amp; \ wget -O flink.tgz &quot;$FLINK_TGZ_URL&quot; &amp;&amp; \ tar -xf flink.tgzENTRYPOINT [&quot;/docker-entrypoint.sh&quot;] 主要做了以下几件事情： 将 OpenJDK 1.8 作为基础镜像 下载并安装 flink 至 /opt/flink 目录中 添加 flink 用户和组等 下面我们以 flink:1.8.1-scala_2.12 作为基础镜像，编写新的 Dockerfile，将打包好的任务 jar 包放置进去。此外，新版 flink 已将 hadoop 依赖从官方发行版本中剥离，因此在打包镜像的时候也要包含进去。Hadoop jar 下载：flink-shaded-hadoop-2-uber-2.8.3-7.0.jar 1234567891011121314FROM flink:1.8.1-scala_2.12ARG hadoop_jarARG job_jarENV FLINK_CONF=$FLINK_HOME/conf/flink-conf.yamlRUN set -x &amp;&amp; \ sed -i -e &quot;s/jobmanager\.heap\.size:.*/jobmanager.heap.size: 128m/g&quot; $FLINK_CONF &amp;&amp; \ sed -i -e &quot;s/taskmanager\.heap\.size:.*/taskmanager.heap.size: 256m/g&quot; $FLINK_CONFCOPY --chown=flink:flink $hadoop_jar $job_jar $FLINK_HOME/lib/USER flink 将 docker 命令行指向 Minikube 中的 Docker 服务，这样打印出来的镜像才能被 k8s 使用： 1$ eval $(minikube docker-env) 移动到 Dockerfile 所在目录，开始构建镜像： 1234$ docker build \ --build-arg hadoop_jar=flink-shaded-hadoop-2-uber-2.8.3-7.0.jar \ --build-arg job_jar=flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar \ --tag flink-on-kubernetes:0.0.1 . 镜像打包完毕，可用于部署： 123$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEflink-on-kubernetes 0.0.1 ed4dfaf07cfe 5 hours ago 618MB 部署 JobManagerjobmanager.yml ： 1234567891011121314151617181920212223242526272829303132apiVersion: batch/v1kind: Jobmetadata: name: $&#123;JOB&#125;-jobmanagerspec: template: metadata: labels: app: flink instance: $&#123;JOB&#125;-jobmanager spec: restartPolicy: OnFailure containers: - name: jobmanager image: flink-on-kubernetes:0.0.1 command: ["/opt/flink/bin/standalone-job.sh"] args: ["start-foreground", "-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager", "-Dparallelism.default=1", "-Dblob.server.port=6124", "-Dqueryable-state.server.ports=6125", "-Dstate.savepoints.dir=hdfs://192.168.99.1:9000/flink/savepoints/", ] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob - containerPort: 6125 name: query - containerPort: 8081 name: ui ${JOB} 变量可以使用 envsubst 命令来替换 容器的入口修改为 standalone-job.sh JobManager 的 rpc 地址修改为了 k8s Service 的名称，集群中的其他组件将通过这个名称来访问 JobManager。 为 Flink Blob Server &amp; Queryable State Server 指定默认端口号 使用 kubectl 命令创建 JobManager pod，并查看状态： 12345$ export JOB=flink-on-kubernetes$ envsubst &lt;jobmanager.yml | kubectl create -f -$ kubectl get podNAME READY STATUS RESTARTS AGEflink-on-kubernetes-jobmanager-dzhcs 1/1 Running 0 77m 创建一个 k8s Service 把 JobManager 的端口开放出来，以便 TaskManager 前来注册。service.yml： 123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: $&#123;JOB&#125;-jobmanagerspec: selector: app: flink instance: $&#123;JOB&#125;-jobmanager type: NodePort ports: - name: rpc port: 6123 - name: blob port: 6124 - name: query port: 6125 - name: ui port: 8081 使用 kubectl 命令创建 JobManager service，并查看状态： 12345678910$ envsubst &lt;service.yml | kubectl create -f -$ kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEflink-on-kubernetes-jobmanager NodePort 10.104.157.70 &lt;none&gt; 6123:30261/TCP,6124:31158/TCP,6125:30509/TCP,8081:30262/TCP 89m$ minikube service $JOB-jobmanager --urlhttp://192.168.99.100:30261http://192.168.99.100:31158http://192.168.99.100:30509http://192.168.99.100:30262 部署 TaskManagertaskmanager.yml ： 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: $&#123;JOB&#125;-taskmanagerspec: selector: matchLabels: app: flink instance: $&#123;JOB&#125;-taskmanager replicas: 1 template: metadata: labels: app: flink instance: $&#123;JOB&#125;-taskmanager spec: containers: - name: taskmanager image: flink-on-kubernetes:0.0.1 command: ["/opt/flink/bin/taskmanager.sh"] args: ["start-foreground", "-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"] 使用 kubectl 命令创建 TaskManager pod，并查看状态： 1234$ kubectl get podNAME READY STATUS RESTARTS AGEflink-on-kubernetes-jobmanager-dzhcs 1/1 Running 0 77mflink-on-kubernetes-taskmanager-64b7cc4bf-9t6cr 1/1 Running 2 77m 至此，Flink 脚本集群已经在运行中了。在监听终端下输入如下内容： 123$ nc -lk 9999hello worldhello flink 打开另一个终端，查看 TaskManager 的标准输出日志： 1234$ kubectl logs -f -l instance=$JOB-taskmanager(hello,2)(flink,1)(world,1)]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用命令]]></title>
    <url>%2F2020%2F05%2F24%2FHexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[使用 Hexo + Github 搭建个人博客的常用命令，在此记录下。 init新建一个网站 1hexo init "文件夹名称" version显示 Hexo 版本。 1hexo version new1hexo new "新文章名" generate生成静态网页 1hexo g server启动服务器。默认情况下，访问网址为：http://localhost:4000/ 1hexo s deploy部署网站。 1hexo d list列出网站资料 1hexo list &lt;type&gt; 参考资料Hexo 指令]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令]]></title>
    <url>%2F2020%2F05%2F24%2FGit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Git 命令比较多，会经常 fork git 仓库，fork 仓库同步原远程仓库的操作，也会经常忘记，特此记录下。 新建代码库123456789# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url] 配置Git 的设置文件为 .gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 123456789# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name "[name]"$ git config [--global] user.email "[email address]" 增加/删除文件123456789101112131415161718192021# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 代码提交123456789101112131415161718# 提交暂存区到仓库区$ git commit -m [message]# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a# 提交时显示所有diff信息$ git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 分支12345678910111213141516171819202122232425262728293031323334353637383940414243# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 标签1234567891011121314151617181920212223242526# 列出所有tag$ git tag# 新建一个tag在当前commit$ git tag [tag]# 新建一个tag在指定commit$ git tag [tag] [commit]# 删除本地tag$ git tag -d [tag]# 删除远程tag$ git push origin :refs/tags/[tagName]# 查看tag信息$ git show [tag]# 提交指定tag$ git push [remote] [tag]# 提交所有tag$ git push [remote] --tags# 新建一个分支，指向某个tag$ git checkout -b [branch] [tag] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序$ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码$ git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog 远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop fork仓库合并原仓库merge 前的设定 进入本地仓库目录 查看远程仓库的路径 1git remote -v 将远程仓库设置成 fork 仓库的上游代码库 1git remote add upstream 远程仓库.git 检查本地提交执行命令 git status 检查本地是否有未提交的修改。如果有，则把你本地的有效修改，先从本地仓库推送到你的github仓库。最后再执行一次 git status 检查本地已无未提交的修改 1234git add -A 或者 git add filenamegit commit -m "your note"git push origin mastergit status merge 的相关命令 抓取原仓库的更新 1git fetch upstream 远程仓库.git 切换到 fork 仓库的 master 分支 1git checkout master 合并远程的 master 分支 1git merge upstream/master 推送 fork 仓库的修改 1git push 其他12# 生成一个可供发布的压缩包$ git archive 参考资料常用 Git 命令清单Github进行fork后如何与原仓库同步：重新fork很省事，但不如反复练习版本合并]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink源码剖析-flink-streaming-java_JobGraph]]></title>
    <url>%2F2020%2F04%2F22%2FFlink%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-flink-streaming-java_JobGraph%2F</url>
    <content type="text"><![CDATA[本文主要围绕 Flink 源码中 flink-streaming-java 模块。介绍下 StreamGraph 转成 JobGraph 的过程等。 StreamGraph 和 JobGraph 都是在 Client 端生成的，也就是说我们可以在 IDE 中通过断点调试观察 StreamGraph 和 JobGraph 的生成过程。 前置调用从 StreamExecutionEnvironment 中的 execute() 方法一直往下跟： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Streaming 程序的提交入口 */public JobExecutionResult execute() throws Exception &#123; return execute(DEFAULT_JOB_NAME);&#125;/** * 生成 StreamGraph */public JobExecutionResult execute(String jobName) throws Exception &#123; Preconditions.checkNotNull(jobName, "Streaming Job name should not be null."); return execute(getStreamGraph(jobName));&#125;/** * 生成 JobGraph ，提交任务，并响应 JobListeners */@Internalpublic JobExecutionResult execute(StreamGraph streamGraph) throws Exception &#123; // 异步执行 final JobClient jobClient = executeAsync(streamGraph); try &#123; final JobExecutionResult jobExecutionResult; if (configuration.getBoolean(DeploymentOptions.ATTACHED)) &#123; jobExecutionResult = jobClient.getJobExecutionResult(userClassloader).get(); &#125; else &#123; jobExecutionResult = new DetachedJobExecutionResult(jobClient.getJobID()); &#125; jobListeners.forEach(jobListener -&gt; jobListener.onJobExecuted(jobExecutionResult, null)); return jobExecutionResult; &#125; catch (Throwable t) &#123; jobListeners.forEach(jobListener -&gt; &#123; jobListener.onJobExecuted(null, ExceptionUtils.stripExecutionException(t)); &#125;); ExceptionUtils.rethrowException(t); // never reached, only make javac happy return null; &#125;&#125; 下面我们详细看看 StreamExecutionEnvironment 中的 executeAsync 方法： 12345678910111213141516171819202122232425262728293031323334353637383940/** * 根据 execution.target 配置反射得到 PipelineExecutorFactory，拿出工厂类对应的 PipelineExecutor，执行其 execute 方法 * execute的主要工作是将 StreamGraph 转成了 JobGraph，并创建相应的 ClusterClient 完成提交任务的操作。 */@Internalpublic JobClient executeAsync(StreamGraph streamGraph) throws Exception &#123; checkNotNull(streamGraph, "StreamGraph cannot be null."); checkNotNull(configuration.get(DeploymentOptions.TARGET), "No execution.target specified in your configuration file."); // SPI机制 // 根据flink Configuration中的"execution.target"加载 PipelineExecutorFactory // PipelineExecutorFactory 的实现类在flink-clients包或者flink-yarn包里，因此需要在pom.xml中添加此依赖 final PipelineExecutorFactory executorFactory = executorServiceLoader.getExecutorFactory(configuration); // 反射出的 PipelineExecutorFactory 类不能为空 checkNotNull( executorFactory, "Cannot find compatible factory for specified execution.target (=%s)", configuration.get(DeploymentOptions.TARGET)); // 根据加载到的 PipelineExecutorFactory 工厂类，获取其对应的 PipelineExecutor， // 并执行 PipelineExecutor 的 execute() 方法，将 StreamGraph 转成 JobGraph CompletableFuture&lt;JobClient&gt; jobClientFuture = executorFactory .getExecutor(configuration) .execute(streamGraph, configuration); // 异步调用的返回结果 try &#123; JobClient jobClient = jobClientFuture.get(); jobListeners.forEach(jobListener -&gt; jobListener.onJobSubmitted(jobClient, null)); return jobClient; &#125; catch (Throwable t) &#123; jobListeners.forEach(jobListener -&gt; jobListener.onJobSubmitted(null, t)); ExceptionUtils.rethrow(t); // make javac happy, this code path will not be reached return null; &#125;&#125; executeAsync 有涉及到 PipelineExecutorFactory 和 PipelineExecutor 。PipelineExecutorFactory 是通过 SPI ServiceLoader 加载的，我们看下 flink-clients 模块的 META-INF.services 文件： PipelineExecutorFactory 的实现子类，分别对应着 Flink 的不同部署模式，local、standalone、yarn、kubernets 等： 这里我们只看下 LocalExecutorFactory 的实现： 12345678910111213141516171819@Internalpublic class LocalExecutorFactory implements PipelineExecutorFactory &#123; /** * execution.target 配置项对应的值为 "local" */ @Override public boolean isCompatibleWith(final Configuration configuration) &#123; return LocalExecutor.NAME.equalsIgnoreCase(configuration.get(DeploymentOptions.TARGET)); &#125; /** * 直接 new 一个 LocalExecutor 返回 */ @Override public PipelineExecutor getExecutor(final Configuration configuration) &#123; return new LocalExecutor(); &#125;&#125; PipelineExecutor 的实现子类与 PipelineExecutorFactory 与工厂类一一对应，负责将 StreamGraph 转成 JobGraph，并生成 ClusterClient 执行任务的提交： LocalExecutorFactory 对应的 LocalExecutor 实现如下： 1234567891011121314151617181920212223242526272829303132333435363738@Internalpublic class LocalExecutor implements PipelineExecutor &#123; public static final String NAME = "local"; @Override public CompletableFuture&lt;JobClient&gt; execute(Pipeline pipeline, Configuration configuration) throws Exception &#123; checkNotNull(pipeline); checkNotNull(configuration); // we only support attached execution with the local executor. checkState(configuration.getBoolean(DeploymentOptions.ATTACHED)); // StreamGraph 转成 JobGraph final JobGraph jobGraph = getJobGraph(pipeline, configuration); // local 模式，本地启动一个 Mini Cluster final MiniCluster miniCluster = startMiniCluster(jobGraph, configuration); // 创建 MiniClusterClient ，准备提交任务 final MiniClusterClient clusterClient = new MiniClusterClient(configuration, miniCluster); // 提交任务 CompletableFuture&lt;JobID&gt; jobIdFuture = clusterClient.submitJob(jobGraph); jobIdFuture .thenCompose(clusterClient::requestJobResult) .thenAccept((jobResult) -&gt; clusterClient.shutDownCluster()); return jobIdFuture.thenApply(jobID -&gt; new ClusterClientJobClientAdapter&lt;&gt;(() -&gt; clusterClient, jobID)); &#125; private JobGraph getJobGraph(Pipeline pipeline, Configuration configuration) &#123; ... // 这里调用 FlinkPipelineTranslationUtil 的 getJobGraph() 方法 return FlinkPipelineTranslationUtil.getJobGraph(pipeline, configuration, 1); &#125;&#125; 回归主题，我们看下 FlinkPipelineTranslationUtil 的 getJobGraph() 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public static JobGraph getJobGraph( Pipeline pipeline, Configuration optimizerConfiguration, int defaultParallelism) &#123; // 通过反射得到 FlinkPipelineTranslator FlinkPipelineTranslator pipelineTranslator = getPipelineTranslator(pipeline); return pipelineTranslator.translateToJobGraph(pipeline, optimizerConfiguration, defaultParallelism);&#125;private static FlinkPipelineTranslator getPipelineTranslator(Pipeline pipeline) &#123; PlanTranslator planToJobGraphTransmogrifier = new PlanTranslator(); if (planToJobGraphTransmogrifier.canTranslate(pipeline)) &#123; return planToJobGraphTransmogrifier; &#125; FlinkPipelineTranslator streamGraphTranslator = reflectStreamGraphTranslator(); // 其实就是判断当前的 Pipeline 实例是不是 StreamGraph if (!streamGraphTranslator.canTranslate(pipeline)) &#123; throw new RuntimeException("Translator " + streamGraphTranslator + " cannot translate " + "the given pipeline " + pipeline + "."); &#125; return streamGraphTranslator;&#125;private static FlinkPipelineTranslator reflectStreamGraphTranslator() &#123; Class&lt;?&gt; streamGraphTranslatorClass; try &#123; streamGraphTranslatorClass = Class.forName( // 因为这个类在 flink-streaming-java 模块中，FlinkPipelineTranslationUtil 在 flink-clients 模块中， // flink-clients 模块没有引入 flink-streaming-java 模块，所以只能通过反射拿到 "org.apache.flink.streaming.api.graph.StreamGraphTranslator", true, FlinkPipelineTranslationUtil.class.getClassLoader()); &#125; catch (ClassNotFoundException e) &#123; throw new RuntimeException("Could not load StreamGraphTranslator.", e); &#125; FlinkPipelineTranslator streamGraphTranslator; try &#123; streamGraphTranslator = (FlinkPipelineTranslator) streamGraphTranslatorClass.newInstance(); &#125; catch (InstantiationException | IllegalAccessException e) &#123; throw new RuntimeException("Could not instantiate StreamGraphTranslator.", e); &#125; return streamGraphTranslator;&#125; 接着走到 StreamGraphTranslator 的 translateToJobGraph 方法： 1234567891011121314151617181920212223242526272829303132public class StreamGraphTranslator implements FlinkPipelineTranslator &#123; /** * 其实就是调用 StreamGraph 自己的 getJobGraph 方法生成 JobGraph */ @Override public JobGraph translateToJobGraph( Pipeline pipeline, Configuration optimizerConfiguration, int defaultParallelism) &#123; checkArgument(pipeline instanceof StreamGraph, "Given pipeline is not a DataStream StreamGraph."); StreamGraph streamGraph = (StreamGraph) pipeline; return streamGraph.getJobGraph(null); &#125; @Override public String translateToJSONExecutionPlan(Pipeline pipeline) &#123; checkArgument(pipeline instanceof StreamGraph, "Given pipeline is not a DataStream StreamGraph."); StreamGraph streamGraph = (StreamGraph) pipeline; return streamGraph.getStreamingPlanAsJSON(); &#125; @Override public boolean canTranslate(Pipeline pipeline) &#123; return pipeline instanceof StreamGraph; &#125;&#125; StreamGraph 到 JobGraph 的转换接着走到 StreamGraph 中的 getJobGraph() 方法： 123public JobGraph getJobGraph(@Nullable JobID jobID) &#123; return StreamingJobGraphGenerator.createJobGraph(this, jobID);&#125; 接着走到 StreamingJobGraphGenerator 的 createJobGraph() 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 传入 StreamGraph，生成 JobGraph */public static JobGraph createJobGraph(StreamGraph streamGraph) &#123; return createJobGraph(streamGraph, null);&#125;public static JobGraph createJobGraph(StreamGraph streamGraph, @Nullable JobID jobID) &#123; return new StreamingJobGraphGenerator(streamGraph, jobID).createJobGraph();&#125;/** * 核心方法 * StreamGraph 转 JobGraph 的整体流程 */private JobGraph createJobGraph() &#123; preValidate(); // make sure that all vertices start immediately // 设置调度模式，streaming 模式下，调度模式是所有节点一起启动 jobGraph.setScheduleMode(streamGraph.getScheduleMode()); // 1. 广度优先遍历 StreamGraph 并且为每个 SteamNode 生成一个唯一确定的 hash id // Generate deterministic hashes for the nodes in order to identify them across // submission iff they didn't change. // 保证如果提交的拓扑没有改变，则每次生成的 hash id 都是一样的，这里只要保证 source 的顺序是确定的，就可以保证最后生产的 hash id 不变 // 它是利用 input 节点的 hash 值及该节点在 map 中位置（实际上是 map.size 算的）来计算确定的 Map&lt;Integer, byte[]&gt; hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph); // Generate legacy version hashes for backwards compatibility // 这个设置主要是为了防止 hash 机制变化时出现不兼容的情况 List&lt;Map&lt;Integer, byte[]&gt;&gt; legacyHashes = new ArrayList&lt;&gt;(legacyStreamGraphHashers.size()); for (StreamGraphHasher hasher : legacyStreamGraphHashers) &#123; legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph)); &#125; Map&lt;Integer, List&lt;Tuple2&lt;byte[], byte[]&gt;&gt;&gt; chainedOperatorHashes = new HashMap&lt;&gt;(); // 2. 最重要的函数，生成 JobVertex/JobEdge/IntermediateDataSet 等，并尽可能地将多个 StreamNode 节点 chain 在一起 setChaining(hashes, legacyHashes, chainedOperatorHashes); // 3. 将每个 JobVertex 的入边集合也序列化到该 JobVertex 的 StreamConfig 中 (出边集合已经在 setChaining 的时候写入了) setPhysicalEdges(); // 4. 根据 group name，为每个 JobVertex 指定所属的 SlotSharingGroup 以及设置 CoLocationGroup setSlotSharingAndCoLocation(); // 5. 其他设置 // 设置 ManagedMemory 因子 setManagedMemoryFraction( Collections.unmodifiableMap(jobVertices), Collections.unmodifiableMap(vertexConfigs), Collections.unmodifiableMap(chainedConfigs), id -&gt; streamGraph.getStreamNode(id).getMinResources(), id -&gt; streamGraph.getStreamNode(id).getManagedMemoryWeight()); // checkpoint相关的配置 configureCheckpointing(); // savepoint相关的配置 jobGraph.setSavepointRestoreSettings(streamGraph.getSavepointRestoreSettings()); // 用户的第三方依赖包就是在这里（cacheFile）传给 JobGraph JobGraphGenerator.addUserArtifactEntries(streamGraph.getUserArtifacts(), jobGraph); // set the ExecutionConfig last when it has been finalized try &#123; // 将 StreamGraph 的 ExecutionConfig 序列化到 JobGraph 的配置中 jobGraph.setExecutionConfig(streamGraph.getExecutionConfig()); &#125; catch (IOException e) &#123; throw new IllegalConfigurationException("Could not serialize the ExecutionConfig." + "This indicates that non-serializable types (like custom serializers) were registered"); &#125; return jobGraph;&#125;]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink源码剖析-flink-streaming-java_StreamGraph]]></title>
    <url>%2F2020%2F04%2F22%2FFlink%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-flink-streaming-java_StreamGraph%2F</url>
    <content type="text"><![CDATA[本文主要围绕 Flink 源码中 flink-streaming-java 模块。介绍如何使用 DataStream API 进行 Flink 流任务开发，flink-streaming-java 模块中的一些重要类，贯穿着介绍下从 DataStreamAPI 到 StreamGraph 的构建过程。 DataStream API使用一览使用 DataStream API 通常有以下步骤： 如何创建 Environment(Local、Remote) 并设置属性 setParallelism(int)：StreamExecutionEnvironment setMaxParallelism(int)：StreamExecutionEnvironment setBufferTimeout(long)：StreamExecutionEnvironment enableCheckpointing(long,CheckpointingMode)：StreamExecutionEnvironment setStateBackend(StateBackend)：StreamExecutionEnvironment setStreamTimeCharacteristic(TimeCharacteristic)：void 如何读取数据？添加 Source 数据源获得 DataStream fromElements(OUT …): DataStreamSource … readTextFile(String): DataStreamSource … readFile(FileInputFormat,String): DataStreamSource … socketTextStream(String ,int ,String ,long): DataStreamSource … createInput(InputFormat&lt;OUT,?&gt;,TypeInformation): DataStreamSource … addSource(SourceFunction,TypeInformation): DataStreamSource … 如何操作转换数据？ Basic Transformationsmap、filter、flatMap KeyedStream TransformationskeyBy、aggregations、reduce MultiStream Transformationsunion、connect、coMap、coFlatMap、split、select Distribution Transformations物理分组： 关系 表示 图示 global 全部发往第1个task broadcast 广播，复制上游的数据发送到所有下游节点 forward 上下游并发度一样时一对一发送 shuffle 随机均匀分配 reblance Round-Robin（轮流分配） rescale Local Round-Robin (本地轮流分配)，只会看到本机的实例 partitionCustom 自定义单播 如何输出数据？添加 Sink writeAsText(String path): DataStreamSink … writeAsCsv(String path): DataStreamSink … addSink(SinkFunction sinkFunction): DataStreamSink 如何提交执行？DataStream 通过不同的算子不停地在 DataStream 上实现转换过滤等逻辑，最终将结果输出到 DataSink 中。在 StreamExecutionEnvironment 内部使用一个 List&lt;StreamTransformation&lt;?&gt;&gt; transformations 来保留生成 DataStream 的所有转换。 execute()：JobExecutionResult 我们看下基于 Flink DataStream API 的自带 WordCount 示例：实时统计单词数量，每来一个计算一次并输出一次。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class WordCount &#123; // ************************************************************************* // PROGRAM // ************************************************************************* public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); // 1. 设置运行环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); // 2. 配置数据源读取数据 DataStream&lt;String&gt; text; if (params.has("input")) &#123; // read the text file from given input path text = env.readTextFile(params.get("input")); &#125; else &#123; // get default test text data text = env.fromElements(new String[] &#123; "miao,She is a programmer", "wu,He is a programmer", "zhao,She is a programmer" &#125;); &#125; // 3. 进行一系列转换 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = // split up the lines in pairs (2-tuples) containing: (word,1) text.flatMap(new Tokenizer()) // group by the tuple field "0" and sum up tuple field "1" .keyBy(0).sum(1); // 4. 配置数据汇写出数据 if (params.has("output")) &#123; counts.writeAsText(params.get("output")); &#125; else &#123; System.out.println("Printing result to stdout. Use --output to specify output path."); counts.print(); &#125; // 5. 提交执行 env.execute("Streaming WordCount"); &#125; // ************************************************************************* // USER FUNCTIONS // ************************************************************************* public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123; // normalize and split the line String[] tokens = value.toLowerCase().split("\\W+"); // emit the pairs for (String token : tokens) &#123; if (token.length() &gt; 0) &#123; out.collect(new Tuple2&lt;&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 源码剖析StreamExecutionEnvironmentStreamExecutionEnvironment 是 Flink 流处理任务执行的上下文，是我们编写 Flink 程序的入口。根据执行环境的不同，选择不同的 StreamExecutionEnvironment 类，有 LocalStreamEnvironment、RemoteStreamEnvironment 等。如下图： StreamExecutionEnvironment 依赖 ExecutionConfig 类来设置并行度等，依赖 CheckpointConfig 设置 Checkpointing 等相关属性。 这里再补充说明下 StreamExecutionEnvironment类中的重要属性和方法： TransformationTransformation 代表了从一个或多个 DataStream 生成新 DataStream 的操作。在 DataStream 上通过 map 等算子不断进行转换，就得到了由 Transformation构成的图。当需要执行的时候，底层的这个图就会被转换成 StreamGraph 。 Transformation 有很多子类，如 SourceTransformation、OneInputTransformation、TwoInputTransformation、SideOutputTransformation 等，分别对应了 DataStream 上的不同转换操作。 每一个 Transformation 都有一个关联 id，这个 id 是全局递增的，还有 uid、slotSharingGroup、parallelism 等信息。 查看 Transformation 的其中两个子类 OneInputTransformation、TwoInputTransformation 的实现，都对应有输入 Transformation，也正是基于此才能还原出 DAG 的拓扑结构。 Transformation 在运行时并不对应着一个物理转换操作，有一些操作只是逻辑层面上的，比如 split/select/partitioning 等。Transformations 组成的 graph ，也就是我们写代码时的图结构如下： 123456789101112131415161718192021222324 Source Source + + | | v vRebalance HashPartition + + | | | | +------&gt;Union&lt;------+ + | v Split + | v Select + v Map + | v Sink 但是，在运行时将生成如下操作图，split/select/partitioning 等转换操作会被编码到边中，这个边连接 sources 和 map 操作： 123456789Source Source + + | | | | +-------&gt;Map&lt;-------+ + | v Sink DataStream一个 DataStream 就代表了同一种类型元素构成的数据流。通过对 DataStream 应用 map/filter 等操作，就可以将一个 DataStream 转换成另一个 DataStream 。这个转换的过程就是根据不同的操作生成不同的 Transformation ，并将其加入到 StreamExecutionEnvironment 的 transformations 列表中。 DataStream 的子类包括 DataStreamSource、KeyedStream、IterativeStream、SingleOutputStreamOperator。 除了 DataStream 及其子类以外，其它的表征数据流的类还有 ConnectedStreams、WindowedStream、AllWindowedStream，这些会在后续的文章中陆续介绍。 DataStream 类中的重要属性和方法： 下面我们看下 map 操作是如何被添加进来的： 1234public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; map(MapFunction&lt;T, R&gt; mapper, TypeInformation&lt;R&gt; outputType) &#123; // 将 MapFunction 封装成 StreamMap 这个 StreamOperator return transform("Map", outputType, new StreamMap&lt;&gt;(clean(mapper)));&#125; 12345678@PublicEvolvingpublic &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; transform( String operatorName, TypeInformation&lt;R&gt; outTypeInfo, OneInputStreamOperator&lt;T, R&gt; operator) &#123; return doTransform(operatorName, outTypeInfo, SimpleOperatorFactory.of(operator));&#125; 接着我们看下其中一个比较重要的方法 doTransform ： 12345678910111213141516171819202122232425protected &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; doTransform( String operatorName, TypeInformation&lt;R&gt; outTypeInfo, StreamOperatorFactory&lt;R&gt; operatorFactory) &#123; // read the output type of the input Transform to coax out errors about MissingTypeInfo transformation.getOutputType(); // 构造 Transformation OneInputTransformation&lt;T, R&gt; resultTransform = new OneInputTransformation&lt;&gt;( this.transformation, operatorName, operatorFactory, outTypeInfo, environment.getParallelism()); // 将 Transformation 封装进 SingleOutputStreamOperator 返回 @SuppressWarnings(&#123;"unchecked", "rawtypes"&#125;) SingleOutputStreamOperator&lt;R&gt; returnStream = new SingleOutputStreamOperator(environment, resultTransform); // 添加到 StreamExecutionEnvironment 的 transformations 列表中 getExecutionEnvironment().addOperator(resultTransform); return returnStream;&#125; StreamOperator在操作 DataStream 的时候，比如 DataStream.map(MapFunction&lt;T, R&gt; mapper) 时，都会传入一个自定义的 Function 。那么这些信息是如何保存在 Transformation 中的呢？这里就引入了一个新的接口 StreamOpertor ，DataStream 上的每一个 Transformation 都对应了一个 StreamOperator，StreamOperator 是运行时的具体实现，会决定 UDF 的调用方式。 StreamOperator 的类继承关系如下： 接口 StreamOpertor 定义了对一个具体的算子的生命周期的管理。StreamOperator 的两个子接口 OneInputStreamOperator 和 TwoInputStreamOperator 提供了数据流中具体元素的操作方法，而 AbstractUdfStreamOperator 抽象子类则提供了自定义处理函数对应的算子的基本实现： 下面我们还是拿 map 举例，map 操作对应的 StreamOperator 为 StreamMap ，继承了 AbstractUdfStreamOperator 类，实现了 OneInputStreamOperator 接口： 1234567891011121314151617@Internalpublic class StreamMap&lt;IN, OUT&gt; extends AbstractUdfStreamOperator&lt;OUT, MapFunction&lt;IN, OUT&gt;&gt; implements OneInputStreamOperator&lt;IN, OUT&gt; &#123; private static final long serialVersionUID = 1L; public StreamMap(MapFunction&lt;IN, OUT&gt; mapper) &#123; super(mapper); chainingStrategy = ChainingStrategy.ALWAYS; &#125; @Override public void processElement(StreamRecord&lt;IN&gt; element) throws Exception &#123; output.collect(element.replace(userFunction.map(element.getValue()))); &#125;&#125; 以上，我们可以知道通过 DataStream -&gt; Function -&gt; StreamOperator -&gt; StreamTransformation 这种依赖关系，就可以完成 DataStream 的转换，并且可以保存数据流和应用在流上的算子之间的关系。 Function StreamGraphStreamGraph 是在 Client 端构造的。了解 StreamGraph 之前我们首先要知道 StreamGraphGenerator 这个类，它会基于 StreamExecutionEnvironment 的 transformations 列表来生成 StreamGraph。 首先看下 StreamGraphGenerator 的 generate() 方法，这个方法会由触发程序执行的方法 StreamExecutionEnvironment.execute() 调用到： 12345678910111213141516171819202122232425262728public StreamGraph generate() &#123; streamGraph = new StreamGraph(executionConfig, checkpointConfig, savepointRestoreSettings); streamGraph.setStateBackend(stateBackend); streamGraph.setChaining(chaining); streamGraph.setScheduleMode(scheduleMode); streamGraph.setUserArtifacts(userArtifacts); streamGraph.setTimeCharacteristic(timeCharacteristic); streamGraph.setJobName(jobName); streamGraph.setBlockingConnectionsBetweenChains(blockingConnectionsBetweenChains); alreadyTransformed = new HashMap&lt;&gt;(); /** * 遍历 transformations 列表，递归调用 transform 方法。 * 对于每一个 Transformation ，确保当前上游已经完成转换，转换成 StreamGraph 中的 StreamNode，并为上下游节点添加 StreamEdge */ for (Transformation&lt;?&gt; transformation: transformations) &#123; transform(transformation); &#125; final StreamGraph builtStreamGraph = streamGraph; alreadyTransformed.clear(); alreadyTransformed = null; streamGraph = null; return builtStreamGraph;&#125; 在遍历 List 生成 StreamGraph 时，会递归调用其 transform 方法。对于每一个 Transformation ，确保当前其上游已经完成转换。最终，部分 Transformation 节点被转换为 StreamGraph 中的 StreamNode 节点，并会为上下游节点添加边 StreamEdge。下面看下 transform() 方法： 12345678910111213141516171819202122232425262728293031323334353637383940private Collection&lt;Integer&gt; transform(Transformation&lt;?&gt; transform) &#123; if (alreadyTransformed.containsKey(transform)) &#123; return alreadyTransformed.get(transform); &#125; // 对于不同类型的 Transformation，分别调用对应的转换方法 // 只有 OneInputTransformation、TwoInputTransformation、SourceTransformation、SinkTransformation 会生成 StreamNode， // 会生成 StreamNode. // 像 Partitioning, split/select, union 这些是不包含物理转换操作的，会生成一个带有特定属性的虚拟节点， // 当添加一条有虚拟节点指向下游节点的边时，会找到虚拟节点上游的物理节点，在两个物理节点之间添加边，并把虚拟转换操作的属性附着上去。 Collection&lt;Integer&gt; transformedIds; if (transform instanceof OneInputTransformation&lt;?, ?&gt;) &#123; transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform); &#125; else if (transform instanceof TwoInputTransformation&lt;?, ?, ?&gt;) &#123; transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform); &#125; else if (transform instanceof SourceTransformation&lt;?&gt;) &#123; transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof SinkTransformation&lt;?&gt;) &#123; transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof UnionTransformation&lt;?&gt;) &#123; transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof SplitTransformation&lt;?&gt;) &#123; transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof SelectTransformation&lt;?&gt;) &#123; transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof FeedbackTransformation&lt;?&gt;) &#123; transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof CoFeedbackTransformation&lt;?&gt;) &#123; transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof PartitionTransformation&lt;?&gt;) &#123; transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform); &#125; else if (transform instanceof SideOutputTransformation&lt;?&gt;) &#123; transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform); &#125; else &#123; throw new IllegalStateException("Unknown transformation: " + transform); &#125; return transformedIds;&#125; 对于另外一部分 Transformation ，如 partitioning, split/select, union，并不包含真正的物理转换操作，是不会生成 StreamNode 的，而是生成一个带有特定属性的虚拟节点。当添加一条有虚拟节点指向下游节点的边时，会找到虚拟节点上游的物理节点，在两个物理节点之间添加边，并把虚拟转换操作的属性附着上去。下面我们首先看下 transformOneInputTransform() 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243private &lt;IN, OUT&gt; Collection&lt;Integer&gt; transformOneInputTransform(OneInputTransformation&lt;IN, OUT&gt; transform) &#123; // 首先确保上游节点完成转换 Collection&lt;Integer&gt; inputIds = transform(transform.getInput()); // the recursive call might have already transformed this // 由于是递归调用的，可能已经完成了转换 if (alreadyTransformed.containsKey(transform)) &#123; return alreadyTransformed.get(transform); &#125; // 确定共享资源组，如果用户没有指定，默认是 default String slotSharingGroup = determineSlotSharingGroup(transform.getSlotSharingGroup(), inputIds); // 向 StreamGraph 中添加 Operator，这一步会生成对应的 StreamNode streamGraph.addOperator(transform.getId(), slotSharingGroup, transform.getCoLocationGroupKey(), transform.getOperatorFactory(), transform.getInputType(), transform.getOutputType(), transform.getName()); // 设置 stateKey if (transform.getStateKeySelector() != null) &#123; TypeSerializer&lt;?&gt; keySerializer = transform.getStateKeyType().createSerializer(executionConfig); streamGraph.setOneInputStateKey(transform.getId(), transform.getStateKeySelector(), keySerializer); &#125; // 设置 parallelism int parallelism = transform.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT ? transform.getParallelism() : executionConfig.getParallelism(); streamGraph.setParallelism(transform.getId(), parallelism); streamGraph.setMaxParallelism(transform.getId(), transform.getMaxParallelism()); // 在每一个物理节点的转换上 // 依次连接到上游 input 节点，创建 StreamEdge，在输入节点和当前节点之间建立边的连接 for (Integer inputId: inputIds) &#123; streamGraph.addEdge(inputId, transform.getId(), 0); &#125; return Collections.singleton(transform.getId());&#125; 接着看下 StreamGraph 中对应的添加节点的方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public &lt;IN, OUT&gt; void addOperator( Integer vertexID, @Nullable String slotSharingGroup, @Nullable String coLocationGroup, StreamOperatorFactory&lt;OUT&gt; operatorFactory, TypeInformation&lt;IN&gt; inTypeInfo, TypeInformation&lt;OUT&gt; outTypeInfo, String operatorName) &#123; if (operatorFactory.isStreamSource()) &#123; // 从传入的 StreamOperatorFactory 得知当前 operator 代表的是 source 流。SourceStreamTask addNode(vertexID, slotSharingGroup, coLocationGroup, SourceStreamTask.class, operatorFactory, operatorName); &#125; else &#123; // 上游节点输入流，OneInputStreamTask addNode(vertexID, slotSharingGroup, coLocationGroup, OneInputStreamTask.class, operatorFactory, operatorName); &#125;&#125;protected StreamNode addNode(Integer vertexID, @Nullable String slotSharingGroup, @Nullable String coLocationGroup, // 表示该节点在 TM 中运行时的实际任务类型 Class&lt;? extends AbstractInvokable&gt; vertexClass, StreamOperatorFactory&lt;?&gt; operatorFactory, String operatorName) &#123; if (streamNodes.containsKey(vertexID)) &#123; throw new RuntimeException("Duplicate vertexID " + vertexID); &#125; // 构造 StreamNode StreamNode vertex = new StreamNode( vertexID, slotSharingGroup, coLocationGroup, operatorFactory, operatorName, new ArrayList&lt;OutputSelector&lt;?&gt;&gt;(), vertexClass); // 保存在 streamNodes 这个 map 中 streamNodes.put(vertexID, vertex); return vertex;&#125; 下面我们再看下 transformPartition() 非物理节点的转换方法： 123456789101112131415161718192021222324252627282930private &lt;T&gt; Collection&lt;Integer&gt; transformPartition(PartitionTransformation&lt;T&gt; partition) &#123; Transformation&lt;T&gt; input = partition.getInput(); List&lt;Integer&gt; resultIds = new ArrayList&lt;&gt;(); // 递归遍历转换上游节点 Collection&lt;Integer&gt; transformedIds = transform(input); for (Integer transformedId: transformedIds) &#123; int virtualId = Transformation.getNewNodeId(); // 添加虚拟的 Partition 节点 streamGraph.addVirtualPartitionNode( transformedId, virtualId, partition.getPartitioner(), partition.getShuffleMode()); resultIds.add(virtualId); &#125; return resultIds;&#125;public void addVirtualPartitionNode( Integer originalId, Integer virtualId, StreamPartitioner&lt;?&gt; partitioner, ShuffleMode shuffleMode) &#123; if (virtualPartitionNodes.containsKey(virtualId)) &#123; throw new IllegalStateException("Already has virtual partition node with id " + virtualId); &#125; // 添加一个虚拟节点到 virtualPartitionNodes 中，后续添加边的时候会连接到实际的物理节点 virtualPartitionNodes.put(virtualId, new Tuple3&lt;&gt;(originalId, partitioner, shuffleMode));&#125; 在实际的物理节点执行添加边的操作时，会判断上游是不是虚拟节点，如果是则会一直递归调用，将虚拟节点的信息添加到边中，直到连接到一个物理转换节点为止： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758private void addEdgeInternal(Integer upStreamVertexID, Integer downStreamVertexID, int typeNumber, StreamPartitioner&lt;?&gt; partitioner, List&lt;String&gt; outputNames, OutputTag outputTag, ShuffleMode shuffleMode) &#123; // 先判断是不是虚拟节点上的边，如果是，则找到虚拟节点上游对应的物理节点 // 在两个物理节点之间添加边，并把对应的 outputTag 或 StreamPartitioner 添加到 StreamEdge 中 if (virtualSideOutputNodes.containsKey(upStreamVertexID)) &#123; &#125; else if (virtualSelectNodes.containsKey(upStreamVertexID)) &#123; &#125; else if (virtualPartitionNodes.containsKey(upStreamVertexID)) &#123; int virtualId = upStreamVertexID; upStreamVertexID = virtualPartitionNodes.get(virtualId).f0; if (partitioner == null) &#123; partitioner = virtualPartitionNodes.get(virtualId).f1; &#125; shuffleMode = virtualPartitionNodes.get(virtualId).f2; addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames, outputTag, shuffleMode); &#125; else &#123; // 两个物理节点 StreamNode upstreamNode = getStreamNode(upStreamVertexID); StreamNode downstreamNode = getStreamNode(downStreamVertexID); // If no partitioner was specified and the parallelism of upstream and downstream // operator matches use forward partitioning, use rebalance otherwise. if (partitioner == null &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123; partitioner = new ForwardPartitioner&lt;Object&gt;(); &#125; else if (partitioner == null) &#123; partitioner = new RebalancePartitioner&lt;Object&gt;(); &#125; if (partitioner instanceof ForwardPartitioner) &#123; if (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123; throw new UnsupportedOperationException("Forward partitioning does not allow " + "change of parallelism. Upstream operation: " + upstreamNode + " parallelism: " + upstreamNode.getParallelism() + ", downstream operation: " + downstreamNode + " parallelism: " + downstreamNode.getParallelism() + " You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global."); &#125; &#125; if (shuffleMode == null) &#123; shuffleMode = ShuffleMode.UNDEFINED; &#125; // 创建 StreamEdge，带着 outputTag 、StreamPartitioner 等属性 StreamEdge edge = new StreamEdge(upstreamNode, downstreamNode, typeNumber, outputNames, partitioner, outputTag, shuffleMode); // 分别将 StreamEdge 添加到上游节点和下游节点 // 获取上游节点，添加 OutEdge getStreamNode(edge.getSourceId()).addOutEdge(edge); getStreamNode(edge.getTargetId()).addInEdge(edge); &#125;&#125; StreamGraph 是 Flink 任务最接近用户逻辑的 DAG 表示，后面到具体执行时还会进行一系列转换。 类之间的层级关系 map 转换将用户自定义函数 MapFunction 包装到 StreamMap 这个 StreamOperator 中，再将 StreamMap 包装到 OneInputTransformation，最后该 transformation 会存到StreamExecutionEnvironment 中。当调用 env.execute() 时，会遍历其中的 transformations 集合构造出 StreamGraph 。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[UML]]></title>
    <url>%2F2020%2F04%2F14%2FUML%2F</url>
    <content type="text"><![CDATA[本文介绍Java开发中的软技能之一，UML图。UML 即统一建模语言，它是一种开放的方法，用于说明、可视化、构建和编写一个正在开发的、面向对象的、软件密集系统的制品的开放方法。UML 展现了一系列最佳工程实践，这些最佳实践在对大规模，复杂系统进行建模方面，特别是在软件架构层次已经被验证有效。我们知道开发一个软件系统，不光只有程序员参与，另外还有分析师、设计师、测试人员等等，为了让不同人能够理解交流这个软件系统，就诞生了这么一套语言。这套语言是由图表组成的，最常用的有：用例图、类图、时序图、状态图、活动图、组件图和部署图等。大致可以将这些图归类为结构图和行为图： 结构图是静态图，如类图、对象图等 行为图是动态图，像序列图、协作图等 类图类图主要是用来显示系统中的类、接口以及它们之间的静态结构和关系的一种静态模型。许多项目立项文档、需求分析文档中，都会有关 UML 类图的涉及。类图基本上是一个系统的静态视图的图形表示，代表应用的不同方面，集合类图就表示整个系统。画类图需要关注以下几点： 类图中的名称应该是有意义的描述，并且是面向系统的 画类图前应该先确定每个元素之间的关系 类图中每个类职责（属性和方法）应该清晰标明 对于每个类的属性应改精简，不必要的属性将使图表变得复杂 可见性符号+ public# protected- private 类之间的关系 关系 表示 图示 解释 结构和语义 泛化（Generalization） A继承B，B为非抽象类 继承结构 实现（Realization） A实现B，B为抽象类或接口 继承结构 聚合（Aggregation） A聚合到B上，B由A组成 表示整体由部分构成的语义。 (不是强依赖：整体不存在了，部分仍然存在) 组合（Composition） A组成B，B由A组成 表示整体由部分构成的语义。 (强依赖：整体不存在了，部分也不存在了) 关联（Association） A知道B，但是B不知道A 不同类的对象之间的结构关系。 不强调方向，表示对象间相互知道。 依赖（Dependence） A依赖于B 描述一个对象在运行期间会用到另一个对象的关系。 应该保持单向依赖，杜绝双向依赖。 首先，我们给出一张具有整体关系的 UML 类图，后面再逐步分解说明。 泛化（Generalization）泛化即 Java 中的继承关系，是类与类或者接口与接口之间最常见的关系。 两个子类 Fish 和 Cat 分别继承自 Animal。 1234567891011121314public class Animal &#123; public String name; protected boolean isPet; private String state; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 1234567public class Fish extends Animal &#123; public String fishType; public void swim() &#123; &#125;&#125; 1234567public class Cat extends Animal&#123; protected boolean hasFeet; public void playToy(Doll doll)&#123; &#125;&#125; 实现（Realization）实现即 Java 中类对抽象类或接口的实现关系，是类与接口之间最常见的关系。 123public interface ToyAction &#123; void toyMoved();&#125; 123456789public class Doll implements ToyAction&#123; public Body body; public Cloth cloth; @Override public void toyMoved() &#123; &#125;&#125; 关联（Association）关联关系是一种比较强的关系，他们的关系是比较持久的、稳定的，而且双方的关系一般是平等的，分单向关联、双向关联等。表现在代码层面，就是类 B 作为类 A 的属性，也可能是类 A 引用了一个类型为 B 的全局变量。如 Person 类，他拥有一个宠物猫，他们之间是关联关系。 1234public class Person &#123; public Cat pet; public Head head;&#125; (1) 单向关联用带箭头的实线表示，箭头指向被引用或被包含的类。上面演示的就是一个单向关联关系。 (2) 双向关联用不带箭头的实线来连接两个类，所谓的双向关联就是双方各自持有对方类型的成员变量。例如 Customer 类中维护一个 Product[] 数组，表示一个顾客购买了哪些商品；在 Product 类中维护一个 Customer 类型的成员变量表示这个产品被哪个顾客所购买。 (3) 自关联系统中可能会存在一些类的属性对象类型为该类本身，例如二叉树中的 TreeNode 定义。 依赖（Dependence）就是一个类 A 使用到了另一个类 B ，这种使用关系是具有偶然性的、临时性的、非常弱的，但是类 B 的变化会影响到 A。表现在代码层面，就是类 B 作为参数被类 A 在某个 method 方法中使用。如 Cat 类的 playToy 方法的参数就引用了 Doll 类，因此他们是依赖关系。 聚合（Aggregation）聚合关系强调的整体和部分、拥有的关系，即 has-a 的关系，其中部分可以脱离整体而存在，他们可以具有各自的生命周期。如 Doll 类由 Body 和 Cloth 组成，即使失去了 Cloth，Doll 也可以正常存在。表现在代码层面，和关联关系是一致的，只能从语义级别来区分。 12public class Body &#123;&#125; 12public class Cloth &#123;&#125; 组合（Composition）组合关系也是强调整体和部分的关系，不同的是部分不能脱离整体而存在，它体现的是一种 contains-a 的关系，这种关系比聚合更强，也成为强聚合。整体的生命周期结束也就意味着部分的生命周期结束，如人和大脑。表现在代码层面，和关联关系是一致的，只能从语义级别来区分。 12public class Head &#123;&#125; 类关系强度：组合 &gt; 聚合 &gt; 关联 &gt; 依赖 下面我们在 idea 中构建这几个类，将这几个放到同一 package 下，看下 idea 自动生成的UML类图： 和我们给出整体关系 UML 类图基本一致，区别是聚合关系和组合关系都是用实心菱形表示的。 对象图是类图的一个具体实例。 用例图从用户的角度出发描述系统的功能、需求，展示系统外部的各类角色与系统内部的各种用例之间的关系。 顺序图表示对象之间动态合作的关系。 协作图描述对象之间的协作关系。 活动图描述系统中各种活动的执行顺序。 状态图描述一类对象的所有可能的状态以及事件发生时状态的转移条件。 部署图定义系统中软硬件的物理体系结构。 UML组件图描述代码部件的物理结构以及各部件之间的依赖关系。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink源码剖析-flink-annotations]]></title>
    <url>%2F2020%2F04%2F13%2FFlink%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-flink-annotations%2F</url>
    <content type="text"><![CDATA[本文将先介绍下java注解的实现，再说明下Flink自定义的几个注解及其使用。 java注解注解在一定程度上是在把元数据与源代码文件结合在一起，而不是保存在外部文档中这一大的趋势下所催生的。注解可以提供用来完整的描述程序所需的信息，而这些信息是无法用Java来表达的。因此，注解存储有关程序的额外信息，是可以由编译器来测试和验证的。注解还可以用来生成描述符文件，甚至是新的类定义，并且有助于减轻编写“样板”代码的负担。通过使用注解，我们可以将这些元数据保存在Java源代码中，并利用 annotation API 为自己的注解构造处理工具，同时注解的优点还包括：更加干净易读的代码以及编译器类型检查等。 注解的使用场景： 提供信息给编译器：编译器可以利用注解来探测错误和警告信息 编译阶段时的处理：软件工具可以利用注解信息来生成代码，HTML文档或其他相应处理 运行时的处理：某些注解可以在程序运行时接受代码的提取 注解的分类 按运行机制划分源码注解：只在源码中存在，编译成 .class 文件就不存在了编译时注解：在源码和 .class 文件中都存在，像前面的 @Override、@Deprecated、@SuppressWarnings 都属于编译时注解运行时注解：在运行阶段还有作用，甚至会影响运行逻辑，像 @Autowired 就属于运行时注解，它会在程序运行时把你的成员变量自动的注入进来 按来源划分来自 JDK 的注解来自第三方的注解自定义注解 元注解 元注解负责注解的创建，是注解的注解。 元注解的类图关系如下： @Target 表示注解可以用在什么地方。ElementType可以是： TYPE：类，接口，枚举类上 FIELD：字段上，包括枚举实例 METHOD：方法上 PARAMETER：参数前 CONSTRUCTOR：构造函数上 LOCAL_VARIABLE：局部变量上 ANNOTATION_TYPE：注解类上 PACKAGE：包上 TYPE_PARAMETER： TYPE_USE：可以是某一个值或者以逗号分隔的形式指定多个值，如果想要将注解应用于所有的ElementType，也可以省去 @Target 元注解 123456789101112@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target &#123; /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to */ ElementType[] value();&#125; @Retention 表示需要在什么级别上保留该注解信息。RetentionPolicy可以是： SOURCE：注解将被编译器丢弃 CLASS：注解在class中可用，但会被VM丢弃 RUNTIME：VM在运行期也将保留注解，因此可以通过反射机制读取注解信息 12345678910@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention &#123; /** * Returns the retention policy. * @return the retention policy */ RetentionPolicy value();&#125; @Documented 将此注解中的元素包含到javadoc中。 12345@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Documented &#123;&#125; @Inherited 允许子类继承父类的注解。 12345@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Inherited &#123;&#125; @Repeatable 注解的值可以是多个，元素是一个容器注解。 1234567891011@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Repeatable &#123; /** * Indicates the &lt;em&gt;containing annotation type&lt;/em&gt; for the * repeatable annotation type. * @return the containing annotation type */ Class&lt;? extends Annotation&gt; value();&#125; 注解元素 基本语法 使用 @interface 关键字定义注解，在注解上添加元注解。一般还要为注解添加元素，没有元素的注解称为标识注解。注解只有成员变量，没有方法。注解的成员变量在注解的定义中以”无形参的方法”形式来声明，其方法名定义了该成员变量的名字，返回值定义了该成员变量的类型。 12345678910@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface TestAnnotation &#123; int id() default -1; String msg() default "Hello"; String value() default "";&#125; 注解元素可用的类型 所有基本类型（int,float,boolean等） String Class enum Annotation 以上类型的数组 如果使用了其他类型，那编译器就会报错。也不允许使用任何包装类型。注解也可以作为元素的类型，也就是说注解可以嵌套。 注解元素的默认值限制 编译器对注解元素的默认值有些过分挑剔。首先，注解元素不能有不确定的值。也就是说，注解元素要么具有默认值，要么在使用注解时设置元素值。 内置注解所有的注解都继承自 java.lang.annotation.Annotation 接口。 12345678910public interface Annotation &#123; boolean equals(Object obj); int hashCode(); String toString(); Class&lt;? extends Annotation&gt; annotationType();&#125; JDK 中有几种内置的注解： @Override 表示当前的方法定义将覆盖超类中的方法。如果不小心拼写错误或者方法签名对不上被覆盖的方法，编译器就会发出错误提示。 123456package java.lang;import java.lang.annotation.*;@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override &#123;&#125; 执行如下命令： 12$ javac Override.java$ javap -c Override.class 得到如下内容： 123Compiled from &quot;Override.java&quot;public interface java.lang.Override extends java.lang.annotation.Annotation &#123;&#125; 由此可以看出，注解的本质就是一个继承了 Annotation 接口的接口，是一种典型的标记式注解。一旦编译器检测到某个方法被修饰了 @Override 注解，编译器就会检查当前方法的方法签名是否真正重写了父类的某个方法，也就是比较父类中是否具有一个同样的方法签名，如果没有，自然不能编译通过。编译器只能识别已经熟知的注解类，比如 JDK 内置的几个注解，而我们自定义的注解，编译器是不会知道这个注解的作用的，当然也不知道应该如何处理。 @Deprecated 依然是一种标记式注解，永久存在，可以修饰所有类型，被标记的类、方法、字段等已经不再被推荐使用了，可能下一个版本就会删除。当然，编译器并不会强制要求你做什么，只是会在对象上画出一道线，建议你使用某个替代者。 12345@Documented@Retention(RetentionPolicy.RUNTIME)@Target(value=&#123;CONSTRUCTOR, FIELD, LOCAL_VARIABLE, METHOD, PACKAGE, PARAMETER, TYPE&#125;)public @interface Deprecated &#123;&#125; @SuppressWarnings 抑制告警。它有一个 value 属性需要主动传值，传入需要被抑制的警告类型。 12345@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.SOURCE)public @interface SuppressWarnings &#123; String[] value();&#125; 如下 Date 的构造函数是过时的，在 main() 方法上加上 @SuppressWarning(value = “deprecated”) 注解后，编译器就不会再对这种告警进行检查了。 1234@SuppressWarning(value = "deprecated")public static void main(String[] args) &#123; Date date = new Date(2018, 7, 11);&#125; 注解的提取解析一个类或方法的注解往往有两种形式： 一种是编译期直接扫描：编译器在对java代码编译字节码的过程中会检测到某个类或方法被一些注解修饰，它就会对这些注解进行某些处理。 一种是运行期反射。 上文中有创建注解 TestAnnotation ，下面我们来写一个注解的提取类 Test： 123456789101112131415@TestAnnotation("defaultValue")public class Test &#123; public static void main(String[] args) &#123; // 注解通过反射获取，通过 Class 对象的 isAnnotationPresent() 方法判断它是否应用了某个注解 boolean hasAnnotation = Test.class.isAnnotationPresent(TestAnnotation.class); if (hasAnnotation) &#123; // 通过 getAnnotation() 方法来获取 Annotation 对象实例 TestAnnotation testAnnotation = Test.class.getAnnotation(TestAnnotation.class); System.out.println("id:" + testAnnotation.id()); System.out.println("msg:" + testAnnotation.msg()); &#125; &#125;&#125; 我们前面说过，注解本质上是继承了 Annotation 接口的接口，而当你通过反射，也就是 getAnnotation 方法去获取一个注解类实例的时候，其实 JDK 是通过动态代理生成了一个实现自定义注解（接口）的代理类。 运行 Test 类之前，先设置如下 VM 参数，让其生成代理类 class 文件： 1234/* jdk动态代理 设置此系统属性,让JVM生成的Proxy类写入文件.保存路径为：com/sun/proxy(如果不存在请手工创建) */-Dsun.misc.ProxyGenerator.saveGeneratedFiles=true/* cglib动态代理 设置此系统属性,让JVM生成的Proxy类写入文件.保存路径为：com/sun/proxy(如果不存在请手工创建) */-Dcglib.debugLocation=com/sun/proxy 将生成的代理类 class 文件反编译成可视化文件： 12$ cd ./com/sun/proxy$ javap -c \$Proxy1.class &gt; Proxy1 查看代理类内容，代理类实现接口 TestAnnotation 并重写其所有方法，包括id()、msg()、value()以及接口 TestAnnotation 从 Annotation 接口继承而来的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public final class com.sun.proxy.$Proxy1 extends java.lang.reflect.Proxy implements org.apache.flink.annotation.TestAnnotation &#123; public com.sun.proxy.$Proxy1(java.lang.reflect.InvocationHandler) throws ; Code: 0: aload_0 1: aload_1 2: invokespecial #8 // Method java/lang/reflect/Proxy.&quot;&lt;init&gt;&quot;:(Ljava/lang/reflect/InvocationHandler;)V 5: return public final boolean equals(java.lang.Object) throws ; ...... public final java.lang.String toString() throws ; ...... public final java.lang.String msg() throws ; ...... public final java.lang.Class annotationType() throws ; ...... public final int id() throws ; ...... public final int hashCode() throws ; ...... public final java.lang.String value() throws ; Code: 0: aload_0 1: getfield #16 // Field java/lang/reflect/Proxy.h:Ljava/lang/reflect/InvocationHandler; 4: aload_0 5: getstatic #81 // Field m3:Ljava/lang/reflect/Method; 8: aconst_null 9: invokeinterface #28, 4 // InterfaceMethod java/lang/reflect/InvocationHandler.invoke:(Ljava/lang/Object;Ljava/lang/reflect/Method;[Ljava/lang/Object;)Ljava/lang/Object; 14: checkcast #52 // class java/lang/String 17: areturn 18: athrow 19: astore_1 20: new #42 // class java/lang/reflect/UndeclaredThrowableException 23: dup 24: aload_1 25: invokespecial #45 // Method java/lang/reflect/UndeclaredThrowableException.&quot;&lt;init&gt;&quot;:(Ljava/lang/Throwable;)V 28: athrow Exception table: from to target type 0 18 18 Class java/lang/Error 0 18 18 Class java/lang/RuntimeException 0 18 19 Class java/lang/Throwable static &#123;&#125; throws ; ......&#125; 这里的 InvocationHandler 指的就是 AnnotationInvocationHandler，它是 Java 中专门用于处理注解的 handler，下面就来让我们看看这个类的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class AnnotationInvocationHandler implements InvocationHandler, Serializable &#123; private static final long serialVersionUID = 6182022883658399397L; private final Class&lt;? extends Annotation&gt; type; /** * 注解元素属性的键值对 */ private final Map&lt;String, Object&gt; memberValues; AnnotationInvocationHandler(Class&lt;? extends Annotation&gt; type, Map&lt;String, Object&gt; memberValues) &#123; this.type = type; this.memberValues = memberValues; &#125; /** * 代理类代理了 TestAnnotation 接口中的所有方法 */ public Object invoke(Object proxy, Method method, Object[] args) &#123; String member = method.getName(); Class&lt;?&gt;[] paramTypes = method.getParameterTypes(); // Handle Object and Annotation methods // 如果当前调用的是 toString、equals、hashCode、annotationType。AnnotationInvocationHandler 实例中已经预定义好了这些方法的实现，直接调用即可。 if (member.equals("equals") &amp;&amp; paramTypes.length == 1 &amp;&amp; paramTypes[0] == Object.class) return equalsImpl(args[0]); assert paramTypes.length == 0; if (member.equals("toString")) return toStringImpl(); if (member.equals("hashCode")) return hashCodeImpl(); if (member.equals("annotationType")) return type; // Handle annotation member accessors // 从我们注解的 map 中获取这个注解属性对应的值，即通过方法名返回注解属性值。 Object result = memberValues.get(member); if (result == null) throw new IncompleteAnnotationException(type, member); if (result instanceof ExceptionProxy) throw ((ExceptionProxy) result).generateException(); if (result.getClass().isArray() &amp;&amp; Array.getLength(result) != 0) result = cloneArray(result); return result; &#125; ......&#125; 自定义注解 可重复注解 创建容器注解 Persons，容器注解本身也是一个注解，是用来存放其他注解的地方。它必须要有一个 value 属性，属性类型是一个被 @Repeatable 注解过的注解数组。 123public @interface Persons &#123; Person[] value();&#125; 使用 @Repeatable 注解了 Person，而 @Repeatable 后面括号中的类是一个容器注解。 1234@Repeatable(Persons.class)public @interface Person &#123; String role();&#125; 给 Superman 这个类贴上多个角色标签。 12345@Person(role="Painter")@Person(role="Musician")@Person(role="Actor")public class Superman &#123;&#125; 测试用例注解实现一个注解，用来跟踪一个项目中的用例。如果一个方法实现了某个用例的需求，那么可以为此方法加上该注解。于是，项目经理通过计算已经实现的用例，就可以很好的掌控项目的进展。而且把实现方法和用例绑定，如果要更新或修改系统的业务逻辑，维护该项目的开发人员也可以很容易的在代码中找到对应的用例。 定义 UseCase 注解，id 表示用例编号，description 设置了默认值。 12345678910111213@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface UseCase &#123; /** * 用例id */ int id(); /** * 用例描述 */ String description() default "no description";&#125; 定义需求实现类 PasswordUtils，每一个方法都对应一个需求用例。 12345678910111213141516171819public class PasswordUtils &#123; @UseCase(id = 47 , description = "Passwords must contain at least one numeric") public boolean validatePassword(String password)&#123; return password.matches("\\w*\\d\\w*"); &#125; @UseCase(id = 48) public String encryptPassword(String password)&#123; return new StringBuilder(password).reverse().toString(); &#125; @UseCase(id = 49,description = "New passwords can't equal previously used ones") public boolean checkForNewPassword(List&lt;String&gt; prevPasswords, String password)&#123; return !prevPasswords.contains(password); &#125;&#125; 如果没有用来读取注解的工具，那注解也不会比注释更有用。使用注解的过程中，很重要的一部分就是创建与使用注解处理器。下面实现了一个非常简单的注解处理器 UseCaseTracker ，将用它来读取 PasswordUtils 类，并使用反射机制查找 @UseCase 注解。我们提供了一组 id 值，然后它会列出在 PasswordUtils 中找到的用例，以及缺失的用例。 1234567891011121314151617181920212223public class UseCaseTracker &#123; public static void trackUseCases(List&lt;Integer&gt; useCases, Class&lt;?&gt; cl) &#123; for (Method m : cl.getDeclaredMethods()) &#123; // 返回指定类型的注解对象 UseCase uc = m.getAnnotation(UseCase.class); if (uc != null) &#123; System.out.println("Found Use Case: " + uc.id() + " " + uc.description()); useCases.remove(new Integer(uc.id())); &#125; &#125; for (Integer i : useCases) &#123; System.out.println("Warning: Missing use case-" + i); &#125; &#125; public static void main(String[] args) &#123; List&lt;Integer&gt; useCases = new ArrayList&lt;&gt;(); Collections.addAll(useCases, 47, 48, 49, 50); trackUseCases(useCases, PasswordUtils.class); &#125;&#125; 运行结果： 1234Found Use Case: 47 Passwords must contain at least one numericFound Use Case: 48 no descriptionFound Use Case: 49 New passwords can&apos;t equal previously used onesWarning: Missing use case-50 利用注解生成SQL语句 定义表名注解，它告诉处理器，你需要把我这个类生成一个数据库 DDL 语句。 12345678@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)public @interface DBTable &#123; /** * 数据库表表名 */ String name() default "";&#125; 定义数据库表字段约束的注解：是否为主键，是否可以为空，唯一性约束。 1234567@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface Constraints &#123; boolean primaryKey() default false; boolean allowNull() default true; boolean unique() default false;&#125; 定义表字段类型为 String 的注解：字符串长度，字段名， 字段约束。这里的字段约束就用到了嵌套注解的语法。 12345678@Target(value = ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface SQLString &#123; int len() default 0; String name() default ""; Constraints constraints() default @Constraints;&#125; 定义表字段类型为 Integer 的注解：字段名，字段约束。 123456@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface SQLInteger &#123; String name() default ""; Constraints constraints() default @Constraints;&#125; 定义一个 Member 类，应用了以上定义的注解。类的注解 @DBTable 给定了值 MEMBER，它将会用来作为表的名字。字段属性 firstName 和 lastName 都被注解为 @SQLString 类型，并分别设置了长度为 30 和 50。 1234567891011121314151617181920212223242526272829303132333435363738@DBTable(name = "MEMBER")public class Member &#123; @SQLString(len = 30) String firstName; @SQLString(len = 50) String lastName; @SQLInteger Integer age; @SQLString(len = 30, constraints = @Constraints(primaryKey = true)) String handle; static int memberCount; public String getFirstName() &#123; return firstName; &#125; public String getLastName() &#123; return lastName; &#125; public Integer getAge() &#123; return age; &#125; public String getHandle() &#123; return handle; &#125; @Override public String toString() &#123; return handle; &#125;&#125; 实现处理器 TableCreator ，它将读取一个类文件，检查其上的数据库表注解，并生成用来创建数据库表的 SQL 语句。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class TableCreator &#123; public static void main(String[] args) throws Exception &#123; String className = Member.class.getName(); Class&lt;?&gt; cl = Class.forName(className); // 检查类上是否带有 @DBTable 注解 DBTable dbtable = cl.getAnnotation(DBTable.class); if (dbtable == null) &#123; System.out.println("No DbTable annotations in class " + className); &#125; // 提取 @DBTable 注解的 name String tableName = dbtable.name(); // If the name is empty , use the Class name: if (tableName.length() &lt; 1) &#123; tableName = cl.getName().toUpperCase(); &#125; List&lt;String&gt; columnDefs = new ArrayList&lt;&gt;(); // 遍历 Member 类的所有字段 for (Field field : cl.getDeclaredFields()) &#123; String columnName; // 获取字段属性上的所有注解 Annotation[] annotations = field.getDeclaredAnnotations(); if (annotations.length &lt; 1) &#123; continue; // Not a db table column &#125; if (annotations[0] instanceof SQLInteger) &#123; // 处理 @SQLInteger 注解的属性字段 SQLInteger sInt = (SQLInteger) annotations[0]; // Use field name if name not specified if (sInt.name().length() &lt; 1) &#123; columnName = field.getName().toUpperCase(); &#125; else &#123; columnName = sInt.name(); &#125; columnDefs.add(columnName + " INT" + getConstraints(sInt.constraints())); &#125; else if (annotations[0] instanceof SQLString) &#123; // 处理 @SQLString 注解的属性字段 SQLString sString = (SQLString) annotations[0]; // Use field name if name not specified. if (sString.name().length() &lt; 1) &#123; columnName = field.getName().toUpperCase(); &#125; else &#123; columnName = sString.name(); &#125; columnDefs.add(columnName + " VARCHAR(" + sString.len() + ")" + getConstraints(sString.constraints())); &#125; &#125; StringBuilder createCommand = new StringBuilder("CREATE TABLE " + tableName + "("); for (String columnDef : columnDefs) &#123; createCommand.append("\n " + columnDef + ","); &#125; // Remove trailing comma String tableCreate = createCommand.substring(0, (createCommand.length() - 1)) + ");"; System.out.println("Table.Creation SQL for " + className + " is :\n " + tableCreate); &#125; private static String getConstraints(Constraints con) &#123; String constraints = ""; if (!con.allowNull()) &#123; constraints += " NOT NULL"; &#125; if (con.primaryKey()) &#123; constraints += " PRIMARY KEY"; &#125; if (con.unique()) &#123; constraints += " UNIQUE"; &#125; return constraints; &#125;&#125; 运行结果： 123456Table.Creation SQL for org.apache.flink.annotation.dbtable.Member is : CREATE TABLE MEMBER( FIRSTNAME VARCHAR(30), LASTNAME VARCHAR(50), AGE INT, HANDLE VARCHAR(30) PRIMARY KEY); flink中的注解 docs相关注解 @ConfigGroup 指定一组配置选项，组的名称将用作生成 HTML 文件名，keyPrefix 用于匹配配置项名称前缀。如 @ConfigGroup(name = “firstGroup”, keyPrefix = “first”)，生成的 HTML 文件名为 firstGroup ，其中的配置项名称都是以 first 开头的。 123456@Target(&#123;&#125;)@Internalpublic @interface ConfigGroup &#123; String name(); String keyPrefix();&#125; @ConfigGroups 允许一个配置类中的配置项可以按照配置项名称前缀分成不同的组，生成多个 HTML 文件。如：@ConfigGroups(groups = { @ConfigGroup(name = “firstGroup”, keyPrefix = “first”), @ConfigGroup(name = “secondGroup”, keyPrefix = “second”)})可以从配置类生成 3 个 HTML 文件，分别为 firstGroup、secondGroup、default，具体可以接着往下看，下面会有示例说明。 123456@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Internalpublic @interface ConfigGroups &#123; ConfigGroup[] groups() default &#123;&#125;;&#125; 下面通过一个示例来说明这两个注解的用途。查看测试类 ConfigOptionsDocGeneratorTest 中应用到 @ConfigGroups 和 @ConfigGroup 的单测 testCreatingMultipleGroups： 1234567891011@Testpublic void testCreatingMultipleGroups() &#123; final List&lt;Tuple2&lt;ConfigGroup, String&gt;&gt; tables = ConfigOptionsDocGenerator.generateTablesForClass( TestConfigMultipleSubGroup.class); assertEquals(tables.size(), 3); final HashMap&lt;String, String&gt; tablesConverted = new HashMap&lt;&gt;(); for (Tuple2&lt;ConfigGroup, String&gt; table : tables) &#123; tablesConverted.put(table.f0 != null ? table.f0.name() : "default", table.f1); &#125;&#125; TestConfigMultipleSubGroup 类 mock 了一个配置项类：@ConfigGroup(name = “firstGroup”, keyPrefix = “first”) 将 key 以 first 开头的 ConfigOption 归为 firstGroup，@ConfigGroup(name = “secondGroup”, keyPrefix = “second”) 将 key 以 second 开头的 ConfigOption 归为 secondGroup。 123456789101112131415161718192021222324@ConfigGroups(groups = &#123; @ConfigGroup(name = "firstGroup", keyPrefix = "first"), @ConfigGroup(name = "secondGroup", keyPrefix = "second")&#125;)static class TestConfigMultipleSubGroup &#123; public static ConfigOption&lt;Integer&gt; firstOption = ConfigOptions .key("first.option.a") .defaultValue(2) .withDescription("This is example description for the first option."); public static ConfigOption&lt;String&gt; secondOption = ConfigOptions .key("second.option.a") .noDefaultValue() .withDescription("This is long example description for the second option."); public static ConfigOption&lt;Integer&gt; thirdOption = ConfigOptions .key("third.option.a") .defaultValue(2) .withDescription("This is example description for the third option."); public static ConfigOption&lt;String&gt; fourthOption = ConfigOptions .key("fourth.option.a") .noDefaultValue() .withDescription("This is long example description for the fourth option.");&#125; 我们再看下 ConfigOptionsDocGenerator.generateTablesForClass(Class&lt;?&gt; optionsClass) 12345678910111213141516171819202122232425262728293031@VisibleForTestingstatic List&lt;Tuple2&lt;ConfigGroup, String&gt;&gt; generateTablesForClass(Class&lt;?&gt; optionsClass) &#123; // 获取 optionsClass 类上定义的 @ConfigGroups ConfigGroups configGroups = optionsClass.getAnnotation(ConfigGroups.class); // 抽取 optionsClass 中的所有 ConfigOption 配置项 List&lt;OptionWithMetaInfo&gt; allOptions = extractConfigOptions(optionsClass); // 遍历 @ConfigGroups 注解中的 ConfigGroup[] groups() List&lt;Tuple2&lt;ConfigGroup, String&gt;&gt; tables; if (configGroups != null) &#123; // 解析 optionsClass 上的 ConfigGroup 注解，即是有分组的。另外一个是默认的 ConfigGroup tables = new ArrayList&lt;&gt;(configGroups.groups().length + 1); Tree tree = new Tree(configGroups.groups(), allOptions); for (ConfigGroup group : configGroups.groups()) &#123; List&lt;OptionWithMetaInfo&gt; configOptions = tree.findConfigOptions(group); // 按照 ConfigOption 的 key 进行排序 sortOptions(configOptions); tables.add(Tuple2.of(group, toHtmlTable(configOptions))); &#125; // 所有 @ConfigGroup 前缀都匹配不上的其他 ConfigOption 归为 default 组 List&lt;OptionWithMetaInfo&gt; configOptions = tree.getDefaultOptions(); sortOptions(configOptions); tables.add(Tuple2.of(null, toHtmlTable(configOptions))); &#125; else &#123; sortOptions(allOptions); tables = Collections.singletonList(Tuple2.of(null, toHtmlTable(allOptions))); &#125; return tables;&#125; 运行单测 testCreatingMultipleGroups 的输出结果如下：firstGroup 配置项组里的配置项名称都是以 first 为前缀的。 123456789101112131415161718&lt;table class="table table-bordered"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class="text-left" style="width: 20%"&gt;Key&lt;/th&gt; &lt;th class="text-left" style="width: 15%"&gt;Default&lt;/th&gt; &lt;th class="text-left" style="width: 10%"&gt;Type&lt;/th&gt; &lt;th class="text-left" style="width: 55%"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;first.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;2&lt;/td&gt; &lt;td&gt;Integer&lt;/td&gt; &lt;td&gt;This is example description for the first option.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; secondGroup 配置项组里的配置项名称都是以 second 为前缀的。 123456789101112131415161718&lt;table class="table table-bordered"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class="text-left" style="width: 20%"&gt;Key&lt;/th&gt; &lt;th class="text-left" style="width: 15%"&gt;Default&lt;/th&gt; &lt;th class="text-left" style="width: 10%"&gt;Type&lt;/th&gt; &lt;th class="text-left" style="width: 55%"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;second.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;(none)&lt;/td&gt; &lt;td&gt;String&lt;/td&gt; &lt;td&gt;This is long example description for the second option.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; TestConfigMultipleSubGroup 中的其他配置项都是没有分组的，默认都放到 default 组中。 123456789101112131415161718192021222324&lt;table class="table table-bordered"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class="text-left" style="width: 20%"&gt;Key&lt;/th&gt; &lt;th class="text-left" style="width: 15%"&gt;Default&lt;/th&gt; &lt;th class="text-left" style="width: 10%"&gt;Type&lt;/th&gt; &lt;th class="text-left" style="width: 55%"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;fourth.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;(none)&lt;/td&gt; &lt;td&gt;String&lt;/td&gt; &lt;td&gt;This is long example description for the fourth option.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;third.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;2&lt;/td&gt; &lt;td&gt;Integer&lt;/td&gt; &lt;td&gt;This is example description for the third option.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; Documentation 类中定义了修改文档生成器行为的注解结合，包括 @OverrideDefault、@CommonOption、@TableOption、@ExcludeFromDocumentation。下面依次介绍。 @Documentation.OverrideDefault 作用在 ConfigOption 上的注解，覆盖其默认值。 123456@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Internalpublic @interface OverrideDefault &#123; String value();&#125; 下面通过一个示例来说明这个注解的用途。查看测试类 ConfigOptionsDocGeneratorTest 中应用到 @Documentation.OverrideDefault 的单测 testOverrideDefault： 1234@Testpublic void testOverrideDefault() &#123; String htmlTable = ConfigOptionsDocGenerator.generateTablesForClass(TestConfigGroupWithOverriddenDefault.class).get(0).f1;&#125; TestConfigGroupWithOverriddenDefault 类 mock 了一个配置项类，每个配置项都使用了 @Documentation.OverrideDefault 注解覆盖配置项的默认值。 12345678910111213static class TestConfigGroupWithOverriddenDefault &#123; @Documentation.OverrideDefault("default_1") public static ConfigOption&lt;Integer&gt; firstOption = ConfigOptions .key("first.option.a") .defaultValue(2) .withDescription("This is example description for the first option."); @Documentation.OverrideDefault("default_2") public static ConfigOption&lt;String&gt; secondOption = ConfigOptions .key("second.option.a") .noDefaultValue() .withDescription("This is long example description for the second option.");&#125; 运行单测 testOverrideDefault 的输出结果如下：将 firstOption 的默认值覆盖成了 default_1，secondOption 原先没有默认值，被设置成了 default_2。 123456789101112131415161718192021222324&lt;table class="table table-bordered"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class="text-left" style="width: 20%"&gt;Key&lt;/th&gt; &lt;th class="text-left" style="width: 15%"&gt;Default&lt;/th&gt; &lt;th class="text-left" style="width: 10%"&gt;Type&lt;/th&gt; &lt;th class="text-left" style="width: 55%"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;first.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;default_1&lt;/td&gt; &lt;td&gt;Integer&lt;/td&gt; &lt;td&gt;This is example description for the first option.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;second.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;default_2&lt;/td&gt; &lt;td&gt;String&lt;/td&gt; &lt;td&gt;This is long example description for the second option.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; @Documentation.CommonOption 作用在 ConfigOption 上的注解，使其包含在 “Common Options” 片段中，按 position 值排序，position 值小的配置项排在前面。 123456789101112@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Internalpublic @interface CommonOption &#123; int POSITION_MEMORY = 10; int POSITION_PARALLELISM_SLOTS = 20; int POSITION_FAULT_TOLERANCE = 30; int POSITION_HIGH_AVAILABILITY = 40; int POSITION_SECURITY = 50; int position() default Integer.MAX_VALUE;&#125; 下面通过一个示例来说明这个注解的用途。查看测试类 ConfigOptionsDocGeneratorTest 中应用到 @Documentation.CommonOption 的单测 testCommonOptions： 1234567891011121314@Testpublic void testCommonOptions() throws IOException, ClassNotFoundException &#123; final String projectRootDir = System.getProperty("rootDir"); final String outputDirectory = TMP.newFolder().getAbsolutePath(); final OptionsClassLocation[] locations = new OptionsClassLocation[] &#123; new OptionsClassLocation("flink-docs", TestCommonOptions.class.getPackage().getName()) &#125;; ConfigOptionsDocGenerator.generateCommonSection(projectRootDir, outputDirectory, locations, "src/test/java"); Formatter formatter = new HtmlFormatter(); String output = FileUtils.readFile(Paths.get(outputDirectory, ConfigOptionsDocGenerator.COMMON_SECTION_FILE_NAME).toFile(), StandardCharsets.UTF_8.name());&#125; TestCommonOptions 类 mock 了一个配置项类：COMMON_OPTION 使用了 @Documentation.CommonOption 注解，position 使用默认值为 Integer.MAX_VALUE，COMMON_POSITIONED_OPTION 也是用了 @Documentation.CommonOption 注解，position 值指定为2，这个配置项肯定排在 COMMON_OPTION 前面。 12345678910111213141516171819public class TestCommonOptions &#123; @Documentation.CommonOption public static final ConfigOption&lt;Integer&gt; COMMON_OPTION = ConfigOptions .key("first.option.a") .defaultValue(2) .withDescription("This is the description for the common option."); public static final ConfigOption&lt;String&gt; GENERIC_OPTION = ConfigOptions .key("second.option.a") .noDefaultValue() .withDescription("This is the description for the generic option."); @Documentation.CommonOption(position = 2) public static final ConfigOption&lt;Integer&gt; COMMON_POSITIONED_OPTION = ConfigOptions .key("third.option.a") .defaultValue(3) .withDescription("This is the description for the positioned common option.");&#125; 运行单测 testCommonOptions 的输出结果如下： 123456789101112131415161718192021222324&lt;table class="table table-bordered"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class="text-left" style="width: 20%"&gt;Key&lt;/th&gt; &lt;th class="text-left" style="width: 15%"&gt;Default&lt;/th&gt; &lt;th class="text-left" style="width: 10%"&gt;Type&lt;/th&gt; &lt;th class="text-left" style="width: 55%"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;third.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;3&lt;/td&gt; &lt;td&gt;Integer&lt;/td&gt; &lt;td&gt;This is the description for the positioned common option.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;first.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;2&lt;/td&gt; &lt;td&gt;Integer&lt;/td&gt; &lt;td&gt;This is the description for the common option.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; @Documentation.TableOption 作用于 table 配置项上，用于添加元数据标签，配置执行模式（批处理、流式处理、两者兼有）。 123456@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Internalpublic @interface TableOption &#123; ExecMode execMode();&#125; 我们看下 ConfigOptionsDocGenerator 类中的 toHtmlString 方法： 123456789101112131415161718192021222324252627282930private static String toHtmlString(final OptionWithMetaInfo optionWithMetaInfo) &#123; ConfigOption&lt;?&gt; option = optionWithMetaInfo.option; String defaultValue = stringifyDefault(optionWithMetaInfo); String type = typeToHtml(optionWithMetaInfo); Documentation.TableOption tableOption = optionWithMetaInfo.field.getAnnotation(Documentation.TableOption.class); StringBuilder execModeStringBuilder = new StringBuilder(); if (tableOption != null) &#123; // 如果 ConfigOption 上有 @Documentation.TableOption 注解，则读取它的 execMode 字段，拼接到 html 内容中。 Documentation.ExecMode execMode = tableOption.execMode(); if (Documentation.ExecMode.BATCH_STREAMING.equals(execMode)) &#123; execModeStringBuilder.append("&lt;br&gt; &lt;span class=\"label label-primary\"&gt;") .append(Documentation.ExecMode.BATCH.toString()) .append("&lt;/span&gt; &lt;span class=\"label label-primary\"&gt;") .append(Documentation.ExecMode.STREAMING.toString()) .append("&lt;/span&gt;"); &#125; else &#123; execModeStringBuilder.append("&lt;br&gt; &lt;span class=\"label label-primary\"&gt;") .append(execMode.toString()) .append("&lt;/span&gt;"); &#125; &#125; return "" + " &lt;tr&gt;\n" + " &lt;td&gt;&lt;h5&gt;" + escapeCharacters(option.key()) + "&lt;/h5&gt;" + execModeStringBuilder.toString() + "&lt;/td&gt;\n" + " &lt;td style=\"word-wrap: break-word;\"&gt;" + escapeCharacters(addWordBreakOpportunities(defaultValue)) + "&lt;/td&gt;\n" + " &lt;td&gt;" + type + "&lt;/td&gt;\n" + " &lt;td&gt;" + formatter.format(option.description()) + "&lt;/td&gt;\n" + " &lt;/tr&gt;\n";&#125; @Documentation.ExcludeFromDocumentation 作用于 ConfigOption 配置项，用于从最终生成的 HTML 文档中移除配置项。 12345678910@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Internalpublic @interface ExcludeFromDocumentation &#123; /** * The optional reason why the config option is excluded from documentation. * 解释下从文档中移除配置项的原因 */ String value() default "";&#125; 下面通过一个示例来说明这个注解的用途。查看测试类 ConfigOptionsDocGeneratorTest 中应用到 @Documentation.ExcludeFromDocumentation 的单测 testConfigOptionExclusion： 1234@Testpublic void testConfigOptionExclusion() &#123; final String htmlTable = ConfigOptionsDocGenerator.generateTablesForClass(TestConfigGroupWithExclusion.class).get(0).f1;&#125; TestConfigGroupWithExclusion 类 mock 了一个配置项类：excludedOption 使用了 @Documentation.ExcludeFromDocumentation 注解，在生成的 HTML 文档中它将被移除。 123456789101112static class TestConfigGroupWithExclusion &#123; public static ConfigOption&lt;Integer&gt; firstOption = ConfigOptions .key("first.option.a") .defaultValue(2) .withDescription("This is example description for the first option."); @Documentation.ExcludeFromDocumentation public static ConfigOption&lt;String&gt; excludedOption = ConfigOptions .key("excluded.option.a") .noDefaultValue() .withDescription("This should not be documented.");&#125; 运行单测 testConfigOptionExclusion 的输出结果如下： 123456789101112131415161718&lt;table class="table table-bordered"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class="text-left" style="width: 20%"&gt;Key&lt;/th&gt; &lt;th class="text-left" style="width: 15%"&gt;Default&lt;/th&gt; &lt;th class="text-left" style="width: 10%"&gt;Type&lt;/th&gt; &lt;th class="text-left" style="width: 55%"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;h5&gt;first.option.a&lt;/h5&gt;&lt;/td&gt; &lt;td style="word-wrap: break-word;"&gt;2&lt;/td&gt; &lt;td&gt;Integer&lt;/td&gt; &lt;td&gt;This is example description for the first option.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; 其他标记注解关于这几种标记注解，源码中暂时还没有找到相关测试用例，后续补充。 @Experimental 表示标记对象是试验使用的注解，带有此注解的类是没有经过严格测试和不稳定的，可能在以后的版本中被修改或移除。 12345@Documented@Target(&#123;ElementType.TYPE, ElementType.METHOD, ElementType.FIELD, ElementType.CONSTRUCTOR &#125;)@Publicpublic @interface Experimental &#123;&#125; @Internal 将稳定的公共的api注解为内部开发者api，内部开发者api是稳定的，面向Flink内部，可能随着版本变化。 12345@Documented@Target(&#123; ElementType.TYPE, ElementType.METHOD, ElementType.CONSTRUCTOR &#125;)@Publicpublic @interface Internal &#123;&#125; @Public 标注类为开放的，稳定的。类、方法、属性被这个这个注解修饰时，表示在小版本迭代(1.0,1.1,1.2)中，都维持稳定，应用程序将根据同一大版本进行编译。 1234@Documented@Target(ElementType.TYPE)@Publicpublic @interface Public &#123;&#125; @PublicEvolving 带有此注解的类和方法用于公共使用，并且具有稳定的行为。但是，它们的接口和签名不被认为是稳定的，并且当跨版本时可能会变化。 12345@Documented@Target(&#123; ElementType.TYPE, ElementType.METHOD, ElementType.FIELD, ElementType.CONSTRUCTOR &#125;)@Publicpublic @interface PublicEvolving &#123;&#125; @VisibleForTesting 标注有些方法、属性、构造函数、类等在 test 阶段可见，用于测试。例如，当方法是 private 的，不打算在外部去调用的，但是有些内部测试需要访问它，所以加上 VisibleForTesting 注解进行内部测试。 1234@Documented@Target(&#123; ElementType.TYPE, ElementType.METHOD, ElementType.FIELD, ElementType.CONSTRUCTOR &#125;)@Internalpublic @interface VisibleForTesting &#123;&#125;]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink源码剖析-flink-metrics]]></title>
    <url>%2F2020%2F04%2F05%2FFlink%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-Flink-Metrics%2F</url>
    <content type="text"><![CDATA[本文将详细介绍下Flink中的指标实现，包括自带的指标名和如何自定义指标。还会介绍下现在已经支持的reporter，如jmx、slf4j、influxdb、graphite、prometheus、pushgateway等。最后介绍下flink指标平台化实践。 flink-metrics-core Metric： 指标类型有Gauge、Count、Meter、Histogram。 MetricConfig： MetricGroup： Metric 在 flink 内部以 Group 的方式组织，有多层结构，Metric Group + Metric Name 是 Metric 的唯一标识。 123456789TaskManagerMetricGroup •TaskManagerJobMetricGroup •TaskMetricGroup •TaskIOMetricGroup •OperatorMetricGroup •$&#123;User-defined Group&#125; / $&#123;User-defined Metrics&#125; •OperatorIOMetricGroup•JobManagerMetricGroup •JobManagerJobMetricGroup 可以根据需要埋点自定义指标。 添加一个统计脏数据的指标，指标名为flink_taskmanager_job_task_operator_dtDirtyData ： 12// 从 RichFunction 中 getRuntimeContext() dirtyDataCounter = runtimeContext.getMetricGroup().counter(MetricConstant.DT_DIRTY_DATA_COUNTER); 添加一个消费延迟指标，自定了两层Group，分别是topic、partition，指标名为flink_taskmanager_job_task_operator_topic_partition_dtTopicPartitionLag ： 12345for(TopicPartition topicPartition : assignedPartitions)&#123; MetricGroup metricGroup = getRuntimeContext().getMetricGroup().addGroup(DT_TOPIC_GROUP, topicPartition.topic()) .addGroup(DT_PARTITION_GROUP, String.valueOf(topicPartition.partition())); metricGroup.gauge(DT_TOPIC_PARTITION_LAG_GAUGE, new KafkaTopicPartitionLagMetric(subscriptionState, topicPartition));&#125; MetricReporter：flink 内置了多种指标 reporter ，如jmx、slf4j、graphite、prometheus、influxdb、statsd、datadog等。 指标 Reportersflink-metrics-dropwizard只是将flink内部定义的指标org.apache.flink.metrics.Metric和dropwizard中定义的指标com.codahale.metrics.Metric接口和子类互相包装转换。并且实现了 ScheduledDropwizardReporter ： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public static final String ARG_HOST = "host";public static final String ARG_PORT = "port";public static final String ARG_PREFIX = "prefix";public static final String ARG_CONVERSION_RATE = "rateConversion";public static final String ARG_CONVERSION_DURATION = "durationConversion";// ------------------------------------------------------------------------/** * dropwizard 包中的 MetricRegistry */protected final MetricRegistry registry;/** * dropwizard 包中的 ScheduledReporter */protected ScheduledReporter reporter;private final Map&lt;Gauge&lt;?&gt;, String&gt; gauges = new HashMap&lt;&gt;();private final Map&lt;Counter, String&gt; counters = new HashMap&lt;&gt;();private final Map&lt;Histogram, String&gt; histograms = new HashMap&lt;&gt;();private final Map&lt;Meter, String&gt; meters = new HashMap&lt;&gt;();/** * 添加指标，添加指标，需要将flink内部的Metric转换成dropwizard中的Metric， * 再注册到 dropwizard 的 MetricRegistry 中 */@Overridepublic void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group) &#123; final String fullName = group.getMetricIdentifier(metricName, this); synchronized (this) &#123; if (metric instanceof Counter) &#123; counters.put((Counter) metric, fullName); registry.register(fullName, new FlinkCounterWrapper((Counter) metric)); &#125; else if (metric instanceof Gauge) &#123; gauges.put((Gauge&lt;?&gt;) metric, fullName); registry.register(fullName, FlinkGaugeWrapper.fromGauge((Gauge&lt;?&gt;) metric)); &#125; else if (metric instanceof Histogram) &#123; Histogram histogram = (Histogram) metric; histograms.put(histogram, fullName); if (histogram instanceof DropwizardHistogramWrapper) &#123; registry.register(fullName, ((DropwizardHistogramWrapper) histogram).getDropwizardHistogram()); &#125; else &#123; registry.register(fullName, new FlinkHistogramWrapper(histogram)); &#125; &#125; else if (metric instanceof Meter) &#123; Meter meter = (Meter) metric; meters.put(meter, fullName); if (meter instanceof DropwizardMeterWrapper) &#123; registry.register(fullName, ((DropwizardMeterWrapper) meter).getDropwizardMeter()); &#125; else &#123; registry.register(fullName, new FlinkMeterWrapper(meter)); &#125; &#125; else &#123; log.warn("Cannot add metric of type &#123;&#125;. This indicates that the reporter " + "does not support this metric type.", metric.getClass().getName()); &#125; &#125;&#125;/** * report 时直接从 dropwizard 的 MetricRegistry 中捞取所有指标，执行 ScheduledReporter 的 report 方法 */@Overridepublic void report() &#123; // we do not need to lock here, because the dropwizard registry is // internally a concurrent map @SuppressWarnings("rawtypes") final SortedMap&lt;String, com.codahale.metrics.Gauge&gt; gauges = registry.getGauges(); final SortedMap&lt;String, com.codahale.metrics.Counter&gt; counters = registry.getCounters(); final SortedMap&lt;String, com.codahale.metrics.Histogram&gt; histograms = registry.getHistograms(); final SortedMap&lt;String, com.codahale.metrics.Meter&gt; meters = registry.getMeters(); final SortedMap&lt;String, com.codahale.metrics.Timer&gt; timers = registry.getTimers(); this.reporter.report(gauges, counters, histograms, meters, timers);&#125;public abstract ScheduledReporter getReporter(MetricConfig config); 只有flink-metrics-graphite模块会引用这个模块，直接复用 dropwizard 包提供的 GraphiteReporter 功能。 flink-metrics-graphiteGraphiteReporter 继承了 flink-metrics-dropwizard 模块中的 ScheduledDropwizardReporter。只需要实现其中的 getReporter() 抽象方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Overridepublic ScheduledReporter getReporter(MetricConfig config) &#123; String host = config.getString(ARG_HOST, null); int port = config.getInteger(ARG_PORT, -1); if (host == null || host.length() == 0 || port &lt; 1) &#123; throw new IllegalArgumentException("Invalid host/port configuration. Host: " + host + " Port: " + port); &#125; String prefix = config.getString(ARG_PREFIX, null); String conversionRate = config.getString(ARG_CONVERSION_RATE, null); String conversionDuration = config.getString(ARG_CONVERSION_DURATION, null); String protocol = config.getString(ARG_PROTOCOL, "TCP"); // 复用 dropwizard 包提供的 GraphiteReporter com.codahale.metrics.graphite.GraphiteReporter.Builder builder = com.codahale.metrics.graphite.GraphiteReporter.forRegistry(registry); if (prefix != null) &#123; builder.prefixedWith(prefix); &#125; if (conversionRate != null) &#123; builder.convertRatesTo(TimeUnit.valueOf(conversionRate)); &#125; if (conversionDuration != null) &#123; builder.convertDurationsTo(TimeUnit.valueOf(conversionDuration)); &#125; Protocol prot; try &#123; prot = Protocol.valueOf(protocol); &#125; catch (IllegalArgumentException iae) &#123; log.warn("Invalid protocol configuration: " + protocol + " Expected: TCP or UDP, defaulting to TCP."); prot = Protocol.TCP; &#125; log.info("Configured GraphiteReporter with &#123;host:&#123;&#125;, port:&#123;&#125;, protocol:&#123;&#125;&#125;", host, port, prot); switch(prot) &#123; case UDP: return builder.build(new GraphiteUDP(host, port)); case TCP: default: return builder.build(new Graphite(host, port)); &#125;&#125; 配置 复制 flink-metrics-graphite-xxx.jar 到 $FLINK_HOME/lib 下 在 flink-conf.yml 增加如下配置：1234metrics.reporter.grph.class: org.apache.flink.metrics.graphite.GraphiteReportermetrics.reporter.grph.host: localhost # Graphite server hostmetrics.reporter.grph.port: 2003 # Graphite server portmetrics.reporter.grph.protocol: TCP # protocol to use (TCP/UDP) flink-metrics-influxdbinfluxdb基本概念使用方法参考：时序数据库 Influxdb 使用详解为了方便理解 InfluxdbReporter 的实现，这里简单说下 Influxdb 中的几个概念： 1234567891011name: census-————————————time butterflies honeybees location scientist2015-08-18T00:00:00Z 12 23 1 langstroth2015-08-18T00:00:00Z 1 30 1 perpetua2015-08-18T00:06:00Z 11 28 1 langstroth2015-08-18T00:06:00Z 3 28 1 perpetua2015-08-18T05:54:00Z 2 11 2 langstroth2015-08-18T06:00:00Z 1 10 2 langstroth2015-08-18T06:06:00Z 8 23 2 perpetua2015-08-18T06:12:00Z 7 22 2 perpetua timestamp既然是时间序列数据库，influxdb 的数据都有一列名为 time 的列。 field key,field value,field setbufferflies 和 honeybees 为 field key，它们为String类型，用于存储元数据。数据 12-7 为 bufferflies 的field value，数据 23-22 为 honeybees 的field value。field value可以为String,float,integer或boolean类型。field key 和 field value 对组成的集合称之为 field set，如下： 12345678butterflies = 12 honeybees = 23butterflies = 1 honeybees = 30butterflies = 11 honeybees = 28butterflies = 3 honeybees = 28butterflies = 2 honeybees = 11butterflies = 1 honeybees = 10butterflies = 8 honeybees = 23butterflies = 7 honeybees = 22 在 influxdb 中，field 是必须的，但是字段是没有索引的，如果字段作为查询条件，会扫描所有符合查询条件的所有字段值。相当于SQL的没有索引的列。 tag key,tag value,tag setlocation 和 scientist 是两个tag，location 有两个 tag value：1和2，scientist 有两个 tag value：langstroth 和 perpetua。tag key 和 tag value 对组成的集合称之为 tag set，如下：1234location = 1, scientist = langstrothlocation = 2, scientist = langstrothlocation = 1, scientist = perpetualocation = 2, scientist = perpetua 在 influxdb 中，tag 是可选的，但 tag 相当于SQL中有索引的列，因此强烈建议使用。 measurement指标项，是 fields，tags 以及 time 列的容器。 retention policy数据保留策略，默认是 autogen，表示数据一直保留永不过期，副本数量为1。 series指共享同一个 retention policy，measurement 以及 tag set 的数据集合，如下： 123456｜ Arbitrary series number ｜ Retention policy ｜ Measurement ｜ Tag set ｜｜ ----------------------- ｜ ---------------- ｜ ----------- ｜ ------------------------------- ｜｜ series 1 ｜ autogen ｜ census ｜ location=1,scientist=langstroth ｜｜ series 2 ｜ autogen ｜ census ｜ location=2,scientist=perpetua ｜｜ series 3 ｜ autogen ｜ census ｜ location=1,scientist=langstroth ｜｜ series 4 ｜ autogen ｜ census ｜ location=2,scientist=perpetua ｜ point指的是同一个series中具有相同时间的 field set，points 相当于SQL中的数据行。如下： 1234name: census-----------------time butterflies honeybees location scientist2015-08-18T00:00:00Z 1 30 1 perpetua database 一个数据库可以有多个 measurement,retention policy,continuous queries以及user。提供InfluxQL语言查询和修改数据。 Reporter实现InfluxdbReporter的详细类图如下，包括继承以及依赖关系： MeasurementInfo 12345678/** * 指标项名称 */private final String name;/** * tag key 和 tag value对集合 */private final Map&lt;String, String&gt; tags; MeasurementInfoProvider 1234567/** * 根据 metricName 和 MetricGroup，将该指标项封装成 MeasurementInfo 返回 */@Overridepublic MeasurementInfo getMetricInfo(String metricName, MetricGroup group) &#123; return new MeasurementInfo(getScopedName(metricName, group), getTags(group));&#125; InfluxdbReporterOptions连接 influxdb 写指标的配置项，类似正常写RDBMS需要的配置 MetricMapper将 MeasurementInfo 转成 influxdb 中的 Point InfluxdbReporter extends AbstractReporter 构造函数中设置 MeasurementInfoProvider： 1234public InfluxdbReporter() &#123; // 设置 MeasurementInfoProvider super(new MeasurementInfoProvider());&#125; open() 方法中要根据指标配置文件初始化 InfluxDB 操作类： 1234567891011121314151617181920212223242526272829303132333435363738/** * 根据配置项初始化得到 InfluxDB 操作类 */@Overridepublic void open(MetricConfig config) &#123; String host = getString(config, HOST); int port = getInteger(config, PORT); if (!isValidHost(host) || !NetUtils.isValidClientPort(port)) &#123; throw new IllegalArgumentException("Invalid host/port configuration. Host: " + host + " Port: " + port); &#125; String database = getString(config, DB); if (database == null) &#123; throw new IllegalArgumentException("'" + DB.key() + "' configuration option is not set"); &#125; String url = String.format("http://%s:%d", host, port); String username = getString(config, USERNAME); String password = getString(config, PASSWORD); this.database = database; this.retentionPolicy = getString(config, RETENTION_POLICY); this.consistency = getConsistencyLevel(config, CONSISTENCY); int connectTimeout = getInteger(config, CONNECT_TIMEOUT); int writeTimeout = getInteger(config, WRITE_TIMEOUT); // 使用 okhttp 包中提供的 HttpClient OkHttpClient.Builder client = new OkHttpClient.Builder() .connectTimeout(connectTimeout, TimeUnit.MILLISECONDS) .writeTimeout(writeTimeout, TimeUnit.MILLISECONDS); if (username != null &amp;&amp; password != null) &#123; influxDB = InfluxDBFactory.connect(url, username, password, client); &#125; else &#123; influxDB = InfluxDBFactory.connect(url, client); &#125; log.info("Configured InfluxDBReporter with &#123;host:&#123;&#125;, port:&#123;&#125;, db:&#123;&#125;, retentionPolicy:&#123;&#125; and consistency:&#123;&#125;&#125;", host, port, database, retentionPolicy, consistency.name());&#125; AbstractReporter 中的 notifyOfAddedMetric() 方法中添加指标时将 flink 内部定义的 Metric 转成 MeasurementInfo： 123456789101112131415161718@Overridepublic void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group) &#123; final MetricInfo metricInfo = metricInfoProvider.getMetricInfo(metricName, group); synchronized (this) &#123; if (metric instanceof Counter) &#123; counters.put((Counter) metric, metricInfo); &#125; else if (metric instanceof Gauge) &#123; gauges.put((Gauge&lt;?&gt;) metric, metricInfo); &#125; else if (metric instanceof Histogram) &#123; histograms.put((Histogram) metric, metricInfo); &#125; else if (metric instanceof Meter) &#123; meters.put((Meter) metric, metricInfo); &#125; else &#123; log.warn("Cannot add unknown metric type &#123;&#125;. This indicates that the reporter " + "does not support this metric type.", metric.getClass().getName()); &#125; &#125;&#125; report()方法要将 MeasurementInfo 转成 influxdb 中的 Point 对象： 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic void report() &#123; BatchPoints report = buildReport(); if (report != null) &#123; influxDB.write(report); &#125;&#125;/*** 将指标信息封装成 influxdb 中的 BatchPoints* @return BatchPoints*/@Nullableprivate BatchPoints buildReport() &#123; // 取当前时间点 Instant timestamp = Instant.now(); BatchPoints.Builder report = BatchPoints.database(database); // 设置保留策略 report.retentionPolicy(retentionPolicy); report.consistency(consistency); try &#123; for (Map.Entry&lt;Gauge&lt;?&gt;, MeasurementInfo&gt; entry : gauges.entrySet()) &#123; // MeasurementInfo -&gt; Point report.point(MetricMapper.map(entry.getValue(), timestamp, entry.getKey())); &#125; for (Map.Entry&lt;Counter, MeasurementInfo&gt; entry : counters.entrySet()) &#123; report.point(MetricMapper.map(entry.getValue(), timestamp, entry.getKey())); &#125; for (Map.Entry&lt;Histogram, MeasurementInfo&gt; entry : histograms.entrySet()) &#123; report.point(MetricMapper.map(entry.getValue(), timestamp, entry.getKey())); &#125; for (Map.Entry&lt;Meter, MeasurementInfo&gt; entry : meters.entrySet()) &#123; report.point(MetricMapper.map(entry.getValue(), timestamp, entry.getKey())); &#125; &#125; catch (ConcurrentModificationException | NoSuchElementException e) &#123; // ignore - may happen when metrics are concurrently added or removed // report next time return null; &#125; return report.build();&#125; 配置 复制 flink-metrics-influxdb-xxx.jar 到 $FLINK_HOME/lib 下 在 flink-conf.yml 增加如下配置： 1234567metrics.reporter.influxdb.class: org.apache.flink.metrics.influxdb.InfluxdbReportermetrics.reporter.influxdb.host: localhost # the InfluxDB server hostmetrics.reporter.influxdb.port: 8086 # (optional) the InfluxDB server port, defaults to 8086metrics.reporter.influxdb.db: flink # the InfluxDB database to store metricsmetrics.reporter.influxdb.username: flink-metrics # (optional) InfluxDB usernamemetrics.reporter.influxdb.password: qwerty # (optional) InfluxDB username’s passwordmetrics.reporter.influxdb.retentionPolicy: one_hour # (optional) InfluxDB retention policy flink-metrics-prometheusprometheus基本概念Reporter实现Prometheus Reporter的详细类图如下，包括继承以及依赖关系： AbstractPrometheusReporter123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Overridepublic void notifyOfAddedMetric(final Metric metric, final String metricName, final MetricGroup group) &#123; // 维度key集合 List&lt;String&gt; dimensionKeys = new LinkedList&lt;&gt;(); // 维度value集合 List&lt;String&gt; dimensionValues = new LinkedList&lt;&gt;(); for (final Map.Entry&lt;String, String&gt; dimension : group.getAllVariables().entrySet()) &#123; final String key = dimension.getKey(); dimensionKeys.add(CHARACTER_FILTER.filterCharacters(key.substring(1, key.length() - 1))); dimensionValues.add(labelValueCharactersFilter.filterCharacters(dimension.getValue())); &#125; final String scopedMetricName = getScopedName(metricName, group); final String helpString = metricName + " (scope: " + getLogicalScope(group) + ")"; final Collector collector; Integer count = 0; synchronized (this) &#123; if (collectorsWithCountByMetricName.containsKey(scopedMetricName)) &#123; final AbstractMap.SimpleImmutableEntry&lt;Collector, Integer&gt; collectorWithCount = collectorsWithCountByMetricName.get(scopedMetricName); collector = collectorWithCount.getKey(); count = collectorWithCount.getValue(); &#125; else &#123; collector = createCollector(metric, dimensionKeys, dimensionValues, scopedMetricName, helpString); try &#123; // 注册当前的 collector 到 CollectorRegistry.defaultRegistry 中 collector.register(); &#125; catch (Exception e) &#123; log.warn("There was a problem registering metric &#123;&#125;.", metricName, e); &#125; &#125; // addMetric(metric, dimensionValues, collector); collectorsWithCountByMetricName.put(scopedMetricName, new AbstractMap.SimpleImmutableEntry&lt;&gt;(collector, count + 1)); &#125;&#125;/*** 将 Metric 转成 prometheus 的 Collector*/private Collector createCollector(Metric metric, List&lt;String&gt; dimensionKeys, List&lt;String&gt; dimensionValues, String scopedMetricName, String helpString) &#123; Collector collector; if (metric instanceof Gauge || metric instanceof Counter || metric instanceof Meter) &#123; collector = io.prometheus.client.Gauge .build() .name(scopedMetricName) .help(helpString) .labelNames(toArray(dimensionKeys)) .create(); &#125; else if (metric instanceof Histogram) &#123; collector = new HistogramSummaryProxy((Histogram) metric, scopedMetricName, helpString, dimensionKeys, dimensionValues); &#125; else &#123; log.warn("Cannot create collector for unknown metric type: &#123;&#125;. This indicates that the metric type is not supported by this reporter.", metric.getClass().getName()); collector = null; &#125; return collector;&#125;/*** 取出 Metric 中的值，为 Collector 设置 label values*/private void addMetric(Metric metric, List&lt;String&gt; dimensionValues, Collector collector) &#123; if (metric instanceof Gauge) &#123; ((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Gauge) metric), toArray(dimensionValues)); &#125; else if (metric instanceof Counter) &#123; ((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Counter) metric), toArray(dimensionValues)); &#125; else if (metric instanceof Meter) &#123; ((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Meter) metric), toArray(dimensionValues)); &#125; else if (metric instanceof Histogram) &#123; ((HistogramSummaryProxy) collector).addChild((Histogram) metric, dimensionValues); &#125; else &#123; log.warn("Cannot add unknown metric type: &#123;&#125;. This indicates that the metric type is not supported by this reporter.", metric.getClass().getName()); &#125;&#125; 注意：从Gauge中取值时不支持返回值为String： 1234567891011121314151617181920212223242526@VisibleForTestingio.prometheus.client.Gauge.Child gaugeFrom(Gauge gauge) &#123; return new io.prometheus.client.Gauge.Child() &#123; @Override public double get() &#123; final Object value = gauge.getValue(); // 注意：这里只支持 Gauge 的返回值为 Double、Number、Boolean 的，暂时不支持String if (value == null) &#123; log.debug("Gauge &#123;&#125; is null-valued, defaulting to 0.", gauge); return 0; &#125; if (value instanceof Double) &#123; return (double) value; &#125; if (value instanceof Number) &#123; return ((Number) value).doubleValue(); &#125; if (value instanceof Boolean) &#123; return ((Boolean) value) ? 1 : 0; &#125; log.debug("Invalid type for Gauge &#123;&#125;: &#123;&#125;, only number types and booleans are supported by this reporter.", gauge, value.getClass().getName()); return 0; &#125; &#125;;&#125; 如 LatestCompletedCheckpointExternalPathGauge 这个指标，用来记录上次完成的 checkpoint 路径，它的返回值是 String 类型，在向 PrometheusPushgateway 推送的时候会报错。 1234567891011private class LatestCompletedCheckpointExternalPathGauge implements Gauge&lt;String&gt; &#123; @Override public String getValue() &#123; CompletedCheckpointStats completed = latestCompletedCheckpoint; if (completed != null &amp;&amp; completed.getExternalPath() != null) &#123; return completed.getExternalPath(); &#125; else &#123; return "n/a"; &#125; &#125;&#125; 报错如下： 123456789101112131420:06:36.782 [Flink-MetricRegistry-thread-1] DEBUG org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter - Invalid type for Gauge org.apache.flink.runtime.checkpoint.CheckpointStatsTracker$LatestCompletedCheckpointExternalPathGauge@78b86b65: java.lang.String, only number types and booleans are supported by this reporter.20:06:36.810 [Flink-MetricRegistry-thread-1] WARN org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter - Failed to push metrics to PushGateway with jobName flinkSql_KUDUside_KUDUsink_20200324110200_69196657381606602020032420061183311.java.io.IOException: Response code from http://xx.xx.xx.xx:8891/metrics/job/flinkSql_KUDUside_KUDUsink_20200324110200_69196657381606602020032420061183311 was 200 at org.apache.flink.shaded.io.prometheus.client.exporter.PushGateway.doRequest(PushGateway.java:297) at org.apache.flink.shaded.io.prometheus.client.exporter.PushGateway.push(PushGateway.java:105) at org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter.report(PrometheusPushGatewayReporter.java:76) at org.apache.flink.runtime.metrics.MetricRegistryImpl$ReporterTask.run(MetricRegistryImpl.java:436) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) PrometheusPushGatewayReporterOptions连接 PrometheusPushGateway 写指标的配置项 PrometheusPushGatewayReporter open() 方法中要根据指标配置文件初始化 PushGateway 操作类： 12345678910111213141516171819202122232425262728/** * 根据配置项初始化得到 PushGateway 操作类 */@Overridepublic void open(MetricConfig config) &#123; super.open(config); String host = config.getString(HOST.key(), HOST.defaultValue()); int port = config.getInteger(PORT.key(), PORT.defaultValue()); String configuredJobName = config.getString(JOB_NAME.key(), JOB_NAME.defaultValue()); boolean randomSuffix = config.getBoolean(RANDOM_JOB_NAME_SUFFIX.key(), RANDOM_JOB_NAME_SUFFIX.defaultValue()); deleteOnShutdown = config.getBoolean(DELETE_ON_SHUTDOWN.key(), DELETE_ON_SHUTDOWN.defaultValue()); groupingKey = parseGroupingKey(config.getString(GROUPING_KEY.key(), GROUPING_KEY.defaultValue())); if (host == null || host.isEmpty() || port &lt; 1) &#123; throw new IllegalArgumentException("Invalid host/port configuration. Host: " + host + " Port: " + port); &#125; if (randomSuffix) &#123; this.jobName = configuredJobName + new AbstractID(); &#125; else &#123; this.jobName = configuredJobName; &#125; pushGateway = new PushGateway(host + ':' + port); log.info("Configured PrometheusPushGatewayReporter with &#123;host:&#123;&#125;, port:&#123;&#125;, jobName:&#123;&#125;, randomJobNameSuffix:&#123;&#125;, deleteOnShutdown:&#123;&#125;, groupingKey:&#123;&#125;&#125;", host, port, jobName, randomSuffix, deleteOnShutdown, groupingKey);&#125; report() 方法中调用 pushgateway 的 push() 方法，直接走HTTP将指标推送出去了： 123456789@Overridepublic void report() &#123; try &#123; // 使用 PushGateway 的 push 方法，走 HTTP 协议，将指标推送到 PushGateway pushGateway.push(CollectorRegistry.defaultRegistry, jobName, groupingKey); &#125; catch (Exception e) &#123; log.warn("Failed to push metrics to PushGateway with jobName &#123;&#125;, groupingKey &#123;&#125;.", jobName, groupingKey, e); &#125;&#125; PrometheusReporter open() 方法中要根据指标配置文件初始化一个HttpServer，让 Prometheus 来拉取：注意：PrometheusReporter 类没有实现 Scheduled 接口，没有 report() 方法，因为它的指标是拉的，不是主动推的。 1234567891011121314151617181920212223242526/** * 根据配置项初始化一个 HttpServer，让 Prometheus 来拉取指标 */@Overridepublic void open(MetricConfig config) &#123; super.open(config); String portsConfig = config.getString(ARG_PORT, DEFAULT_PORT); Iterator&lt;Integer&gt; ports = NetUtils.getPortRangeFromString(portsConfig); while (ports.hasNext()) &#123; int port = ports.next(); try &#123; // internally accesses CollectorRegistry.defaultRegistry httpServer = new HTTPServer(port); this.port = port; log.info("Started PrometheusReporter HTTP server on port &#123;&#125;.", port); break; &#125; catch (IOException ioe) &#123; //assume port conflict log.debug("Could not start PrometheusReporter HTTP server on port &#123;&#125;.", port, ioe); &#125; &#125; if (httpServer == null) &#123; throw new RuntimeException("Could not start PrometheusReporter HTTP server on any configured port. Ports: " + portsConfig); &#125;&#125; 配置 复制 flink-metrics-prometheus-xxx.jar 到 $FLINK_HOME/lib 下 如果使用 PrometheusReporter ，则在 flink-conf.yml 增加如下配置： 12metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReportermetrics.reporter.prom.port: 9249 如果使用 PrometheusPushGatewayReporter ，则在 flink-conf.yml 增加如下配置： 123456metrics.reporter.promgateway.class: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReportermetrics.reporter.promgateway.host: localhostmetrics.reporter.promgateway.port: 9091metrics.reporter.promgateway.jobName: myJobmetrics.reporter.promgateway.randomJobNameSuffix: truemetrics.reporter.promgateway.deleteOnShutdown: false flink-metrics-jmxjmx基本概念JMX（Java Management Extensions）是一个应用程序植入管理功能的框架。JMX 是一套标准的代理和服务，实际上，用户可以在任何Java应用程序中使用这些代理和服务实现管理。 JMX 架构图如下： Reporter实现 JMXReporter： 首先通过 ManagementFactory.getPlatformMBeanServer() 获取JVM中全局唯一的 MBeanServer 单例。 123456789101112131415161718192021222324252627282930313233343536JMXReporter(@Nullable final String portsConfig) &#123; // 获取 MBeanServer 单例 this.mBeanServer = ManagementFactory.getPlatformMBeanServer(); // 存放注册指标的Map this.registeredMetrics = new HashMap&lt;&gt;(); if (portsConfig != null) &#123; Iterator&lt;Integer&gt; ports = NetUtils.getPortRangeFromString(portsConfig); JMXServer successfullyStartedServer = null; while (ports.hasNext() &amp;&amp; successfullyStartedServer == null) &#123; JMXServer server = new JMXServer(); int port = ports.next(); try &#123; // 创建并启动 Registry 和 JMXConnectorServer server.start(port); LOG.info("Started JMX server on port " + port + "."); successfullyStartedServer = server; &#125; catch (IOException ioe) &#123; //assume port conflict LOG.debug("Could not start JMX server on port " + port + ".", ioe); try &#123; server.stop(); &#125; catch (Exception e) &#123; LOG.debug("Could not stop JMX server.", e); &#125; &#125; &#125; if (successfullyStartedServer == null) &#123; throw new RuntimeException("Could not start JMX server on any configured port. Ports: " + portsConfig); &#125; this.jmxServer = successfullyStartedServer; &#125; else &#123; this.jmxServer = null; &#125; LOG.info("Configured JMXReporter with &#123;port:&#123;&#125;&#125;", portsConfig);&#125; JMXRreporter中的MetricMBean： 添加指标项时，需要将 flink 中的 Metric 对象转换成 MetricBean ，再注册到 MBeanServer 中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Overridepublic void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group) &#123; final String domain = generateJmxDomain(metricName, group); final Hashtable&lt;String, String&gt; table = generateJmxTable(group.getAllVariables()); AbstractBean jmxMetric; ObjectName jmxName; try &#123; jmxName = new ObjectName(domain, table); &#125; catch (MalformedObjectNameException e) &#123; /** * There is an implementation error on our side if this occurs. Either the domain was modified and no longer * conforms to the JMX domain rules or the table wasn't properly generated. */ LOG.debug("Implementation error. The domain or table does not conform to JMX rules." , e); return; &#125; // 将 flink 中的 Metric 转成 MBean if (metric instanceof Gauge) &#123; jmxMetric = new JmxGauge((Gauge&lt;?&gt;) metric); &#125; else if (metric instanceof Counter) &#123; jmxMetric = new JmxCounter((Counter) metric); &#125; else if (metric instanceof Histogram) &#123; jmxMetric = new JmxHistogram((Histogram) metric); &#125; else if (metric instanceof Meter) &#123; jmxMetric = new JmxMeter((Meter) metric); &#125; else &#123; LOG.error("Cannot add unknown metric type: &#123;&#125;. This indicates that the metric type " + "is not supported by this reporter.", metric.getClass().getName()); return; &#125; try &#123; synchronized (this) &#123; // 注册到 MetricBean 到 MBeanServer 中 mBeanServer.registerMBean(jmxMetric, jmxName); registeredMetrics.put(metric, jmxName); &#125; &#125; catch (NotCompliantMBeanException e) &#123; // implementation error on our side LOG.debug("Metric did not comply with JMX MBean rules.", e); &#125; catch (InstanceAlreadyExistsException e) &#123; LOG.warn("A metric with the name " + jmxName + " was already registered.", e); &#125; catch (Throwable t) &#123; LOG.warn("Failed to register metric", t); &#125;&#125; 配置 在 flink-conf.yml 增加如下配置：12metrics.reporter.jmx.factory.class: org.apache.flink.metrics.jmx.JMXReporterFactorymetrics.reporter.jmx.port: 8789 # 如果有多个 TM 在同一台机器，端口可以设置成范围 9250-9260 flink-metrics-slf4jSlf4jReporter 继承了 flink-metrics-core 模块中的 AbstractReporter，复用其添加移除指标的方法。report() 方法的逻辑其实就是遍历所有的指标项，拼接成字符串，打印到日志文件中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869private void tryReport() &#123; // initialize with previous size to avoid repeated resizing of backing array // pad the size to allow deviations in the final string, for example due to different double value representations StringBuilder builder = new StringBuilder((int) (previousSize * 1.1)); builder .append(lineSeparator) .append("=========================== Starting metrics report ===========================") .append(lineSeparator); builder .append(lineSeparator) .append("-- Counters -------------------------------------------------------------------") .append(lineSeparator); for (Map.Entry&lt;Counter, String&gt; metric : counters.entrySet()) &#123; builder .append(metric.getValue()).append(": ").append(metric.getKey().getCount()) .append(lineSeparator); &#125; builder .append(lineSeparator) .append("-- Gauges ---------------------------------------------------------------------") .append(lineSeparator); for (Map.Entry&lt;Gauge&lt;?&gt;, String&gt; metric : gauges.entrySet()) &#123; builder .append(metric.getValue()).append(": ").append(metric.getKey().getValue()) .append(lineSeparator); &#125; builder .append(lineSeparator) .append("-- Meters ---------------------------------------------------------------------") .append(lineSeparator); for (Map.Entry&lt;Meter, String&gt; metric : meters.entrySet()) &#123; builder .append(metric.getValue()).append(": ").append(metric.getKey().getRate()) .append(lineSeparator); &#125; builder .append(lineSeparator) .append("-- Histograms -----------------------------------------------------------------") .append(lineSeparator); for (Map.Entry&lt;Histogram, String&gt; metric : histograms.entrySet()) &#123; HistogramStatistics stats = metric.getKey().getStatistics(); builder .append(metric.getValue()).append(": count=").append(stats.size()) .append(", min=").append(stats.getMin()) .append(", max=").append(stats.getMax()) .append(", mean=").append(stats.getMean()) .append(", stddev=").append(stats.getStdDev()) .append(", p50=").append(stats.getQuantile(0.50)) .append(", p75=").append(stats.getQuantile(0.75)) .append(", p95=").append(stats.getQuantile(0.95)) .append(", p98=").append(stats.getQuantile(0.98)) .append(", p99=").append(stats.getQuantile(0.99)) .append(", p999=").append(stats.getQuantile(0.999)) .append(lineSeparator); &#125; builder .append(lineSeparator) .append("=========================== Finished metrics report ===========================") .append(lineSeparator); LOG.info(builder.toString()); previousSize = builder.length();&#125; 配置 复制 flink-metrics-slf4j-xxx.jar 到 $FLINK_HOME/lib 下 在 flink-conf.yml 增加如下配置：12metrics.reporter.slf4j.class: org.apache.flink.metrics.slf4j.Slf4jReportermetrics.reporter.slf4j.interval: 60 SECONDS flink-metrics-statsdstatsd基本概念statsd 从狭义上讲，其实就是一个监听 UDP（Default）/TCP的守护程序。statsd 系统包括三部分：客户端（client）、服务器（server）和后端（backend）StatsDReporter 相当于 statsd 系统的客户端，将 metrics 上报给 statsd server，statsd server 聚合这些 metrics 之后，定时发送给 backend，backend 则负责存储这些时间序列数据，并通过适当的图表工具展示。 statsd 经常与 graphite 一起使用，statsd 负责收集并聚合测量值，之后将数据传给 graphite ，graphite 以时间序列为依据存储数据，并绘制图表。 Reporter实现这里我们只关注下发送 UDP 数据包的方法： 1234567891011121314private void send(final String name, final String value) &#123; try &#123; // statsd 的协议其实非常简单，每一行就是一条数据，g 表示 Gauge // 如 "system.load.1min:0.5|g"，表示某一时刻系统1分钟的负载为0.5 String formatted = String.format("%s:%s|g", name, value); byte[] data = formatted.getBytes(StandardCharsets.UTF_8); // 默认通过 socket 发送 UDP 数据包到 StatsD // 因为 UDP 比 TCP 更快，不想为了追踪应用的表现而减慢其速度 socket.send(new DatagramPacket(data, data.length, this.address)); &#125; catch (IOException e) &#123; LOG.error("unable to send packet to statsd at '&#123;&#125;:&#123;&#125;'", address.getHostName(), address.getPort()); &#125;&#125; 配置 复制 flink-metrics-statsd-xxx.jar 到 $FLINK_HOME/lib 下 在 flink-conf.yml 增加如下配置：123metrics.reporter.stsd.class: org.apache.flink.metrics.statsd.StatsDReportermetrics.reporter.stsd.host: localhost # the StatsD server hostmetrics.reporter.stsd.port: 8125 # the StatsD server port flink-metrics-datadogdatadog这里就不详细说了，其实就是添加指标时将 flink 中的 Metric 转成 DMetric汇报时将 DMetric 指标封装成 DatadogHttpRequest，使用 HttpClient 发送出去 12345678910111213141516171819202122232425262728293031323334353637383940414243@Overridepublic void report() &#123; DatadogHttpRequest request = new DatadogHttpRequest(); List&lt;Gauge&gt; gaugesToRemove = new ArrayList&lt;&gt;(); for (Map.Entry&lt;Gauge, DGauge&gt; entry : gauges.entrySet()) &#123; DGauge g = entry.getValue(); try &#123; // Will throw exception if the Gauge is not of Number type // Flink uses Gauge to store many types other than Number g.getMetricValue(); request.addGauge(g); &#125; catch (ClassCastException e) &#123; LOGGER.info("The metric &#123;&#125; will not be reported because only number types are supported by this reporter.", g.getMetric()); gaugesToRemove.add(entry.getKey()); &#125; catch (Exception e) &#123; if (LOGGER.isDebugEnabled()) &#123; LOGGER.debug("The metric &#123;&#125; will not be reported because it threw an exception.", g.getMetric(), e); &#125; else &#123; LOGGER.info("The metric &#123;&#125; will not be reported because it threw an exception.", g.getMetric()); &#125; gaugesToRemove.add(entry.getKey()); &#125; &#125; gaugesToRemove.forEach(gauges::remove); for (DCounter c : counters.values()) &#123; request.addCounter(c); &#125; for (DMeter m : meters.values()) &#123; request.addMeter(m); &#125; try &#123; client.send(request); LOGGER.debug("Reported series with size &#123;&#125;.", request.getSeries().getSeries().size()); &#125; catch (SocketTimeoutException e) &#123; LOGGER.warn("Failed reporting metrics to Datadog because of socket timeout: &#123;&#125;.", e.getMessage()); &#125; catch (Exception e) &#123; LOGGER.warn("Failed reporting metrics to Datadog.", e); &#125;&#125; 配置 复制 flink-metrics-datadog-xxx.jar 到 $FLINK_HOME/lib 下 在 flink-conf.yml 增加如下配置：123456metrics.reporter.dghttp.class: org.apache.flink.metrics.datadog.DatadogHttpReportermetrics.reporter.dghttp.apikey: xxx # the Datadog API key#(optional) the global tags that will be applied to metrics when sending to Datadog. Tags should be separated by comma onlymetrics.reporter.dghttp.tags: myflinkapp,prod metrics.reporter.dghttp.proxyHost: my.web.proxy.com #(optional) The proxy host to use when sending to Datadogmetrics.reporter.dghttp.proxyPort: 8080 #(optional) The proxy port to use when sending to Datadog, defaults to 8080 flink中的指标项在看 flink 指标项时，可以 Overview 指标名 flink_taskmanager_job_task_operator_dtNumBytesIn flink_taskmanager_job_task_operator_dtNumBytesInRate flink_taskmanager_job_task_operator_dtNumRecordsIn flink_taskmanager_job_task_operator_dtNumRecordsInRate flink_taskmanager_job_task_operator_dtNumRecordsInResolve flink_taskmanager_job_task_operator_dtNumRecordsInResolveRate flink_taskmanager_job_task_operator_dtNumRecordsOut flink_taskmanager_job_task_operator_dtNumRecordsOutRate flink_taskmanager_job_task_operator_dtDirtyData flink_taskmanager_job_task_operator_topic_partition_dtTopicPartitionLag flink_taskmanager_job_task_operator_dtEventDelay Checkpoint 指标名 flink_jobmanager_job_lastCheckpointDuration flink_jobmanager_job_lastCheckpointSize flink_jobmanager_job_numberOfFailedCheckpoints Watermark 指标名 flink_taskmanager_job_task_operator_currentInputWatermark flink_taskmanager_job_task_operator_currentOutputWatermark flink_taskmanager_job_task_operator_numLateRecordsDropped BackPressure 指标名 flink_taskmanager_job_task_buffers_inPoolUsage flink_taskmanager_job_task_buffers_outPoolUsage flink_taskmanager_job_task_buffers_inputQueueLength flink_taskmanager_job_task_buffers_outputQueueLength Kafka Connector 指标名 flink_taskmanager_job_task_operator_commitsFailed flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_currentOffsets flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max JVM 指标名 flink_jobmanager_Status_JVM_CPU_Load flink_jobmanager_Status_JVM_CPU_Time flink_jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Count flink_jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Time flink_jobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Count flink_jobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Time flink_jobmanager_Status_JVM_Memory_Heap_Max flink_jobmanager_Status_JVM_Memory_Heap_Used flink_jobmanager_Status_JVM_Memory_NonHeap_Max flink_jobmanager_Status_JVM_Memory_NonHeap_Used flink_jobmanager_Status_JVM_Threads_Count flink_taskmanager_Status_JVM_CPU_Load flink_taskmanager_Status_JVM_CPU_Time flink_taskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Count flink_taskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Time flink_taskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Count flink_taskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Time flink_taskmanager_Status_JVM_Memory_Heap_Max flink_taskmanager_Status_JVM_Memory_Heap_Used flink_taskmanager_Status_JVM_Memory_NonHeap_Max flink_taskmanager_Status_JVM_Memory_NonHeap_Used flink_taskmanager_Status_JVM_Threads_Count 指标平台化实践首先，参考：Monitor with Prometheus And Grafana，安装 Prometheus、pushgateway、Grafana 服务，以及学习如何在Grafana中添加指标项图标。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[插件化技术]]></title>
    <url>%2F2020%2F03%2F29%2F%E6%8F%92%E4%BB%B6%E5%8C%96%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[本文将介绍代码设计中的插件化实现。涉及到的关键技术点 自定义ClassLoader 和 ServiceLoader 。接着，会说下插件化技术的典型应用场景。 ClassLoader类加载的过程参考：JVM 中关于3.2 类的生命周期 介绍。 显式与隐式加载显式：在代码中通过调用 ClassLoader 加载 class 对象，如直接使用 Class.forName(name) 或 this.getClass().getClassLoader().loadClass() 加载 class 对象隐式：通过虚拟机自动加载到内存中，如在加载某个类的 class 文件时，该类的 class 文件中引用了另外一个类的对象，此时额外引用的类将通过 JVM 自动加载到内存中 一段源程序代码： 123456789101112public class Demo &#123; static int hello() &#123; int a = 1; int b = 2; int c = a + b; return c; &#125; public static void main(String[] args) &#123; System.out.println(hello()); &#125;&#125; 生成字节码文件： 1javac demo.java 对class文件反汇编： 12345javap -v -l -c demo.class &gt; Demo.txt-v: 不仅会输出行号、本地变量表信息、反编译汇编代码，还会输出当前类用到的常量池等信息-l: 输出行号和本地变量表信息-c: 会对当前 class 字节码进行反编译生成汇编代码 通过文件编译工具来查看demo.txt的内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485Classfile /private/tmp/Demo.class Last modified 2020-4-4; size 464 bytes MD5 checksum 2b2ee02c5a47ef7f4ed5388443f76800 Compiled from &quot;Demo.java&quot;public class Demo minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #6.#17 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #18.#19 // java/lang/System.out:Ljava/io/PrintStream; #3 = Methodref #5.#20 // Demo.hello:()I #4 = Methodref #21.#22 // java/io/PrintStream.println:(I)V #5 = Class #23 // Demo #6 = Class #24 // java/lang/Object #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 hello #12 = Utf8 ()I #13 = Utf8 main #14 = Utf8 ([Ljava/lang/String;)V #15 = Utf8 SourceFile #16 = Utf8 Demo.java #17 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #18 = Class #25 // java/lang/System #19 = NameAndType #26:#27 // out:Ljava/io/PrintStream; #20 = NameAndType #11:#12 // hello:()I #21 = Class #28 // java/io/PrintStream #22 = NameAndType #29:#30 // println:(I)V #23 = Utf8 Demo #24 = Utf8 java/lang/Object #25 = Utf8 java/lang/System #26 = Utf8 out #27 = Utf8 Ljava/io/PrintStream; #28 = Utf8 java/io/PrintStream #29 = Utf8 println #30 = Utf8 (I)V&#123; public Demo(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 1: 0 static int hello(); descriptor: ()I flags: ACC_STATIC Code: stack=2, locals=3, args_size=0 0: iconst_1 1: istore_0 2: iconst_2 3: istore_1 4: iload_0 5: iload_1 6: iadd 7: istore_2 8: iload_2 9: ireturn LineNumberTable: line 3: 0 line 4: 2 line 5: 4 line 6: 8 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: invokestatic #3 // Method hello:()I 6: invokevirtual #4 // Method java/io/PrintStream.println:(I)V 9: return LineNumberTable: line 10: 0 line 11: 9&#125; 解释下 #1 = Methodref #6.#17 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V：执行类的构造方法时，首先会执行父类的构造方法，java.lang.Object是任何类的父类，所以这边会首先执行 Object 类的构造方法，#1 会引用 #6、#17 对应的符号常量。 在JVM中表示两个class对象是否为同一个类对象存在两个必要条件： 类的完整类名必须一致，包括包名。 加载这个类的ClassLoader(指ClassLoader实例对象)必须相同。 Launcher启动类Launcher启动类图： 加载器类型 启动类加载器，由C++实现，没有父类。 拓展类加载器(ExtClassLoader)，由Java语言实现，父类加载器为null 系统类加载器(AppClassLoader)，由Java语言实现，父类加载器为ExtClassLoader 自定义类加载器，父类加载器肯定为AppClassLoader。 加载器之间的类图关系： loadClass(String)将类加载请求到来时，先从缓存中查找该类对象，如果不存在就走双亲委派模式。 1234567891011121314151617181920212223242526272829303132333435363738394041424344protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded // 首先判断这个 class 是否已经加载成功，只判断全限定名是否相同 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; // 先通过父类加载器查找，递归下去，直到 BootstrapClassLoader if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; // 如果父加载器为null，则用 BootstrapClassLoader 去加载 // 这也解释了 ExtClassLoader 的parent为null，但仍然说 BootstrapClassLoader 是它的父加载器 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); // 如果向上委托父加载没有加载成功，则通过 findClass(String) 查找 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; // 生成最终的Class对象，对应着验证、准备、解析的过程 resolveClass(c); &#125; return c; &#125; &#125; findClass(String)不建议直接覆盖 loadClass() 去打破双亲委派模式，建议把自定义逻辑写在 findClass() 中，findClass() 方法通常是和 defineClass() 方法一起使用的。 123protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name); &#125; defineClass(byte[] b,int off,int len)将byte字节流解析成JVM 能够识别的Class对象。 12345678910protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len, ProtectionDomain protectionDomain) throws ClassFormatError &#123; protectionDomain = preDefineClass(name, protectionDomain); String source = defineClassSourceLocation(protectionDomain); Class&lt;?&gt; c = defineClass1(name, b, off, len, protectionDomain, source); postDefineClass(c, protectionDomain); return c; &#125; resolveClass(Class&lt;?&gt; c)对应链接阶段，它是native方法，主要对字节码进行验证，为类变量分配内存并设置初始值，将字节码文件中的符号引用转换为直接引用。 123protected final void resolveClass(Class&lt;?&gt; c) &#123; resolveClass0(c); &#125; 自定义ClassLoader为什么要自定义ClassLoader呢？ 当 class 文件不在 classpath 路径下，默认系统类加载无法找到该 class 文件，此时需要实现一个自定义的 ClassLoader 来加载特定路径下的 class 文件生成 Class 对象 当一个 class 文件是通过网络传输并且可能会进行相应的加密操作时，需要先对 class 文件进行相应的解密后再加载到 JVM 内存中 当需要实现热部署功能时，一个 class 文件通过不同的类加载器产生不同 class 对象从而实现热部署功能 自定义FileClassLoader： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * Description: * * @author mwt * @version 1.0 * @date 2020-04-03 */public class FileClassLoader extends ClassLoader &#123; private static final String CLASS_FILE_SUFFIX = ".class"; private String mLibpath; public FileClassLoader(String mLibpath) &#123; this.mLibpath = mLibpath; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; String classFileName = getClassFileName(name); File file = new File(mLibpath, classFileName); try &#123; FileInputStream is = new FileInputStream(file); ByteArrayOutputStream bos = new ByteArrayOutputStream(); int len = 0; try &#123; while ((len = is.read()) != -1) &#123; bos.write(len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; byte[] data = bos.toByteArray(); is.close(); bos.close(); return defineClass(name, data, 0, data.length); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return super.findClass(name); &#125; /** * 读取java类对应的class文件 * * @param name * @return */ private String getClassFileName(String name) &#123; int index = name.lastIndexOf("."); if (index == -1) &#123; return name + CLASS_FILE_SUFFIX; &#125; else &#123; return name.substring(index + 1) + CLASS_FILE_SUFFIX; &#125; &#125;&#125; SPI在Java应用中存在着很多服务提供者接口，Service Provider Interface，这些接口允许第三方为它们提供实现，如常见的 SPI 有 JDBC、JNDI等。这些SPI的接口属于Java核心库，一般存在于rt.jar中，由 Bootstrap 类加载器加载，而 SPI 的第三方实现代码则是作为 Java 应用所依赖的jar包被存放在 classpath 路径下。SPI 接口中的代码经常需要加载第三方实现类并调用其相关方法，但 SPI 的核心接口类是由 Bootstrap 类加载器加载，由于双亲委派模式的存在，Bootstrap 类加载器也无法反向委托 AppClassLoader 加载 SPI 的实现类。此时，就需要一种特殊的类加载来加载第三方的类库，而线程上下文加载器就是很好的选择，可以破坏双亲委派模型。 如果没有手动设置上下文类加载器，线程将继承其父线程的上下文类加载器。如在Launcher类中，会将AppClassLoader设置到当前线程上下文： 1234567891011// Now create the class loader to use to launch the application try &#123; loader = AppClassLoader.getAppClassLoader(extcl); &#125; catch (IOException e) &#123; throw new InternalError( "Could not create application class loader", e); &#125; // Also set the context class loader for the primordial thread. // 设置AppClassLoader为线程上下文类加载器 Thread.currentThread().setContextClassLoader(loader); ServiceLoader首先看下 ServiceLoader 的成员： 1234567891011121314151617public final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt; &#123; // 指明了路径是在"META-INF/services“下 private static final String PREFIX = "META-INF/services/"; // 表示正在加载的服务的类或接口 private final Class&lt;S&gt; service; // 使用的类加载器 private final ClassLoader loader; // 创建ServiceLoader时获取的访问控制上下文 private final AccessControlContext acc; // 缓存的服务提供者集合 private LinkedHashMap&lt;String,S&gt; providers = new LinkedHashMap&lt;&gt;(); // 内部使用的迭代器，用于类的懒加载，只有在迭代时才加载 // ServiceLoader 的实际加载过程是交给 LazyIterator 来做的 private LazyIterator lookupIterator; ......&#125; 调用其静态的load方法： 12345678public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl);&#125;public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service, ClassLoader loader) &#123; return new ServiceLoader&lt;&gt;(service, loader);&#125; 关注下LazyIterator中的nextService方法： 12345678910111213141516171819202122232425262728293031private S nextService() &#123; if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try &#123; // 在迭代器的next中才会进行真正的类加载 c = Class.forName(cn, false, loader); &#125; catch (ClassNotFoundException x) &#123; fail(service, "Provider " + cn + " not found"); &#125; if (!service.isAssignableFrom(c)) &#123; fail(service, "Provider " + cn + " not a subtype"); &#125; try &#123; S p = service.cast(c.newInstance()); providers.put(cn, p); return p; &#125; catch (Throwable x) &#123; fail(service, "Provider " + cn + " could not be instantiated", x); &#125; throw new Error(); // This cannot happen&#125; JDBC DriverManager类的static块中会加载所用的Driver实现类： 12345678910111213141516171819//DriverManager是Java核心包rt.jar的类public class DriverManager &#123; //省略不必要的代码 static &#123; loadInitialDrivers();//执行该方法 println("JDBC DriverManager initialized"); &#125;//loadInitialDrivers方法 private static void loadInitialDrivers() &#123; sun.misc.Providers() AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; //加载外部的Driver的实现类 ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); //省略不必要的代码...... &#125; &#125;); &#125; ServiceLoader中的load方法： 12345public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; // 通过线程上下文类加载器加载，默认情况下就是AppClassLoader ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl);&#125; 在不同的数据库驱动包中的 META-INF/services 目录下都会有一个名为 java.sql.Driver 的文件，记录Driver的实现类。Mysql驱动包中： Oracle驱动包中： 最佳实践Flink中的插件化应用DataX插件加载原理插件的加载都是使用ClassLoader动态加载。 为了避免类的冲突，对于每个插件的加载，对应着独立的加载器。加载器由JarLoader实现，插件的加载接口由LoadUtil类负责。当要加载一个插件时，需要实例化一个JarLoader，然后切换thread class loader之后，才加载插件。 自定义JarLoaderJarLoader 继承 URLClassLoader，扩充了可以加载目录的功能。可以从指定的目录下，把传入的路径，及其子路径、以及路径中的jar文件加入到classpath。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class JarLoader extends URLClassLoader &#123; public JarLoader(String[] paths) &#123; this(paths, JarLoader.class.getClassLoader()); &#125; public JarLoader(String[] paths, ClassLoader parent) &#123; // 调用getURLS，获取所有的jar包路径 super(getURLs(paths), parent); &#125; // 获取所有的jar包 private static URL[] getURLs(String[] paths) &#123; // 获取包括子目录的所有目录路径 List&lt;String&gt; dirs = new ArrayList&lt;String&gt;(); for (String path : paths) &#123; dirs.add(path); // 获取path目录和其子目录的所有目录路径 JarLoader.collectDirs(path, dirs); &#125; // 遍历目录，获取jar包的路径 List&lt;URL&gt; urls = new ArrayList&lt;URL&gt;(); for (String path : dirs) &#123; urls.addAll(doGetURLs(path)); &#125; return urls.toArray(new URL[0]); &#125; // 递归的方式，获取所有目录 private static void collectDirs(String path, List&lt;String&gt; collector) &#123; // path为空，终止 if (null == path || StringUtils.isBlank(path)) &#123; return; &#125; // path不为目录，终止 File current = new File(path); if (!current.exists() || !current.isDirectory()) &#123; return; &#125; // 遍历完子文件，终止 for (File child : current.listFiles()) &#123; if (!child.isDirectory()) &#123; continue; &#125; collector.add(child.getAbsolutePath()); collectDirs(child.getAbsolutePath(), collector); &#125; &#125; private static List&lt;URL&gt; doGetURLs(final String path) &#123; File jarPath = new File(path); // 只寻找文件以.jar结尾的文件 FileFilter jarFilter = new FileFilter() &#123; @Override public boolean accept(File pathname) &#123; return pathname.getName().endsWith(".jar"); &#125; &#125;; File[] allJars = new File(path).listFiles(jarFilter); List&lt;URL&gt; jarURLs = new ArrayList&lt;URL&gt;(allJars.length); for (int i = 0; i &lt; allJars.length; i++) &#123; try &#123; jarURLs.add(allJars[i].toURI().toURL()); &#125; catch (Exception e) &#123; throw DataXException.asDataXException( FrameworkErrorCode.PLUGIN_INIT_ERROR, "系统加载jar包出错", e); &#125; &#125; return jarURLs; &#125;&#125; LoadUtil类LoadUtil管理着插件的加载器，调用getJarLoader返回插件对应的加载器。 12345678910111213141516171819202122232425262728293031public class LoadUtil &#123; // 加载器的HashMap, Key由插件类型和名称决定, 格式为plugin.&#123;pulginType&#125;.&#123;pluginName&#125; private static Map&lt;String, JarLoader&gt; jarLoaderCenter = new HashMap&lt;String, JarLoader&gt;(); public static synchronized JarLoader getJarLoader(PluginType pluginType, String pluginName) &#123; Configuration pluginConf = getPluginConf(pluginType, pluginName); JarLoader jarLoader = jarLoaderCenter.get(generatePluginKey(pluginType, pluginName)); if (null == jarLoader) &#123; // 构建加载器JarLoader // 获取jar所在的目录 String pluginPath = pluginConf.getString("path"); jarLoader = new JarLoader(new String[]&#123;pluginPath&#125;); //添加到HashMap中 jarLoaderCenter.put(generatePluginKey(pluginType, pluginName), jarLoader); &#125; return jarLoader; &#125; private static final String pluginTypeNameFormat = "plugin.%s.%s"; // 生成HashMpa的key值 private static String generatePluginKey(PluginType pluginType, String pluginName) &#123; return String.format(pluginTypeNameFormat, pluginType.toString(), pluginName); &#125; 当获取类加载器，就可以调用 LoadUtil 来加载插件。 1234567891011121314151617181920// 加载插件类// pluginType 代表插件类型// pluginName 代表插件名称// pluginRunType 代表着运行类型，Job或者Taskprivate static synchronized Class&lt;? extends AbstractPlugin&gt; loadPluginClass( PluginType pluginType, String pluginName, ContainerType pluginRunType) &#123; // 获取插件配置 Configuration pluginConf = getPluginConf(pluginType, pluginName); // 获取插件对应的ClassLoader JarLoader jarLoader = LoadUtil.getJarLoader(pluginType, pluginName); try &#123; // 加载插件的class return (Class&lt;? extends AbstractPlugin&gt;) jarLoader .loadClass(pluginConf.getString("class") + "$" + pluginRunType.value()); &#125; catch (Exception e) &#123; throw DataXException.asDataXException(FrameworkErrorCode.RUNTIME_ERROR, e); &#125;&#125; ClassLoaderSwapper类 123456789101112131415161718192021222324public final class ClassLoaderSwapper &#123; // 保存切换之前的加载器 private ClassLoader storeClassLoader = null; public ClassLoader setCurrentThreadClassLoader(ClassLoader classLoader) &#123; // 保存切换前的加载器 this.storeClassLoader = Thread.currentThread().getContextClassLoader(); // 切换加载器到classLoader Thread.currentThread().setContextClassLoader(classLoader); return this.storeClassLoader; &#125; public ClassLoader restoreCurrentThreadClassLoader() &#123; ClassLoader classLoader = Thread.currentThread() .getContextClassLoader(); // 切换到原来的加载器 Thread.currentThread().setContextClassLoader(this.storeClassLoader); // 返回切换之前的类加载器 return classLoader; &#125;&#125; 切换类加载器： 123456789// 实例化ClassLoaderSwapper classLoaderSwapper = ClassLoaderSwapper.newCurrentThreadClassLoaderSwapper();ClassLoader classLoader1 = new URLClassLoader();// 切换加载器classLoader1classLoaderSwapper.setCurrentThreadClassLoader(classLoader1);Class&lt;? extends MyClass&gt; myClass = classLoader1.loadClass("MyClass");// 切回加载器classLoaderSwapper.restoreCurrentThreadClassLoader(); 解决大数据引擎及版本众多问题WMRouter中对ServiceLoader的改进与使用]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[朋友只是人生过客]]></title>
    <url>%2F2020%2F03%2F28%2F%E6%9C%8B%E5%8F%8B%E5%8F%AA%E6%98%AF%E4%BA%BA%E7%94%9F%E8%BF%87%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[散了很好啊 就是人的每一分钟都在变化所以也要接受别人有变化 如果那个人跟你一日为友就终身为友你应该心里很紧张才对 就是怎么啦 我们两个都从此不变化了吗所以 如果有了变化然后人际关系跟着有了变化他（她）是你某一阶段最好的朋友然后他当完了他该当的朋友他就去当别人的朋友了就接受人生的变化是最好的态度 不要轻易去依赖一个人它会成为你的习惯当分别来临时你失去的不是某个人而是某一种精神寄托无论何时何地都要学会独立行走它会让你走的更坦然更处变不惊]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于数据中台的思考与总结]]></title>
    <url>%2F2020%2F03%2F24%2F%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E7%9A%84%E6%80%9D%E8%80%83%E4%B8%8E%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文将总结下数据中台的相关理论知识。Flink平台化需要改进的点等等。 参考：《数据中台：让数据用起来》 数据中台 数据汇聚数据汇聚是数据中台必须提供的核心工具，把各种异构网络、异构数据源的数据方便地采集到数据中台中进行集中存储，为后续的加工建模做准备。数据汇聚方式一般有数据库同步、埋点、网络爬虫、消息队列等；从汇聚的时效性来分，有离线批量汇聚和实时采集。 数据采集工具Canal、DataX、Sqoop 数据开发数据开发模块主要面向开发人员、分析人员，提供离线、实时、算法开发工具。 离线开发 作业调度 依赖调度：所有父作业运行完成后，当前作业才能开始运行。图64中的作业B，只有父作业A和C运行完成后，才能开始被调度。 时间调度：可指定作业的调度开始时间。图64中的作业B，只有到达05：00后才能开始被调度。 基线控制在大数据离线作业中，作业执行时间较长，经常遇到急着用数据发现数据还没出来的情况。采用算法对作业完成时间进行智能预测，根据预测，当作业无法正常产出且动态调整无法完成时，调度中心会及时通过监控告警通知运维值班人员提前介入处理，为大数据作业执行留出充裕的时间。 异构存储企业内部的存储计算引擎呈多元化趋势。离线开发中心针对每种类型的计算引擎会开发不同的组件，例如，针对Oracle开发Oracle插件，针对Hadoop体系分别开发出Hive、Spark、MR等插件。用户在界面新建各种作业类型，在执行时自动根据作业的类型寻找相应的插件来运行作业。 代码校验对于常见的SQL任务类型，SQL检查器会做好严格的管控，做到事前发现问题。 多环境级联通过环境级联的方式灵活支持企业的各类环境需求，方便对资源、权限进行控制和隔离。每个环境有独立的Hive数据库、Yarn调度队列，甚至不同的Hadoop集群。常见的环境如下： 单一环境：只有一个生产环境，内部管理简单。 经典环境：开发环境中存放脱敏数据、供开发测试使用，上生产环境走发布流程，用于真实数据生产。任务、资源和函数必须在开发环境下进行新建、修改或删除，再经过提交、创建发布包、同意发布三个操作后，才能同步到生产环境。 复杂环境：企业有外部人员和内部人员，会给外部人员提供一个脱敏管控的环境，外部人员开发完的数据模型经过测试后发布到内部开发环境。 推荐依赖随着业务的不断深入，数据开发人员需要开发的作业会不断累加。既能保证准确找到需要定位的上游作业，又能保证不会形成环路。获取推荐依赖的核心原理在于上下游作业输入和输出的表级血缘依赖图；通过血缘分析当前作业的输入和输出，找到合适的上游作业；对合适的作业进行环路检测，剔除存在闭环的作业；返回合适的节点列表。 数据权限企业内部计算引擎多样化，数据权限管理面临如下问题： 部分引擎拥有独立的权限管理系统（例如Oracle、HANA、LibrA），导致权限申请需要到每一种引擎上单独操作，让使用变得复杂。 同一种计算引擎，不同厂商的权限系统有多种，例如Hadoop自身无数据权限系统，由不同厂商各自去实现，目前主要有两种策略：RBAC（Role-Based Access Control）：如Cloudera用的是Sentry，华为的FI也是类似的机制PBAC（Policy-Based Access Control）：如Hortonworks用的Ranger 数据权限是由大数据集群或数据库运维人员管理的，开发人员无法直接操作或者接触，所有的权限申请都需要运维人员开通，造成运维人员负担过重。在实际开发中，一般需要运维人员把整个库的权限授权给某个开发负责人，然后库里面的表、字段、函数的权限管理由开发负责人负责就行。数据权限管理中心提供界面化操作，数据申请方直接在页面上进行各种权限的申请，数据管理方在界面上审核权限，执行同意或拒绝操作。同时，所有权限的申请、审批都会有记录，便于进行权限审计。在统一数据权限服务中，会对接底层的各种权限管理系统，例如Sentry、Ranger、Oracle，同时对数据权限管理中心提供服务，执行权限的申请、授权、撤销等操作。 实时开发 元数据管理 SQL驱动 组件化开发 智能运维任务的管理、代码发布、运维、监控、告警等一系列集成工具，方便使用，提升效率。重跑、重跑下游、补数据。 数据体系有了数据汇聚、数据开发模块，中台已经具备传统数据仓库（后面简称：数仓）平台的基本能力，可以做数据的汇聚以及各种数据开发，就可以建立企业的数据体系。之前说数据体系是中台的血肉，开发、管理、使用的都是数据。 中台数据体系应具备以下特征： 覆盖全域数据：数据集中建设、覆盖所有业务过程数据，业务中台在数据体系中总能找到需要的数据。 结构层次清晰：纵向的数据分层、横向主题域、业务过程划分，让整个层次结构清晰易理解。 数据准确一致：定义一致性指标，统一命名、统一业务含义、统一计算口径，并有专业团队负责建模，保证数据的准确一致。 性能提升：统一的规划设计，选用合理的数据模型，清晰的定义并统一规范，并且考虑使用场景，使整体性能更好。 降低成本：数据体系的建设使得数据能被业务共享，这避免了大量烟囱式的重复建设，节约了计算、存储和人力成本。 方便易用：易用的总体原则是越往后越能方便地直接使用数据，把一些复杂的处理尽可能前置，必要时做适当的冗余处理。 不同行业的数据体系建设： 地产行业 证券行业 零售行业 制造行业 传媒行业 检务行业 贴源数据层ODS对各业务系统数据进行采集、汇聚，尽可能保留原始业务流程数据，与业务系统基本保持一致，仅做简单整合、非结构化数据结构化处理或者增加标识数据日期描述信息，不做深度清洗加工。表名：ODS_系统简称_业务系统表名字段名：与业务系统字段名保持一致，字段类型也尽可能保持一致对于数据量比较大的业务表，采用增量同步的方式，则要同时建立增量表和全量表，增量表命名加后缀：ODS_系统简称_业务系统表名_delta。对于日志、文件等半结构数据，不仅要存储原始数据，还要存储结构化之后的数据。 使用DataX同步数据步骤：1）确定业务系统源表与贴源数据层目标表2）配置数据字段映射关系，目标表可能会增加采集日期、分区、原系统标识等必要信息，业务相关内容不做转换3）如果是增量同步或着有条件的同步部分数据，则配置数据同步条件4）清理目标表对应数据5）启动同步任务，往贴源数据层目标表导入数据6）验证任务是否可以正确运行，并且采集到准确数据7）发布采集任务，加入生产调度，并配置相关限速、容错、质量监控、告警机制 统一数仓层DW 明细数据层DWD 汇总数据层DWS与传统数据仓库功能基本一致，对全历史业务过程数据进行建模存储。对来源于业务系统的数据进行重新组织。业务系统是按照业务流程方便操作的方式来组织数据的，而统一数仓层从业务易理解的视角来重新组织，定义一致的指标、维度，各业务板块、业务域按照统一规范独立建设，从而形成统一规范的标准业务数据体系。 标签数据层TDM面向对象建模，对跨业务板块、跨数据域的特定对象数据进行整合，通过IDMapping把各个业务板块、各个业务过程中的同一对象的数据打通，形成对象的全域标签体系，方便深度分析、挖掘、应用。 应用数据层ADS按照业务的需要从统一数仓层、标签数据层抽取数据，并面向业务的特殊需要加工业务特定数据，以满足业务及性能需求，向特定应用组装应用数据。 数据资产管理数据资产管理包括对数据资产目录、元数据、数据质量、数据血缘、数据生命周期等进行管理和展示，以一种更直观的方式展现企业的数据资产，提升企业的数据意识。数据资产对上支持以价值挖掘和业务赋能为导向的数据应用开发，对下依托大数据平台实现数据全生命周期的管理，并对企业数据资产的价值、质量进行评估，促进企业数据资产不断自我完善，持续向业务输出动力。 数据治理传统的数据治理通常包含数据标准管理、元数据管理、数据质量管理、数据安全管理、数据生命周期管理等内容。 数据服务体系前面利用数据汇聚、数据开发建设企业的数据资产，利用数据管理展现企业的数据资产，但是并没有发挥数据的价值。数据服务体系就是把数据变为一种服务能力，通过数据服务让数据参与到业务，快速开发企业的业务中台等。 查询服务输入特定的查询条件，返回该条件下的数据，以API形式供上层应用调用。1）支持配置查询标识，底层数据组织一般会对该标识建立索引，以加快查询速度2）支持配置过滤项3）支持查询结果配置，包括数据排序规则和分页规则。 分析服务借助分析组件高效的大数据分析能力，对数据进行关联分析，分析结果通过API形式供上层应用调用。1）支持多源数据接入：企业的数据经过清洗加工转换成数据资产后，最终通过服务作用于业务系统，基于企业异构存储的现状，要求分析服务能够支持与Hive、ES、Greenplum、MySQL、Oracle、本地文件等多种数据源进行连接。2）高性能即席查询：随着企业数据爆发式增长，传统的数据分析工具遇到分析能力的瓶颈，也就是对大数据量的分析越来越乏力。因此，这就要求分析服务内置高速计算引擎，以对数据进行高性能的即席计算，实现亿级数据毫秒级（至多秒级）分析和计算，减少用户等待时间。3）多维数据分析分析服务除了支持常规的数据分析、上卷下钻、切片切块之外，还应该支持多维的数据分析以及深层次的数据挖掘，发现数据背后的关联关系。4）灵活对接业务系统 推荐服务按约定的格式提供历史日志行为数据和实时访问数据，推荐模型就会生成相应的推荐API，从而为上层应用提供推荐服务。推荐服务即所谓的千人千面，对不同的人对物的行为进行数据挖掘，构建每个人与物之间的关系程度，来推荐人、物以满足用户的兴趣爱好，以提升用户对业务的粘性。每个人打开手机淘宝看到的内容都不一样，这就是一种基于人的兴趣爱好的推荐服务能力。1）支持不同行业的推荐：不同行业背后的推荐逻辑是有区别的2）支持不同场景的推荐：以内容资讯为例，在用户冷启动场景下，应该推荐哪些资讯？在用户已有浏览行为的场景下，又该为其推荐哪些资讯？3）支持推荐效果优化：从导入的原始数据开始，经过推荐组件生成推荐数据，再根据用户的浏览数据不断修正推荐模型，从而使推荐效果不断优化 圈人服务从全量用户数据中，基于标签组合筛选符合指定特征条件的人群，并通过API形式供上层应用调用。1）支持人群圈选：通过SQL代码或标签取值组合等多种方式，实现人员查找，帮用户找到对的人群2）支持人群计量：营销部门或者广告公司使用圈人服务圈选出目标人群后，往往还要考虑人群量是否符合预期，因为预算有限，不可能不计成本的对人群进行营销。3）支持多渠道对接：将人群名单导出到相应的下游系统。最简单的名单导出方式是先下载文件，再由业务人员导入相应的业务系统中。或者直接对接到短信系统、微信投放接口、营销活动系统等。 离线平台苏宁苏宁大数据离线任务开发调度平台实践苏宁大数据离线任务开发调度平台实践：任务调度模块架构设计 苏宁离线平台产品功能图： 苏宁调度模块功能图： 苏宁离线平台整体架构图： 跨任务流依赖的实现：FTP事件机制，即在 FTP 服务器上建立标识文件，一个事件对应一个标识文件地址，当 FTP 服务器上的标识文件生成的时候，我们认为业务系统已经完成作业，需要触发平台任务执行。 “华佗”平台，实施任务诊断： 立即触发的任务，放入DelayQueue的队列头部周期调度的任务，使用Quartz依赖触发的任务，使用zk，各个子节点监听自己的父节点，所有父节点执行完毕则可触发执行 实时平台美团点评 使用了Grafana，可以内嵌到自己的平台。 bilibili SQL化编程 DAG拖拽编程 一体化托管运维 实时平台由实时传输和实时计算两部分组成，平台底层统一管理元数据、血缘、权限以及作业运维等。实时传输主要负责将数据传入到大数据体系中。实时计算基于 BSQL 提供各种应用场景支持。 如下图所示，实时传输有 APP 日志、数据库 Binlog、服务端日志或系统日志。bilibili 内部的 Lancer 系统解决数据落地到 Kafka 或 HDFS。计算体系主要围绕 Saber 构建一套 BSQL，底层基于 YARN 进行调度管理。 上层核心基于 Flink 构建运行池。再向上一层满足多种维表场景，包括 MySQL、Redis、HBase。状态（State）部分在 RocksDB 基础上，还扩展了 MapDB、Redis。Flink 需要 IO 密集是很麻烦的问题，因为 Flink 的资源调度体系内有内存和 CPU，但 IO 单位未做统一管理。当某一个作业对 IO 有强烈的需求时，需要分配很多以 CPU 或内存为单位的资源，且未必能够很好的满足 IO 的扩展。所以本质上 bilibili 现阶段是将 IO 密集的资源的 State 转移到 Redis 上做缓解。数据经过 BSQL 计算完成之后传输到实时数仓，如 Kafka、HBase、ES 或 MySQL、TiDB。最终到 AI 或 BI、报表以及日志中心。 场景 AI工程方向，解决了广告、搜索、推荐的流式Joiner和维表Joiner 实时计算的特征支持，支持 Player 以及 CDN 的质量监控。包括直播、PCU、卡顿率、CDN 质量等； 用户增长，即如何借助实时计算进行渠道分析、调整渠道投放效果； 实时 ETL，包括 Boss 实时播报、实时大屏、看板等。 网易目前网易流计算覆盖了绝大多数场景，包括广告、电商大屏、ETL、数据分析、推荐、风控、搜索、直播等。 事件管理对于分布式平台的任务操作而言，当前任务启动过程中只允许一个人操作，而不允许两个人同时操作，这就需要以下几个模块来共同配合： Server：事件执行的发起者，接受事件的请求，进行数据校验，拼装，将事件发送给 Kernel 执行。 Kernel：事件具体逻辑的执行者，根据请求向集群发送指令(Shell 脚本方式)。 Admin：事件执行结果的确认者，根据事件类型，获取事件的最终结果，保证结果的正确性。 以启动场景为例： 首先，Server 会接收到来自用户的启动请求，之后会创建一个分布式锁，Admin 会监控这个锁。 然后， Server 向 Kernel 提交任务，提交之后会立即返回，返回之后就会立即更新数据库中的状态，将状态更新为启动中，这样在页面上用户就能够看到任务是启动中的状态了。 接下来，Server 就会等待内核的 Shell 脚本的执行结果，如果 Shell 脚本执行成功了，就会去写 Zookeeper，写完 Zookeeper 之后 Admin 模块就会马上检测到 Zookeeper 节点有状态发生了修改，Admin 会立即去获取 YARN 上的任务状态，如果获取到任务状态是运行中，就将数据库的任务状态更新为运行中，这会在前端看到任务就已经是运行状态了。 最后一步是 Admin 更为完数据库之后，会释放掉 Zookeeper 上的锁，其他人这时候就可以操作这个任务了。 Server、Kernel 和 Admin 这三个模块都是不可靠的，那么如何保证其稳定和高可用呢？Server 可以通过部署多个，水平扩展来实现，Kernel 则会由 Server 来进行监听，当发现 Kernel 挂了，可以由 Server 重新拉起或者重新创建。而 Admin 的高可用则是通过热备来实现的，如果主 Admin 挂掉了，可以马上迁移到备 Admin，备 Admin 可以迅速将元数据以及任务信息全部加载进来接替工作，进而实现高可用。 平台任务状态管理平台的任务状态主要由 Server 和 Admin 来控制。Server 主要控制初始状态的执行，Admin 则主要负责控制所有与 YARN 相关的状态交互。 任务调试SQL 类型的任务支持调试功能，用户可以根据不同的 source 表和 dim 表，上传不同的 csv 文件作为输入数据，进行调试。调试执行由指定的 kernel 来完成，sloth-server 负责组装请求，调用 kernel，返回结果，搜集日志。 日志检索在 YARN 集群的每个节点上面部署 Filebeat，通过 Filebeat 将节点上面的任务日志写入到 Kafka 消息队列中，然后通过 Logstash 进行解析处理，之后写入 ES 集群中。主要用于两个用途，一个是通过界面 Kibana 来提供给开发和运维人员使用，另外一个就是将运行时状态的任务日志直接在界面上展示供用户进行搜索和查看。 监控在监控方面，使用的是 influxdb metric report 组件对于指标进行监控。时序数据库使用的是网易自研的 ntsdb 时序数据库，其能够支持动态扩展和高可用等功能。监控指标的使用方式有两种：一种是通过 Grafana 的界面来查看指标；另外一种是报警模块会从Ntsdb中获取相关指标数据并进行监控报警。 报警Sloth 流计算平台支持常见的任务失败，数据滞留延迟，failover 报警，也支持用户自定义规则报警，包括对于输入 QPS、输出 QPS，户自定义延迟的监控等。以输入 QPS 为例，可以设置当连续几个周期内 QPS 低于某一值时就触发报警。此外，报警方式也支持多样化的工具，比如各种网易内部的聊天工具、邮件、电话以及短信等，对于任务调试阶段，为了避免被骚扰，可以设置任务报警抑制时间间隔。 实时数仓目前网易很多产品已经开始实时数仓的建设了，但仍旧处于持续完善过程中。实时数仓的建设和离线数仓大致相同，只不过实时数仓是经过实时计算平台进行处理的。大致的过程就是首先收集日志、埋点数据等，将其写入到 Kafka 里面，经过实时计算平台进行处理，将 ODS 层中的明细数据抽取出来，在进行汇总以及维度关联等操作，将结果写入到 Redis，Kudu 等，再通过数据服务提供给前端的业务使用。x 电商应用-数据分析实时活动分析、首页资源分析、流量漏斗以及实时毛利计算等。 电商应用-搜索推荐电商的搜索推荐场景则主要包括用户实时足迹、用户实时特征、商品实时特征、实时 CTR CVR 样本组建、首页 A 区轮播、B 区活动精选等 UV、PV 实时统计等。网络营销中的常见名词解释： 12345678CPC (Cost Per Click): 按点击计费CPA (Cost Per Action): 按成果数计费CPM (Cost Per Mille): 按千次展现计费CVR (Click Value Rate): 转化率，衡量CPA广告效果的指标CTR (Click Through Rate): 点击率PV (Page View): 流量ADPV (Advertisement Page View): 载有广告的pageview流量ADimp (ADimpression): 单个广告的展示次数PV单价: 每PV的收入，衡量页面流量变现能力的指标 离线数仓与实时数仓从0建设离线数仓建设数仓数据仓库定义：在企业管理和决策中面向主题的、集成的、与时间相关的、不可修改的数据集合。数据仓库目标：数据资产、决策信息。 ETL过程：打通你的任督二脉（离线+实时），让数据在整个环节中流通起来 数据分层：一套低耦合、高内聚的层级，是十分重要的，总不想业务、数据等一变化，数仓像又投胎了一次 数据集成：多业务场景下，打破数据信息壁垒，避免数据歧义，统一数据服务 规范化：良好的流程化、规范化设计，易维护、高扩展 监控与辅助：质量监控、调度管理、元数据管理、信息安全管理 走向服务：对外api服务/自助查询平台/OLAP分析平台 ETL业务数据往往涉及多种数据源，数据存储也常常会有多种选择。文本数据、日志数据、RMDB、Nosql等。则要求etl工具能够覆盖这些业务场景。工具有datax/sqoop/kettle/informatica等等。ETL一般为最开始的部分，凌晨之后的时间点。a：避免集中式的对某个jdbc海量同步，影响业务(部分从库可能提供查询服务)、b：明确调度的时间，应尽可能的在某个时间段内完成(不能仅依靠调度，实现任务流的串行；为后期的大作业空间，占用等待的系统资源) 分层 Stage缓冲层事务性数据，每日增量方式进行数据同步。需要注意数据同步时的边界问题，避免脏数据。对于非事务性数据，一般通过快照/全量更新。不对外开放数据查询。 ods层一般场景下，我们认为该层数据与线上保持一致。实际处理过程中，为了处理时间维度上的数据变化，会记录数据的变化轨迹。对于该部分数据，应该有选择的实施，避免业务处理过程变得复杂和问题发生后难以回溯。 dim/dw层 (模型层)dim：维度层dw：主题事实及业务宽表在ods基础上，设计一个宽表/模型层，通过维度建模的方式，实现维度数据与事实数据的分离（星型模型）。 da层（应用层）面向不同的应用，聚合类的数据层。该层对于dim/dw层的使用，是对模型层的一个检视维度。 代码规范 脚本格式规范：脚本头部注释编码规范、注释规范、sql规范参考goole规范 文件/表命名规范：一个文件中，只应该有一张表，其余只能是临时表；表名称应与文件名相同 字段命名规范：去除多词同义，和同词多义的问题。尤其是在模型层（一般也叫做一致性维度） 区别离线数仓主要基于sqoop、datax、hive等技术来构建 T+1 的离线数据，通过定时任务每天垃取增量数据导入到hive表中，然后创建各个业务相关的主题，对外提供T+1的数据查询接口。实时数仓主要是基于数据采集工具，如canal等原始数据写入到kafka这样的数据通道中，最后一般都是写入到类似于HBase这样的OLAP存储系统中。对外提供分钟级别，甚至秒级别的查询方案。 数仓类型 准确性 实时性 稳定性 离线数仓 准确度高 时延一般在一天 稳定性好，方便重算 实时数仓 准确度低，数据延迟、数据乱序造成数据准确度低 分钟级延迟 稳定性差，需要考虑数据回溯处理 数据仓库的建设主要包括数据的采集、数据的处理、数据归档、数据应用四个方面。当前主要的应用场景包括报表展示、即席查询、BI展示、数据分析、数据挖掘、模型训练等方面。数据仓库的建设是面向主题的、集成性的、不可更新的、时许变化的。 实时数仓的实施关键点： 端到端数据延迟、数据流量的监控 故障的快速恢复能力 数据的回溯处理，系统支持消费指定时间段内的数据 实时数据从实时数仓中查询，T+1数据借助离线通道修正 数据地图、数据血缘关系的梳理 业务数据质量的实时监控，初期可以根据规则的方式来识别质量状况 其实，你需要的不是实时数仓，需要的是一款合适且强大的OLAP数据库。在实时数仓的建设中，OLAP数据库的选型直接制约实时数仓的可用性和功能性。 原始层明细层汇总层应用层 ods：原始数据层，事实数据，存储在kafka中dwd：数据明细层，可以做一些join等加宽处理，可以存储在kafka和redis中dim：维度数据，如存储在HBase中的数据dm：MySQL -&gt; 汇总指标模型；Greenplum -&gt; 明细，多维分析关联；HBase -&gt; 汇总指标(大量并发)；Redis -&gt; 汇总、大列表TopN 数据中台解决方案零售行业 RPS (Revenue Per Search): 每搜索产生的收入，衡量搜索结果变现能力指标 ROI： 投资回报率（ROI）是指通过投资而应返回的价值，它涵盖了企业的获利目标。利润和投入的经营所必备的财产相关，因为管理人员必须通过投资和现有财产获得利润。又称会计收益率、投资利润率。]]></content>
      <categories>
        <category>思考总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据结构和算法]]></title>
    <url>%2F2020%2F03%2F23%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文将介绍常见的数据结构，算法思想，排序算法、树等。 经典机试题(剑指offer)二维数组中的查找 题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 思路通过分析可以很简单的找出一个规律，二维数组的最左下角的的点，该点的所在列上边的点都是减少的，该点所在行右边的点都是增加的。因此，我们以该点作为切入点，如果目标数比左下角的数大，则往右边移动；如果目标数比左下角的数小，则往上边移动；之后以此类推，如果匹配到目标数，则返回true；如果当移动到最右上角的点仍然没有匹配到目标数，则返回false。 代码示例 12345678910111213141516171819202122public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int rows = array.length; int cols = array[0].length; int row; int col; for(row = rows -1,col=0; row&gt;=0 &amp;&amp; col&lt;=cols-1;)&#123; if(target == array[row][col])&#123; return true; &#125; else if(target &lt; array[row][col])&#123; row--; continue; &#125; else if(target &gt; array[row][col])&#123; col++; continue; &#125; &#125; return false; &#125;&#125; 替换空格 题目描述请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 代码示例 1234567891011121314151617public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; if(str == null)&#123; return null; &#125; for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i); if(c==' ')&#123; str.replace(i,i+1,"%20"); &#125; &#125; String newStr = str.toString(); return newStr; &#125;&#125; 从尾到头打印链表 题目描述输入一个链表，按链表从尾到头的顺序返回一个ArrayList。 代码示例 123456789101112131415161718192021222324/*** public class ListNode &#123;* int val;* ListNode next = null;** ListNode(int val) &#123;* this.val = val;* &#125;* &#125;**/import java.util.ArrayList;public class Solution &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; if(listNode != null)&#123; // 递归调用 printListFromTailToHead(listNode.next); list.add(listNode.val); &#125; return list; &#125;&#125; 重建二叉树 题目描述输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 思路通过分析前序遍历和中序遍历的规律，前序遍历的第一个节点就是二叉树的根节点，中序遍历中，位于根节点前面的所有节点都位于左子树上，位于根节点后面的所有节点都位于右子树上面。通过这个规律，我们可以使用递归方法来重建二叉树。 代码示例 123456789101112131415161718192021222324252627282930313233343536/** * Definition for binary tree * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; if(pre == null || in == null || pre.length == 0 || in.length == 0)&#123; return null; &#125; return buildTree(pre,in,0,pre.length-1,0,in.length-1); &#125; public TreeNode buildTree(int[] pre,int[] in,int preStart,int preEnd,int inStart,int inEnd)&#123; TreeNode root = new TreeNode(pre[preStart]); int rootIn = 0; for(; rootIn&lt;in.length; rootIn++)&#123; if(in[rootIn] == root.val)&#123; break; &#125; &#125; int leftLength = rootIn - inStart; int rightLength = inEnd - rootIn; if(leftLength &gt; 0)&#123; root.left = buildTree(pre, in, preStart+1, preStart+leftLength, inStart, rootIn -1); &#125; if(rightLength &gt; 0)&#123; root.right = buildTree(pre, in, preStart+leftLength+1, preEnd, rootIn+1, inEnd); &#125; return root; &#125;&#125; 用两个栈实现队列 题目描述用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 思路入队：将元素进栈A出队：判断栈B是否为空，如果为空，则将栈A中所有元素pop，并push进栈B，栈B出栈；如果不为空，栈B直接出栈。 代码示例 12345678910111213141516171819import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; public int pop() &#123; if(stack2.isEmpty())&#123; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; &#125; return stack2.pop(); &#125;&#125; 旋转数组的最小数字 题目描述把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 思路如果是完全正序的{1,2,3,4,5}，那么遍历到倒数第二个，应该能确定这个数组没有被旋转过，可以确定数组中的第一个为最小值。如果是经过旋转的数组{3,4,5,1,2}，那么发现乱序的第一个值就应该是最小值。 代码示例 1234567891011121314151617181920212223242526import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; if(array.length==0)&#123; return 0; &#125; if(array.length==1)&#123; return array[0]; &#125; for(int i=0;i&lt;array.length;i++)&#123; if(array[i+1]&lt;array[i])&#123; // 数组经过旋转之后 return array[i+1]; &#125; else &#123; if(i == array.length -2)&#123; // 完全正序递增的数组 return array[0]; &#125; &#125; &#125; return 0; &#125; &#125; 斐波那契数列 题目描述大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39 思路这里可以使用递归的方法实现，但是递归的方式的时间复杂是输入规模的指数级别，不建议使用，因此这里采用迭代的方法实现。 代码示例1234567891011121314151617181920212223public class Solution &#123; public int Fibonacci(int n) &#123; if(n &lt;= 0)&#123; return 0; &#125; int first = 0; int second = 1; int result = 1; if(n == 1)&#123; return n; &#125; for(int i=2; i&lt;=n; i++)&#123; result = first + second; first = second; second = result; &#125; return result; &#125;&#125; 跳台阶 题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 思路（1）可以考虑，小青蛙每一步跳跃只有两种选择：一是再跳一级阶梯到达第 i 级阶梯，此时小青蛙处于第 i-1 级阶梯；或者再跳两级阶梯到达第 i 级阶梯，此时小青蛙处于第 i-2 级阶梯。 （2）于是，i级阶梯的跳法总和f(i)依赖于前 i-1 级阶梯的跳法总数f(i-1)和前 i-2 级阶梯的跳法总数f(i-2)。因为只有两种可能性，所以，f(i)=f(i-1)+f(i-2); （3）通过上面的分析，我们可以很清晰地看到，这其实就是一个Fibonacci数列。 代码示例123456789101112131415161718192021/** * f(i)=f(i-1)+f(i-2) * 其实就是一个斐波那契数列 */public class Solution &#123; public int JumpFloor(int target) &#123; if(target&lt;1)&#123; return 0; &#125;else if(target&lt;2)&#123; return target; &#125;else&#123; int first=0,second=1,result=1; for(int i=2;i&lt;=target;i++)&#123; first=second; second=result; result=first+second; &#125; return result; &#125; &#125;&#125; 变态跳台阶 题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 思路用Fib(n)表示跳上n阶台阶的跳法数。如果按照定义，Fib(0)肯定需要为0，否则没有意义。但是我们设定Fib(0) = 1；n = 0是特殊情况，通过下面的分析就会知道，强制令Fib(0) = 1很有好处。ps. Fib(0)等于几都不影响我们解题，但是会影响我们下面的分析理解。 当n = 1 时， 只有一种跳法，即1阶跳：Fib(1) = 1; 当n = 2 时， 有两种跳的方式，一阶跳和二阶跳：Fib(2) = 2; 到这里为止，和普通跳台阶是一样的。 当n = 3 时，有三种跳的方式，第一次跳出一阶后，对应Fib(3-1)种跳法； 第一次跳出二阶后，对应Fib(3-2)种跳法；第一次跳出三阶后，只有这一种跳法。Fib(3) = Fib(2) + Fib(1)+ 1 = Fib(2) + Fib(1) + Fib(0) = 4; 当n = 4时，有四种方式：第一次跳出一阶，对应Fib(4-1)种跳法；第一次跳出二阶，对应Fib(4-2)种跳法；第一次跳出三阶，对应Fib(4-3)种跳法；第一次跳出四阶，只有这一种跳法。所以，Fib(4) = Fib(4-1) + Fib(4-2) + Fib(4-3) + 1 = Fib(4-1) + Fib(4-2) + Fib(4-3) + Fib(4-4) 种跳法。 当n = n 时，共有n种跳的方式，第一次跳出一阶后，后面还有Fib(n-1)中跳法； 第一次跳出二阶后，后面还有Fib(n-2)中跳法……………………..第一次跳出n阶后，后面还有 Fib(n-n)中跳法。Fib(n) = Fib(n-1)+Fib(n-2)+Fib(n-3)+……….+Fib(n-n) = Fib(0)+Fib(1)+Fib(2)+…….+Fib(n-1)。 通过上述分析，我们就得到了通项公式： Fib(n) = Fib(0)+Fib(1)+Fib(2)+.......+ Fib(n-2) + Fib(n-1)因此，有 Fib(n-1)=Fib(0)+Fib(1)+Fib(2)+…….+Fib(n-2) 两式相减得：Fib(n)-Fib(n-1) = Fib(n-1) =====》 Fib(n) = 2*Fib(n-1) n &gt;= 3 这就是我们需要的递推公式：Fib(n) = 2*Fib(n-1) n &gt;= 3 代码示例12345678910111213141516171819202122232425/** * Fib(1) = 1; * Fib(2) = 2; * 递推公式：Fib(n) = 2*Fib(n-1) n &gt;= 3 */public class Solution &#123; public int JumpFloorII(int target) &#123; if(target &lt; 0)&#123; return 0; &#125; else if(target &lt; 3)&#123; return target; &#125; else &#123; Integer[] array = new Integer[target+1]; array[0] = 1; array[1] = 1; array[2] = 2; for(int n=3; n &lt;=target; n++)&#123; array[n] = 2*array[n-1]; &#125; return array[target]; &#125; &#125;&#125; 矩阵覆盖 题目描述我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？比如n=3时，2*3的矩形块有3种覆盖方法。 思路f(n) = f(n-1) + f(n-2) 代码示例 1234567891011121314151617public class Solution &#123; public int RectCover(int target) &#123; if( target&lt;3 )&#123; return target; &#125; else &#123; int first = 1; int second = 2; int result = 0; for(int n=3; n&lt;=target; n++)&#123; result = first + second; first = second; second = result; &#125; return result; &#125; &#125;&#125; 二进制中1的个数 题目描述输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 思路使用java内置的的toBinaryString方法来实现。 代码示例 12345678910111213public class Solution &#123; public int NumberOf1(int n) &#123; int count = 0; String str = Integer.toBinaryString(n); for(int i=0; i&lt;str.length(); i++)&#123; char c = str.charAt(i); if(c == '1')&#123; count++; &#125; &#125; return count; &#125;&#125; 数值的整数次方 题目描述给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。保证base和exponent不同时为0 代码示例 12345678910111213141516171819202122public class Solution &#123; public double Power(double base, int exponent) &#123; double result = 1.0; if(exponent == 0)&#123; return 1; &#125; else if(exponent&gt;0)&#123; for(int i=0;i&lt;exponent;i++)&#123; result *= base; &#125; &#125; else &#123; if(base == 0)&#123; throw new RuntimeException("分母不能为0"); &#125; for(int j=0;j&lt;-exponent;j++)&#123; result *= base; &#125; &#125; return exponent&gt;0 ? result:1/result; &#125;&#125; 调整数组顺序使奇数位于偶数前面 题目描述输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 思路 类似冒泡排序的遍历方式，前面是偶数，后面是奇数，就交换位置 用空间换时间的做法，new一个数组，从头开始遍历，遇到偶数保存进新数组，并且删除原先的偶数，最后将偶数部分接在奇数部分之后。 类似快排的思想，使用双指针，i从左到右寻找偶数，j从右到左寻找奇数，然后交换两个数。(但是第3种方法不能保证相对顺序) 代码示例1 - 冒泡排序思想1234567891011121314public class Solution &#123; public void reOrderArray(int [] array) &#123; for(int i=0; i&lt;array.length-1; i++)&#123; for(int j=0; j&lt;array.length-1-i; j++)&#123; // 前面是偶数，后面是奇数，就交换位置 if(array[j]%2==0 &amp;&amp; array[j+1]%2==1)&#123; int temp = array[j]; array[j] = array[j+1]; array[j+1] = temp; &#125; &#125; &#125; &#125;&#125; 代码示例2 - 快速排序思想123456789101112131415161718192021222324252627282930public void fun(int[] array)&#123; int length = array.length; if(length==0)&#123; return; &#125; int front=0; int end = length-1; while(front&lt;end)&#123; // 向后找偶数 while(front&lt;length &amp;&amp; array[front]%2==0)&#123; front++; &#125; // 向前找奇数 while(end&gt;=0 &amp;&amp; array[end]%2==1)&#123; end--; &#125; if(front&lt;end)&#123; // 交换偶数和奇数 int temp = array[front]; array[front] = array[end]; array[end] = temp; &#125; &#125; &#125; 链表中倒数第k个节点 题目描述输入一个链表，输出该链表中倒数第k个结点。 思路先遍历一遍链表，看链表有多少个节点。下一次直接遍历到count-k处即可。 代码示例 123456789101112131415161718192021222324252627282930/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindKthToTail(ListNode head,int k) &#123; if(head == null)&#123; return null; &#125; ListNode node = head; int count = 0; while(node != null)&#123; count ++; node = node.next; &#125; if(count &lt; k)&#123; return null; &#125; ListNode p = head; for(int i=0; i&lt;count-k; i++)&#123; p = p.next; &#125; return p; &#125;&#125; 反转链表 题目描述输入一个链表，反转链表后，输出新链表的表头。 思路 代码示例 123456789101112131415161718192021222324252627282930/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode ReverseList(ListNode head) &#123; if(head==null)&#123; return null; &#125; ListNode newHead = null; ListNode pNode = head; ListNode pPrev = null; while(pNode!=null)&#123; ListNode pNext = pNode.next; if(pNext==null)&#123; newHead = pNode; &#125; pNode.next = pPrev; pPrev = pNode; pNode = pNext; &#125; return newHead; &#125;&#125; 合并两个排序的链表 题目描述输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 思路比较两个链表的第一个节点，取出最小值的节点，接着再按照相同的方式重复比较剩余链表的节点。 代码示例 12345678910111213141516171819202122232425262728/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; if(list1==null)&#123; return list2; &#125; else if(list2==null)&#123; return list1; &#125; ListNode pMergeHead = null; if(list1.val&lt;list2.val)&#123; pMergeHead = list1; pMergeHead.next = Merge(list1.next,list2); &#125; else &#123; pMergeHead = list2; pMergeHead.next = Merge(list2.next,list1); &#125; return pMergeHead; &#125;&#125; 树的子结构 题目描述输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 思路这个题比较简单，利用递归的方式就可以判断B是不是A树的子结构。 代码示例 12345678910111213141516171819202122232425262728293031323334353637383940/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public boolean HasSubtree(TreeNode root1,TreeNode root2) &#123; if(root1==null || root2==null)&#123; return false; &#125; return (isSubTree(root1,root2) || HasSubtree(root1.left,root2) || HasSubtree(root1.right,root2)); &#125; public boolean isSubTree(TreeNode root1,TreeNode root2)&#123; // 如果tree2已经遍历完了，说明可以完全匹配上 if(root2 == null)&#123; return true; &#125; // tree2还没有遍历完，但是tree1已经遍历完了，说明匹配不上 if(root1==null)&#123; return false; &#125; // 如果根节点的值能匹配上，就分别去比较左右子节点 if(root1.val == root2.val)&#123; return (isSubTree(root1.left,root2.left)&amp;&amp;isSubTree(root1.right,root2.right)); &#125;else&#123; return false; &#125; &#125;&#125; 二叉树的镜像 题目描述操作给定的二叉树，将其变换为源二叉树的镜像。 二叉树的镜像定义：源二叉树 1234567891011 8 / \ 6 10 / \ / \5 7 9 11镜像二叉树 8 / \ 10 6 / \ / \11 9 7 5 思路采用递归的方式，递归交换每一个父节点的两个子节点的位置。 代码示例12345678910111213141516171819202122232425262728293031/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public void Mirror(TreeNode root) &#123; if(root==null)&#123; return; &#125; if(root.left==null &amp;&amp; root.right==null)&#123; return; &#125; // 交换左右子树 TreeNode temp = root.left; root.left = root.right; root.right = temp; Mirror(root.left); Mirror(root.right); &#125;&#125; 顺时针打印矩阵 题目描述输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 思路待详细看看 代码示例 12345678910111213141516171819202122232425262728293031323334353637import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printMatrix(int [][] matrix) &#123; if(matrix==null)&#123; return null; &#125; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt; (); int row=matrix.length; int col=matrix[0].length; int left=0,top=0,right=col-1,bottom=row-1; while(left&lt;=right&amp;&amp;top&lt;=bottom)&#123; //从左向右 for(int i=left;i&lt;=right;i++)&#123; list.add(matrix[top][i]); &#125; //从上到下（从下一行开始向下走） for(int j=top+1;j&lt;=bottom;j++)&#123; list.add(matrix[j][right]); &#125; //从右到左 if(top!=bottom)&#123; for(int k=right-1;k&gt;=left;k--)&#123; list.add(matrix[bottom][k]); &#125; &#125; //从下到上 if(left!=right)&#123; for(int l=bottom-1;l&gt;top;l--)&#123; list.add(matrix[l][left]); &#125; &#125; //下一个正方形矩阵 top++;left++;right--;bottom--; &#125; return list; &#125;&#125; 包含min函数的栈 题目描述定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。注意：保证测试中不会当栈为空的时候，对栈调用pop()或者min()或者top()方法。 思路每次入栈2个元素，一个是入栈的元素本身，一个是当前栈元素的最小值。 如：入栈序列为2-3-1，则入栈后栈中元素序列为：2-2-3-2-1-1 * 用空间代价来换取时间代价。 代码示例 12345678910111213141516171819202122232425262728293031323334import java.util.Stack;public class Solution &#123; private Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); public void push(int node) &#123; if(stack.isEmpty())&#123; stack.push(node); stack.push(node); &#125; else &#123; int temp = stack.peek(); stack.push(node); if(temp&lt;node)&#123; stack.push(temp); &#125;else&#123; stack.push(node); &#125; &#125; &#125; public void pop() &#123; stack.pop(); stack.pop(); &#125; public int top() &#123; return stack.get(stack.size()-2); &#125; public int min() &#123; return stack.peek(); &#125;&#125; 栈的压入_弹出序列 题目描述输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 思路借用一个辅助的栈，遍历压栈顺序，先将第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。 举例：入栈1,2,3,4,5出栈4,5,3,2,1首先1入辅助栈，此时栈顶1≠4，继续入栈2此时栈顶2≠4，继续入栈3此时栈顶3≠4，继续入栈4此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3此时栈顶3≠5，继续入栈5此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3….依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 代码示例123456789101112131415161718192021import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; if(pushA.length == 0 || popA.length == 0)&#123; return false; &#125; Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); int popIndex = 0; for(int i=0;i&lt;pushA.length;i++)&#123; stack.push(pushA[i]); while(!stack.isEmpty() &amp;&amp; stack.peek()==popA[popIndex])&#123; // 弹出出栈数据 stack.pop(); popIndex++; &#125; &#125; return stack.isEmpty(); &#125;&#125; 从上往下打印二叉树 题目描述从上往下打印出二叉树的每个节点，同层节点从左至右打印。 思路用arraylist模拟一个队列来存储相应的TreeNode。 代码示例 123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); if(root==null)&#123; return list; &#125; ArrayList&lt;TreeNode&gt; queue = new ArrayList&lt;&gt;(); queue.add(root); while(!queue.isEmpty())&#123; TreeNode node = queue.remove(0); list.add(node.val); if(node.left != null)&#123; queue.add(node.left); &#125; if(node.right != null)&#123; queue.add(node.right); &#125; &#125; return list; &#125;&#125; 二叉搜索树的后序遍历序列 题目描述输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 思路已知条件：后序序列最后一个值为root；二叉搜索树左子树值都比root小，右子树值都比root大。（1）确定root；（2）遍历序列（除去root结点），找到第一个大于root的位置，则该位置左边为左子树，右边为右子树；（3）遍历右子树，若发现有小于root的值，则直接返回false；（4）分别判断左子树和右子树是否仍是二叉搜索树（即递归步骤1、2、3）。 代码示例 123456789101112131415161718192021222324252627282930public class Solution &#123; public boolean VerifySquenceOfBST(int [] sequence) &#123; if(sequence.length == 0)&#123; return false; &#125; if(sequence.length == 1)&#123; return true; &#125; return judge(sequence,0,sequence.length-1); &#125; private boolean judge(int[] sequence,int startIndex,int rootIndex)&#123; if(startIndex &gt; rootIndex)&#123; return true; &#125; int i = startIndex; while(sequence[i]&lt;sequence[rootIndex])&#123; i++; &#125; // 判断右子树中的值，不能比根节点的值小 for(int j=i;j&lt;rootIndex;j++)&#123; if(sequence[j]&lt;sequence[rootIndex])&#123; return false; &#125; &#125; // 分别判断左右子树是否满足平衡二叉搜索树的后序遍历 return judge(sequence,startIndex,i-1)&amp;&amp;judge(sequence,i+1,rootIndex); &#125;&#125; 二叉树中和为某一值的路径 题目描述输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) 思路（1）root入栈，跳入该子树进行寻路操作 ；（2）若root的这条路径，已满足要求，则将该路径加入到listAll中去；（3）对root左右子树，继续寻路；（4）root出栈，该子树访问完毕； 代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.util.*;public class Solution &#123; private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; resultList = new ArrayList&lt;&gt;(); private ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) &#123; // 查找出所有满足条件的路径 find(root,target); // 根据resultList里每个list的大小进行排序 Collections.sort(resultList,new Comparator&lt;ArrayList&lt;Integer&gt;&gt;()&#123; @Override public int compare(ArrayList&lt;Integer&gt; o1,ArrayList&lt;Integer&gt; o2)&#123; return o2.size() - o1.size(); &#125; &#125;); return resultList; &#125; public void find(TreeNode root,int target)&#123; if(root == null)&#123; return; &#125; list.add(root.val); target = target - root.val; // 遍历到叶子节点，并且满足条件，加入到resultList中 if(target==0 &amp;&amp; root.left==null &amp;&amp; root.right==null)&#123; resultList.add(new ArrayList&lt;Integer&gt;(list)); &#125; // 还没有遍历到叶子节点就提前结束了,终止遍历 if(target&lt;0)&#123; list.remove(list.size()-1); return; &#125; // 继续探寻左右子树 find(root.left,target); find(root.right,target); // 删除本次添加到list中的元素，为下一条路径作准备 list.remove(list.size()-1); &#125;&#125;/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/ 复杂链表的复制 题目描述输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 思路首先复制出头节点，再向后循环复制。 代码示例 12345678910111213141516171819202122232425262728293031323334/*public class RandomListNode &#123; int label; RandomListNode next = null; RandomListNode random = null; RandomListNode(int label) &#123; this.label = label; &#125;&#125;*/public class Solution &#123; public RandomListNode Clone(RandomListNode pHead)&#123; if(pHead==null)&#123; return null; &#125; // 先复制头节点 RandomListNode head = new RandomListNode(pHead.label); RandomListNode ans = head; if(pHead.random!=null)&#123; head.random = new RandomListNode(pHead.random.label); &#125; while(pHead.next!=null)&#123; pHead = pHead.next; head.next = new RandomListNode(pHead.label); if(pHead.random!=null)&#123; head.next.random = new RandomListNode(pHead.random.label); &#125; head = head.next; &#125; return ans; &#125;&#125; 二叉搜索树与双向链表 题目描述输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 思路递归实现，中序遍历。每次用一个lastNode存放上一个子树的最后一个结点，该结点是上个已排好子树的最大的结点。因此此时对于结点10来说，只需要建立与8之间的联系。 代码示例 12345678910111213141516171819202122232425262728293031323334353637/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; TreeNode lastNode=null; public TreeNode Convert(TreeNode pRootOfTree) &#123; TreeNode p = pRootOfTree; middleFind(p); while(lastNode!=null &amp;&amp; lastNode.left!=null)&#123; lastNode = lastNode.left; &#125; return lastNode; &#125; public void middleFind(TreeNode p)&#123; if(p==null)&#123; return; &#125; middleFind(p.left); p.left = lastNode; if(lastNode!=null)&#123; lastNode.right = p; &#125; lastNode = p; middleFind(p.right); &#125;&#125; 字符串的全排列 题目描述题目描述输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。输入描述:输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。 思路 代码示例 12345678910111213141516171819202122232425262728293031323334import java.util.*;public class Solution &#123; private ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;String&gt; Permutation(String str) &#123; if(str==null || str=="")&#123; return list; &#125; Permutation(str.toCharArray(),0); // 排序 Collections.sort(list); return list; &#125; public void Permutation(char[] chars,int pos)&#123; if(pos == chars.length-1)&#123; String str = String.valueOf(chars); if(!list.contains(str))&#123; list.add(str); &#125; &#125;else&#123; for(int i=pos;i&lt;chars.length;i++)&#123; char temp = chars[i]; chars[i] = chars[pos]; chars[pos] = temp; Permutation(chars,pos+1); // 回溯 temp = chars[i]; chars[i] = chars[pos]; chars[pos] = temp; &#125; &#125; &#125;&#125; 数组中出现次数超过一半的数字 题目描述数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 思路 代码示例 1234567891011121314151617181920212223242526272829303132public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; int length = array.length; if(array == null || length&lt;0 )&#123; return 0; &#125; int result = array[0]; int times = 1; for(int i=1;i&lt;length;i++)&#123; if(times==0)&#123; result = array[i]; times = 1; &#125; else &#123; if(result==array[i])&#123; times++; // 如果记录的值与统计值相等，记数值增加 &#125;else&#123; times--; // 如果不相同就减少，相互抵消 &#125; &#125; &#125; times = 0; for(int i=0;i&lt;length;i++)&#123; if(result==array[i])&#123; times++; &#125; &#125; if(times*2&lt;=length)&#123; return 0; &#125; return result; &#125;&#125; 最小的k个数 题目描述输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 思路使用冒泡排序k趟即可，每趟都可以确定一个最小值。 代码示例 123456789101112131415161718192021import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int [] input, int k) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); if(k==0 || k&gt;input.length)&#123; return list; &#125; // 使用冒泡排序k趟即可，每趟都可以确定一个最小值 for(int i=0;i&lt;k;i++)&#123; for(int j=0;j&lt;input.length-1-i;j++)&#123; if(input[j+1]&gt;input[j])&#123; int temp = input[j+1]; input[j+1] = input[j]; input[j] = temp; &#125; &#125; list.add(input[input.length-i-1]); &#125; return list; &#125;&#125; 连续子数组的最大和 题目描述HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 思路可以用动态规划的思想来分析这个问题。如果用函数 f(i)表示以第 i 个数字结尾的子数组的最大和，那么我们需要求出 max[f(i)]，其中 0 &lt;= i &lt; n。我们可用如下边归公式求 f(i): 这个公式的意义：当以第 i-1 个数字结尾的子数组中所有数字的和小于 0 时，如果把这个负数与第 i 个数累加，得到的结果比第 i 个数字本身还要小，所以这种情况下以第 i 个数字结尾的子数组就是第 i 个数字本身。如果以第 i-1 个数字结尾的子数组中所有数字的和大于 0，与第 i 个数字累加就得到以第 i 个数字结尾的子数组中所有数字的和。 本题采用第一种实现方式。 代码示例123456789101112131415161718192021222324public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; if(array==null || array.length&lt;0)&#123; return 0; &#125; // 记录最大的子数组和，初始化为int的最小值，可能为负数 int max = Integer.MIN_VALUE; // 当前的和 int curMax =0; for(int i=0;i&lt;array.length;i++)&#123; if(curMax&lt;=0)&#123; // 如果当前和小于等于0，就重新设置当前和 curMax=array[i]; &#125;else&#123; // 如果当前和大于0，累加当前和 curMax+=array[i]; &#125; if(max&lt;curMax)&#123; max=curMax; &#125; &#125; return max; &#125;&#125; 1到n的整数中1出现的次数 题目描述求出113的整数中1出现的次数,并算出1001300的整数中1出现的次数？为此他特别数了一下1-13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数） 思路遍历n个整数，把每一个整数都转成字符串，再将这个字符串转成char数组。 代码示例 12345678910111213141516public class Solution &#123; public int NumberOf1Between1AndN_Solution(int n) &#123; int count=0; while(n&gt;0)&#123; String str = String.valueOf(n); char[] chars = str.toCharArray(); for(int i=0;i&lt;chars.length;i++)&#123; if(chars[i]=='1')&#123; count++; &#125; &#125; n--; &#125; return count; &#125;&#125; 把数组排成最小的数 题目描述输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 思路重写Comparator的compare()方法。 代码示例12345678910111213141516171819202122232425import java.util.*;public class Solution &#123; public String PrintMinNumber(int [] numbers) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for(int i=0;i&lt;numbers.length;i++)&#123; list.add(numbers[i]); &#125; Collections.sort(list,new Comparator&lt;Integer&gt;()&#123; @Override public int compare(Integer o1,Integer o2)&#123; String str1 = o1+""+o2; String str2 = o2+""+o1; return str1.compareTo(str2); &#125; &#125;); StringBuilder builder = new StringBuilder(); for(int i=0;i&lt;list.size();i++)&#123; builder.append(list.get(i)); &#125; return builder.toString(); &#125;&#125; 丑数 题目描述把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 思路首先从丑数的定义我们知道，一个丑数的因子只有2,3,5，那么丑数p = 2 ^ x * 3 ^ y * 5 ^ z，换句话说一个丑数一定由另一个丑数乘以2或者乘以3或者乘以5得到，那么我们从1开始乘以2,3,5，就得到2,3,5三个丑数，在从这三个丑数出发乘以2,3,5就得到4，6,10,6，9,15,10,15,25九个丑数，我们发现这种方法会得到重复的丑数，而且我们题目要求第N个丑数，这样的方法得到的丑数也是无序的。那么我们可以维护三个队列： （1）丑数数组： 1 乘以2的队列：2 乘以3的队列：3 乘以5的队列：5 选择三个队列头最小的数2加入丑数数组，同时将该最小的数乘以2,3,5放入三个队列； （2）丑数数组：1,2 乘以2的队列：4 乘以3的队列：3，6 乘以5的队列：5，10 选择三个队列头最小的数3加入丑数数组，同时将该最小的数乘以2,3,5放入三个队列； （3）丑数数组：1,2,3 乘以2的队列：4,6 乘以3的队列：6,9 乘以5的队列：5,10,15 选择三个队列头里最小的数4加入丑数数组，同时将该最小的数乘以2,3,5放入三个队列； （4）丑数数组：1,2,3,4 乘以2的队列：6，8 乘以3的队列：6,9,12 乘以5的队列：5,10,15,20 选择三个队列头里最小的数5加入丑数数组，同时将该最小的数乘以2,3,5放入三个队列； （5）丑数数组：1,2,3,4,5 乘以2的队列：6,8,10， 乘以3的队列：6,9,12,15 乘以5的队列：10,15,20,25 选择三个队列头里最小的数6加入丑数数组，但我们发现，有两个队列头都为6，所以我们弹出两个队列头，同时将12,18,30放入三个队列； …………………… 代码示例1234567891011121314151617181920212223242526272829public class Solution &#123; public int GetUglyNumber_Solution(int index) &#123; if(index&lt;=0)&#123; return 0; &#125; int[] result = new int[index]; result[0] = 1; int i2 = 0; int i3 = 0; int i5 = 0; int count = 0; int temp = 0; while(count&lt;index-1)&#123; temp = Math.min(result[i2]*2,Math.min(result[i3]*3,result[i5]*5)); if(temp==result[i2]*2)&#123; i2++; &#125; if(temp==result[i3]*3)&#123; i3++; &#125; if(temp==result[i5]*5)&#123; i5++; &#125; result[++count] = temp; &#125; return result[index-1]; &#125;&#125; 第一个只出现一次的字符 题目描述在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 思路见代码注释。 代码示例 1234567891011121314151617181920212223242526272829import java.util.*;public class Solution &#123; public int FirstNotRepeatingChar(String str) &#123; if(str.length()==0)&#123; return -1; &#125; // 先统计每个字符出现的次数 Map&lt;Character,Integer&gt; map = new HashMap&lt;&gt;(); char[] chars = str.toCharArray(); for(int i=0;i&lt;chars.length;i++)&#123; if(map.containsKey(chars[i]))&#123; int times = map.get(chars[i]); map.put(chars[i],++times); &#125;else&#123; map.put(chars[i],1); &#125; &#125; int pos = -1; for(int i=0;i&lt;chars.length;i++)&#123; if(map.get(chars[i])==1)&#123; // 第一次发现字符对应的出现次数为1，立即返回下标 return i; &#125; &#125; return pos; &#125;&#125; 数组中的逆序对 题目描述在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 思路归并排序 代码示例 12 两个链表的第一个公共结点 题目描述输入两个链表，找出它们的第一个公共结点。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的） 思路找出两个链表的长度，然后让长的链表先走两个链表的长度差，接着两个链表一起走。 代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041public class Solution &#123; public ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) &#123; if(pHead1==null||pHead2==null)&#123; return null; &#125; int len1 = getLen(pHead1); int len2 = getLen(pHead2); if(len1&gt;len2)&#123; int step = len1-len2; for(int i=0;i&lt;step;i++)&#123; pHead1=pHead1.next; &#125; &#125; if(len2&gt;len1)&#123; int step = len2-len1; for(int i=0;i&lt;step;i++)&#123; pHead2=pHead2.next; &#125; &#125; while(pHead1!=null&amp;&amp;pHead2!=null)&#123; if(pHead1.val==pHead2.val)&#123; return pHead1; &#125;else&#123; pHead1 = pHead1.next; pHead2 = pHead2.next; &#125; &#125; return null; &#125; public int getLen(ListNode head)&#123; int count=0; while(head!=null)&#123; count++; head = head.next; &#125; return count; &#125;&#125; 数字在排序数组中出现的次数 题目描述统计一个数字在排序数组中出现的次数。 代码示例 12345678910public class Solution &#123; public int GetNumberOfK(int [] array , int k) &#123; int count=0; for(int i=0;i&lt;array.length;i++)&#123; if(array[i]==k) count++; &#125; return count; &#125;&#125; 二叉树的深度 题目描述输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 思路递归 代码示例 123456789101112131415161718192021222324import java.util.*;/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public int TreeDepth(TreeNode root) &#123; if(root==null)&#123; return 0; &#125; int left = TreeDepth(root.left); int right = TreeDepth(root.right); return Math.max(left,right)+1; &#125;&#125; 平衡二叉树 题目描述输入一棵二叉树，判断该二叉树是否是平衡二叉树。 思路左右子树的高度相差不超过 1 。 代码示例 123456789101112131415161718192021import java.util.*;public class Solution &#123; public boolean IsBalanced_Solution(TreeNode root) &#123; if(root==null)&#123; return true; &#125; if(Math.abs(treeDepth(root.left) - treeDepth(root.right)) &gt; 1)&#123; return false; &#125; return IsBalanced_Solution(root.left)&amp;&amp;IsBalanced_Solution(root.right); &#125; public int treeDepth(TreeNode root)&#123; if(root==null)&#123; return 0; &#125; int left = treeDepth(root.left); int right = treeDepth(root.right); return Math.max(left,right)+1; &#125;&#125; 数组中只出现一次的数字 题目描述 思路 代码示例 12 和为S的连续正数序列 题目描述 思路 代码示例 12 和为S的两个数字 题目描述 思路 代码示例 12 左旋转字符串 题目描述汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 思路使用两个StringBuffer，对字符串进行截断，然后再拼接。 代码示例 12345678910111213import java.util.*;import java.lang.*;public class Solution &#123; public String LeftRotateString(String str,int n) &#123; if(str.length()==0||n&lt;0)&#123; return ""; &#125; StringBuffer sb=new StringBuffer(str.substring(0,n)); StringBuffer sb1=new StringBuffer(str.substring(n,str.length())); sb1.append(sb); return sb1.toString(); &#125;&#125; 翻转单词顺序列 题目描述牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 代码示例 1234567891011121314public class Solution &#123; public String ReverseSentence(String str) &#123; String trim = str.trim(); String a = ""; if("".equals(str.trim()))&#123; return str; &#125; String[] split = str.split(" "); for (int i = split.length-1; i &gt;=0; i--) &#123; a +=split[i]+" "; &#125; return a.trim(); &#125;&#125; 扑克牌顺子 题目描述 思路 代码示例 12 孩子们的游戏（圆圈中最后剩下的数) 题目描述 思路 代码示例 12 求1到n的整数和 题目描述求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 思路直接使用等差数列的求和公式。利用到Math.pow(n,2)。(n的二次方 + n)/2 代码示例 1234567public class Solution &#123; public int Sum_Solution(int n) &#123; int sum = (int) (Math.pow(n,2) + n); // 除以2右移1位操作 return sum&gt;&gt;1; &#125;&#125; 不用加减乘除做加法 题目描述写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 思路使用java中的BigInteger封装类。 代码示例 12345678import java.math.*;public class Solution &#123; public int Add(int num1,int num2) &#123; BigInteger bi1=new BigInteger(String.valueOf(num1)); BigInteger bi2=new BigInteger(String.valueOf(num2)); return bi1.add(bi2).intValue(); &#125;&#125; 把字符串转化成整数 题目描述 思路 代码示例 12 数组中的重复数字 题目描述在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 思路需要额外的boolean[]数组，以numbers[i]值为下标，如果设置过这个下标值，对应的值就设置为true。 代码示例 12345678910111213141516171819202122public class Solution &#123; // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number,length of duplication array is 1,so using duplication[0] = ? in implementation; // Here duplication like pointor in C/C++, duplication[0] equal *duplication in C/C++ // 这里要特别注意~返回任意重复的一个，赋值duplication[0] // Return value: true if the input is valid, and there are some duplications in the array number // otherwise false public boolean duplicate(int numbers[],int length,int [] duplication) &#123; // 将当前输入数组作为boolean[] k数组的下标 boolean[] k = new boolean[length]; for(int i=0;i&lt;length;i++)&#123; if(k[numbers[i]]==true)&#123; duplication[0] = numbers[i]; return true; &#125; k[numbers[i]] = true; &#125; return false; &#125;&#125; 构建乘积数组 题目描述 思路 代码示例 12 正则表达式匹配 题目描述 思路 代码示例 12 表示数值的字符串 题目描述请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 思路根据题目中列举的场景，进行正则匹配。 代码示例 12345678910111213public class Solution &#123; public boolean isNumeric(char[] str) &#123; String s=String.valueOf(str); //return s.matches("[+-]?[0-9]*(.[0-9]*)?([eE][+-]?[0-9]+)?"); // [] 匹配所包含的任意字符 // ？后面的字符紧跟在前面的字符，非贪婪匹配，非必须的 // *匹配前面的子表达式任意次 // +匹配一次或多次 // \\. 转义特殊字符 // 最前面的[+-]不一定需要，必须以[0-9]数字开头，后面接(\\.[0-9]*)，或者eE等操作 return s.matches("[+-]?[0-9]*(\\.[0-9]*)?([eE][+-]?[0-9]+)?"); &#125;&#125; 字符流中第一个不重复的字符 题目描述请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。输出描述:如果当前字符流没有存在出现一次的字符，返回#字符。 思路与数组中的重复数字的思路类似。 代码示例 12345678910111213141516171819202122public class Solution &#123; // 数组的下标值为字符，字符占8位，不会超过256。对应每个字符出现的次数 int[] hashtable = new int[256]; StringBuilder sb = new StringBuilder(); //Insert one char from stringstream public void Insert(char ch)&#123; sb.append(ch); hashtable[ch] +=1; &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce()&#123; String str = sb.toString(); char[] chars = str.toCharArray(); for(int i=0;i&lt;chars.length;i++)&#123; if(hashtable[chars[i]]==1)&#123; // 找到字符对应的出现次数为1，立即返回 return chars[i]; &#125; &#125; return '#'; &#125;&#125; 链表中环的入口结点 题目描述给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 思路 代码示例 12345678910111213141516171819202122232425262728293031323334353637383940/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode EntryNodeOfLoop(ListNode pHead)&#123; if(pHead==null || pHead.next==null)&#123; return null; &#125; ListNode fast=pHead; ListNode slow=pHead; while(fast!=null &amp;&amp; fast.next!=null)&#123; fast = fast.next.next; slow = slow.next; if(fast==slow)&#123; // 找到相遇点 break; &#125; &#125; // 链表中没有环 if(fast==null||fast.next==null)&#123; return null; &#125; // fast重新指向第一个结点 fast = pHead; while(fast!=slow)&#123; fast=fast.next; slow=slow.next; &#125; return fast; &#125;&#125; 删除链表中重复的结点 题目描述在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 思路 代码示例123456789101112131415161718192021222324252627282930313233/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead)&#123; if(pHead==null)&#123; return null; &#125; if(pHead!=null&amp;&amp;pHead.next==null)&#123; return pHead; &#125; ListNode current; if(pHead.val==pHead.next.val)&#123; current = pHead.next.next; while(current!=null &amp;&amp; current.val==pHead.val)&#123; current = current.next; &#125; return deleteDuplication(current); &#125;else&#123; current = pHead.next; pHead.next = deleteDuplication(current); return pHead; &#125; &#125;&#125; 二叉树的下一个结点 题目描述给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 思路分析二叉树的下一个节点，一共有以下情况：（1）二叉树为空，则返回空；（2）节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点；（3）节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复第三步的判断，返回结果。 代码示例 123456789101112131415161718192021222324252627282930313233343536/*public class TreeLinkNode &#123; int val; TreeLinkNode left = null; TreeLinkNode right = null; TreeLinkNode next = null; TreeLinkNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public TreeLinkNode GetNext(TreeLinkNode pNode)&#123; if(pNode==null)&#123; return null; &#125; // 如果有右子树，则返回右子树的最左结点 if(pNode.right!=null)&#123; pNode = pNode.right; while(pNode.left!=null)&#123; pNode = pNode.left; &#125; return pNode; &#125; // 节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点； while(pNode.next!=null)&#123; TreeLinkNode parent = pNode.next; if(pNode.next.left==pNode)&#123; return parent; &#125; pNode = pNode.next; &#125; return null; &#125;&#125; 对称二叉树 题目描述请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 思路递归判断：①只要pRoot.left和pRoot.right是否对称即可。②左右节点的值相等且对称子树left.left，right.right ;left.rigth,right.left也对称。 代码示例 12345678910111213141516171819202122232425262728293031323334/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; boolean isSymmetrical(TreeNode pRoot)&#123; if(pRoot==null)&#123; return true; &#125; return judge(pRoot.left,pRoot.right); &#125; boolean judge(TreeNode leftNode,TreeNode rightNode)&#123; if(leftNode==null)&#123; return rightNode==null; &#125; if(rightNode==null)&#123; return false; &#125; if(leftNode.val!=rightNode.val)&#123; return false; &#125; return judge(leftNode.left,rightNode.right) &amp;&amp; judge(leftNode.right,rightNode.left); &#125;&#125; 按之字形顺序打印二叉树 题目描述请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 思路利用栈先进后出的性质，创建两个栈分别存放奇数层与偶数层的节点。 代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.*;/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; allList = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); int layer = 1; Stack&lt;TreeNode&gt; odd = new Stack&lt;TreeNode&gt;(); odd.push(pRoot); Stack&lt;TreeNode&gt; even = new Stack&lt;TreeNode&gt;(); while(!odd.empty() || !even.empty())&#123; if(layer%2!=0)&#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); while(!odd.isEmpty())&#123; TreeNode node = odd.pop(); if(node!=null)&#123; list.add(node.val); even.push(node.left); even.push(node.right); &#125; &#125; if(!list.isEmpty())&#123; allList.add(list); layer++; &#125; &#125;else&#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); while(!even.isEmpty())&#123; TreeNode node = even.pop(); if(node!=null)&#123; list.add(node.val); odd.push(node.right); odd.push(node.left); &#125; &#125; if(!list.isEmpty())&#123; allList.add(list); layer++; &#125; &#125; &#125; return allList; &#125;&#125; 把二叉树打印成多行 题目描述从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 思路借助LinkedList这个Queue对象来实现。 代码示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.*;/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot == null)&#123; return result; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); ArrayList&lt;Integer&gt; queueList = new ArrayList&lt;Integer&gt;(); queue.add(pRoot); int start = 0 ,end = 1; while(!queue.isEmpty())&#123; TreeNode current = queue.remove(); queueList.add(current.val); start++; if(current.left!=null)&#123; queue.add(current.left); &#125; if(current.right!=null)&#123; queue.add(current.right); &#125; if(start == end)&#123; end = queue.size(); start=0; result.add(queueList); queueList = new ArrayList&lt;Integer&gt;(); &#125; &#125; return result; &#125; &#125; 序列化二叉树 题目描述请实现两个函数，分别用来序列化和反序列化二叉树 二叉树的序列化是指：把一棵二叉树按照某种遍历方式的结果以某种格式保存为字符串，从而使得内存中建立起来的二叉树可以持久保存。序列化可以基于先序、中序、后序、层序的二叉树遍历方式来进行修改，序列化的结果是一个字符串，序列化时通过 某种符号表示空节点（#），以 ！ 表示一个结点值的结束（value!）。 二叉树的反序列化是指：根据某种遍历顺序得到的序列化字符串结果str，重构二叉树。 思路（1）根据前序遍历规则完成序列化与反序列化。所谓序列化指的是遍历二叉树为字符串；所谓反序列化指的是依据字符串重新构造成二叉树。（2）依据前序遍历序列来序列化二叉树，因为前序遍历序列是从根结点开始的。当在遍历二叉树时碰到Null指针时，这些Null指针被序列化为一个特殊的字符“#”。另外，结点之间的数值用逗号隔开。 代码示例 12345678910111213141516171819202122232425262728293031323334353637383940414243/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public int index=-1; String Serialize(TreeNode root) &#123; StringBuffer sb = new StringBuffer(); if(root==null)&#123; sb.append("#,"); return sb.toString(); &#125; sb.append(root.val+","); sb.append(Serialize(root.left)); sb.append(Serialize(root.right)); return sb.toString(); &#125; TreeNode Deserialize(String str) &#123; index++; int len = str.length(); if(index&gt;=len)&#123; return null; &#125; String[] strr = str.split(","); TreeNode root = null; while(!strr[index].equals("#"))&#123; root = new TreeNode(Integer.valueOf(strr[index])); root.left = Deserialize(str); root.right = Deserialize(str); &#125; return root; &#125;&#125; 二叉搜索树的第k个结点 题目描述给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 思路使用中序遍历找到第k个节点。 代码示例 1234567891011121314151617181920212223242526272829303132333435363738/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public int index = 0; // 中序遍历，左根右 TreeNode KthNode(TreeNode pRoot, int k)&#123; if(pRoot!=null)&#123; TreeNode node = KthNode(pRoot.left,k); if(node!=null)&#123; return node; &#125; index++; if(index==k)&#123; return pRoot; &#125; node = KthNode(pRoot.right,k); if(node!=null)&#123; return node; &#125; &#125; return null; &#125;&#125; 数据流中的中位数 题目描述如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 思路使用一个大顶堆和一个小顶堆，维持大顶堆的数都小于等于小顶堆的数，且两者的个数相等或差1。平均数就在两个堆顶的数之中。 代码示例 12345678910111213141516171819202122232425262728293031323334import java.util.*;public class Solution &#123; // 小顶堆 private PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;Integer&gt;(); // 大顶堆 private PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(15,new Comparator&lt;Integer&gt;()&#123; @Override public int compare(Integer o1,Integer o2)&#123; return o2-o1; &#125; &#125;); // 记录偶数个还是奇数个 int count = 0; public void Insert(Integer num) &#123; if(count%2==0)&#123; maxHeap.offer(num); int max = maxHeap.poll(); minHeap.offer(max); &#125;else&#123; minHeap.offer(num); int min = minHeap.poll(); maxHeap.offer(min); &#125; count++; &#125; public Double GetMedian() &#123; if(count%2==0)&#123; return new Double(minHeap.peek()+maxHeap.peek())/2; &#125;else&#123; return new Double(minHeap.peek()); &#125; &#125;&#125; 滑动窗口的最大值 题目描述给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 思路 代码示例 1234567891011121314151617181920212223242526272829import java.util.*;public class Solution &#123; public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size)&#123; if(num==null || size&lt;0)&#123; return null; &#125; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if(size==0)&#123; return list; &#125; ArrayList&lt;Integer&gt; temp = null; int length = num.length; if(length&lt;size)&#123; return list; &#125;else&#123; // 窗口个数 = num.length-size+1 ，8-3+1=6 int windows = length-size+1; for(int i=0;i&lt;windows;i++)&#123; temp = new ArrayList&lt;Integer&gt;(); for(int j=i;j&lt;size+i;j++)&#123; temp.add(num[j]); &#125; Collections.sort(temp); list.add(temp.get(temp.size()-1)); &#125; &#125; return list; &#125;&#125; 矩阵中的路径 题目描述 思路 代码示例 12 机器人的运动范围 题目描述 思路 代码示例 12 剪绳子 题目描述给你一根长度为n的绳子，请把绳子剪成整数长的m段（m、n都是整数，n&gt;1并且m&gt;1），每段绳子的长度记为k[0],k[1],…,k[m]。请问k[0]xk[1]x…xk[m]可能的最大乘积是多少？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。 思路动态规划。设f(n)代表长度为n的绳子剪成若干段的最大乘积，如果第一刀下去，第一段长度是i，那么剩下的就需要剪n-i，那么f(n)=max{f(i)f(n-i)}。而f(n)的最优解对应着f(i)和f(n-i)的最优解，假如f(i)不是最优解，那么其最优解和f(n-i)乘积肯定大于f(n)的最优解，和f(n)达到最优解矛盾，所以f(n)的最优解对应着f(i)和f(n-i)的最优解。首先，剪绳子是最优解问题，其次，大问题包含小问题，并且大问题的最优解包含着小问题的最优解，所以可以使用动态规划求解问题，并且从小到大求解，把小问题的最优解记录在数组中，求大问题最优解时就可以直接获取，避免重复计算。 n&lt;2时，由于每次至少减一次，所以返回0。n=2时，只能剪成两个1，那么返回1。n=3时，可以剪成3个1，或者1和2，那么最大乘积是2。当n&gt;3时，就可以使用公式进行求解。 f(4)=max{f(1)f(3), f(2)f(2)} f(5)=max{f(1)f(4), f(2)f(3)} … f(n)=max{f(1)f(n-1), f(2)f(n-2), f(3)f(n-3), …, f(i)(fn-i), …} 因为需要保证f(i)f(n-i)不重复，就需要保证i&lt;=n/2，这是一个限制条件，求1～n/2范围内的乘积，得到最大值。 代码示例 123456789101112131415161718192021222324252627282930public class Solution &#123; public int cutRope(int target) &#123; if(target&lt;2)&#123; return 0; &#125; if(target==2)&#123; return 1; &#125; if(target==3)&#123; return 2; &#125; int[] num = new int[target+1]; num[0] = 0; num[1] = 1; num[2] = 2; num[3] = 3; for(int i=4;i&lt;=target;i++)&#123; int max=0; for(int j=1;j&lt;=i/2;j++)&#123; int temp = num[j]*num[i-j]; if(max&lt;temp)&#123; max=temp; &#125; &#125; num[i] = max; &#125; return num[target]; &#125;&#125; 经典排序算法参考：十大经典排序算法（动图演示） 冒泡排序相邻的元素进行两两比较，满足条件则交换，这样一趟会将最小或最大的元素”浮”到顶端。 选择排序每一趟从待排序的元素中选择最小(大)的一个元素作为首元素。 插入排序每一步将一个待排序的元素，插入到前面已经排好的有序序列中，直到插完所有的元素。 归并排序“分”而治之 快速排序先选定一个基数，将小于基数的放到前面，大于基数的放到后面循环往复 希尔排序分组，不断除以2交换元素 堆排序PriorityQueue 计数排序桶排序基数排序算法复杂度树参考：树 普通二叉树 如果根节点选择的是最小或最大的树，那么二叉树就完全退化成了线性结构，树的高度会很高，查找复杂度变高。 平衡二叉树 左右子树的树高差不超过1，和红黑树相比，它是严格的平衡二叉树。在执行插入或删除操作，只要不满足条件，就要通过旋转来保持平衡，旋转是非常耗时的，因此AVL树适合用于插入删除次数比较少，但查找多的情况。 红黑树 每个节点增加了一个存储位表示节点的颜色，red 或 black。通过对任何一条从根到叶子的路径上各个节点的着色的方式的限制，红黑树确保没有一条路径会比其他路径长出两倍。是一种弱平衡二叉树，旋转次数变少，所以对于搜索、插入、删除操作多的情况下，我们就用红黑树。 每个节点非红即黑 根节点是黑色的 每个叶节点（即树尾端NULL指针或NULL节点）都是黑色的 如果一个节点是红色的，那么它的两儿子都是黑色的 每条路径包含相同的黑色节点 应用： map 和 set 都是用红黑树实现的，java中 TreeMap 的实现 linux进程调度Completely Fair Scheduler，用红黑树管理进程控制块，进程的虚拟内存区域都存储在一棵红黑树上，每个虚拟地址区域都对应红黑树的一个节点，左指针指向相邻的地址虚拟存储区域，右指针指向相邻的高地址虚拟地址空间 IO多路复用epoll的实现采用红黑树组织管理 sockfd，以支持快速的增删改查 B/B+树B/B+树是为了磁盘或其他存储设备而设计的一种平衡多路查找树，B树每个内节点有多个分支。与红黑树相比，在相同节点的情况下，一棵B/B+树的高度远远小于红黑树的高度。B/B+树上操作的时间通常由存取磁盘的时间和CPU计算时间两部分构成，而CPU的速度非常快，所以B树的操作小笼包取决于访问磁盘的次数，关键字总数相同的情况下B树的高度越小，磁盘I/O所花的时间越少。 B树B-树和B树是等价的。 在实际中B树节点中关键字很多的，比如上图中的35节点，35代表一个key（索引），而小黑块代表的是这个key所指向的内容在内存中实际的存储位置，是一个指针。 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；（至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； B+树B+树是B树的变形树，文件的目录一级一级索引，只有最底层的叶子结点（文件）保存数据，非叶子结点只保存索引，不保存实际的数据，数据都保存在叶子结点中。所有的非叶子结点都可以看成索引部分。 优点： B+树只有叶节点存放数据，其余节点用来索引，而B树的每个索引节点都会有 Data 域。 B+树所有的Data域在叶子节点，并且所有叶子节点之间都有一个链指针。 这样遍历叶子节点就能获得全部数据，这样就能进行区间访问啦。在数据库中基于范围的查询是非常频繁的，而B树不支持这样的遍历操作。 与B树不相同的性质： 非叶子节点的子树指针与关键字个数相同; 非叶子节点的子树指针p[i],指向关键字值属于[k[i],k[i+1]]的子树.(B树是开区间,也就是说B树不允许关键字重复,B+树允许重复)； 为所有叶子节点增加一个链指针. 所有关键字都在叶子节点出现(稠密索引). (且链表中的关键字恰好是有序的); 非叶子节点相当于是叶子节点的索引(稀疏索引),叶子节点相当于是存储(关键字)数据的数据层. 更适合于文件系统; 不使用红黑树做索引的原因： n个节点的平衡二叉树的高度为H即（logn），而n个节点的B/B+树高度为logt((n+1)/2)+1 AVL 树（平衡二叉树）和红黑树（二叉查找树）基本都是存储在内存中才会使用的数据结构。在大规模数据存储的时候，红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况。为什么会出现这样的情况，我们知道要获取磁盘上数据，必须先通过磁盘移动臂移动到数据所在的柱面，然后找到指定盘面，接着旋转盘面找到数据所在的磁道，最后对数据进行读写。磁盘IO代价主要花费在查找所需的柱面上，树的深度过大会造成磁盘IO频繁读写。根据磁盘查找存取的次数往往由树的高度所决定，所以，只要我们通过某种较好的树结构减少树的结构尽量减少树的高度，B树可以有多个子女，从几十到上千，可以降低树的高度。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式事务]]></title>
    <url>%2F2020%2F03%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[本文将介绍什么是分布式事务，分布式事务解决什么问题，不同场景下方案的选择。 参考：分布式事务，特此整理到自己的博客中。 事务事务的具体定义简单地说，事务提供一种“ 要么什么都不做，要么做全套（All or Nothing）”机制。 数据库事务，简单而言，就是业务上有一组数据操作，需要如果其中有任何一个操作执行失败，整组操作全部不执行并恢复到未执行状态，要么全部成功，要么全部失败。在使用数据库事务时需要注意，尽可能短的保持事务，修改多个不同表的数据的冗长事务会严重妨碍系统中的所有其他用户，这很有可能导致一些性能问题。 ACID属性例如：银行转账，从A账户转100元至B账户，分为两个步骤： 从A账户取100元 存入100元至B账户 原子性 事务操作的整体性。上述两步要么一起完成，要么一起不完成。 一致性 事务操作下数据的正确性。现有完整性约束 A+B=100，如果一个事务改变了 A，那么必须得改变 B，使得事务结束后依然满足 A+B=100，否则事务失败。 隔离性 事务并发操作下数据的正确性。现有有个交易是从 A 账户转 100 元至 B 账户，在这个交易事务还未完成的情况下，如果此时 B 查询自己的账户，是看不到新增加的 100 元的。 持久性 事务对数据修改的可靠性。事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 分布式事务产生的背景随着互联网快速发展，微服务，SOA 等服务架构模式正在被大规模的使用，现在分布式系统一般由多个独立的子系统组成，多个子系统通过网络通信互相协作配合完成各个功能。 有很多用例会跨多个子系统才能完成，比较典型的是电子商务网站的下单支付流程，至少会涉及交易系统和支付系统。 下图所示是互联网常用的交易业务，包含库存和订单两个独立的微服务，每个微服务维护了自己的数据库： 难点 原子性 事务操作跨不同节点，当多个节点某一节点操作失败时，需要保证多节点操作的要么什么都不做，要么做全套（All or Nothing）的原子性。 一致性 当发生网络传输故障或者节点故障，节点间数据复制通道中断，在进行事务操作时需要保证数据一致性，保证事务的任何操作都不会使得数据违反数据库定义的约束、触发器等规则。 隔离性 事务隔离性的本质就是如何正确处理多个并发事务的读写冲突和写写冲突，因为在分布式事务控制中，可能会出现提交不同步的现象，这个时候就有可能出现“部分已经提交”的事务。 此时并发应用访问数据如果没有加以控制，有可能出现“脏读”问题。 分布式系统的一致性可用性和一致性冲突：CAP理论在一个分布式系统中，当涉及写操作时，只能保证三者中的两个，另一个必须牺牲。只能选择 CP 或者 AP 架构，在一致性和可用性间做折中选择。 C：对某个指定的客户端来说，强调读操作保证能够返回最新的写操作结果，不会读取到实物中间写入的数据。A：非故障节点在合理的时间内返回合理的响应（不是错误和超时的响应）。P：由于一些故障（节点间网络连接断开、节点宕机），使得有些节点之间不连通了，整个网络就分成了几块区域，系统能够“继续履行职责”。 常见分布式事务解决方案2PC（二阶段提交）：强一致性 准备阶段 协调者向所有参与者发送事务内容，询问是否可以提交事务，并等待所有参与者答复。 各参与者执行事务操作，将undo和redo信息记入事务日志中（但不提交事务）。 如参与者执行成功，给协调者反馈yes，即可以提交；如执行失败，给协调者反馈no，即不可提交。 提交阶段 如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚（rollback）消息；否则，发送提交（commit）消息。 方案总结： 性能问题：所有参与者在准备阶段处于同步阻塞状态，占用系统资源，容易导致性能瓶颈。 可靠性问题：如果协调者出现故障，参与者将一直处于锁定状态。 数据一致性问题：在提交阶段，如果发生局部网络问题，一部分事务参与者收到了提交消息，另一部分事务参与者没收到提交消息，就导致了节点之间数据的不一致。 3PC（三阶段提交） canCommit 协调者向所有参与者发出包含事务内容的 canCommit 请求，询问是否可以提交事务，并等待所有参与者答复。 参与者收到 canCommit 请求后，如果认为可以执行事务操作，则反馈 yes 并进入预备状态，否则反馈 no。 preCommit 协调者向所有参与者发出 preCommit 请求，进入准备阶段。 参与者收到 preCommit 请求后，执行事务操作，将 undo 和 redo 信息记入事务日志中（但不提交事务）。 各参与者向协调者反馈 ack 响应或 no 响应，并等待最终指令。 doCommit 如果协调者处于工作状态，则向所有参与者发出 do Commit 请求。 参与者收到 do Commit 请求后，会正式执行事务提交，并释放整个事务期间占用的资源。 各参与者向协调者反馈 ack 完成的消息。 协调者收到所有参与者反馈的 ack 消息后，即完成事务提交。 方案总结： 相比两阶段提交，降低了阻塞范围，在 canCommit 阶段等待超时后协调者或参与者会中断事务。在 doCommit 阶段，如果协调者出现问题时，参与者会继续提交事务。但数据不一致问题依然存在，在 doCommit 阶段，如果协调者请求中断事务，而协调者无法与参与者正常通信，会导致参与者继续提交事务，造成数据不一致。 TCC事务：最终一致性 Try阶段 完成所有业务检查（一致性）。 预留必须业务资源（准隔离性）。 Try 尝试执行业务。 Confirm/Cancel阶段 Confirm 和 Cancel 操作满足幂等性，如果 Confirm 和 Cancel 操作执行失败，将会不断重试知道执行完成。 方案总结： 性能提升：具体业务实现控制资源，锁的粒度变小，不会锁定整个资源。 数据最终一致性：基于 Confirm 和 Cancel 的幂等性，保证事务最终完成确认或者取消，保证数据的一致性。 可靠性：解决协调者单点故障问题，由主业务方发起并控制整个业务活动，业务活动管理器也变成多点。 但是，TCC 的 Try、Confirm、Cancel 操作功能要按具体业务来实现，业务耦合度较高，提高了开发成本。 本地消息表：最终一致性下面把分布式事务最先开始处理的事务方称为事务主动方，在事务主动方之后处理的业务内的其他事务称为事务被动方。为了方便理解，下面继续以电商下单为例进行方案解析，这里把整个过程简单分为扣减库存，订单创建 2 个步骤。库存服务和订单服务分别在不同的服务器节点上，其中库存服务是事务主动方，订单服务是事务被动方。事务的主动方需要额外新建事务消息表，用于记录分布式事务的消息的发生、处理状态。 MQ事务：最终一致性 发送方向 MQ Server 发送 half 消息。 MQ Server 将消息持久化成功之后，向发送方 ack 确认消息已经发送成功。 发送方开始执行本地事务逻辑。 发送方根据本地事务执行结果向 MQ Server 提交二次确认 (commit或是rollback)。 MQ Server 收到 commit 状态则将半消息标记为可投递，订阅方最终将收到该消息；MQ Server 收到 rollback 状态则删除半消息，订阅方将不会接受该消息。 二次确认超时的情况： MQ Server 对该消息发起消息回查。 发送方收到消息回查后，需要检查对应消息的本地事务执行的最终结果。 发送方根据检查得到本地事务的最终状态再次提交二次确认， MQ Server基于 commit/rollback 对消息进行投递或者删除。 Soga事务：最终一致性在事件编排方法中，第一个服务执行一个事务，然后发布一个事件。该事件被一个或多个服务进行监听，这些服务再执行本地事务并发布（或不发布）新的事件。当最后一个服务执行本地事务并且不发布任何事件时，意味着分布式事务结束，或者它发布的事件没有被任何 Saga 参与者听到都意味着事务结束。 各方案使用场景]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink内存管理]]></title>
    <url>%2F2020%2F03%2F03%2FFlink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文将介绍一下Flink的内存管理。参考 量身定制的序列化框架 直接操作二进制数据 Flink通过定制的序列化框架将算法中需要操作的数据（如sort中的key）连续存储，而完整的数据存储在其他地方。因为对于完整的数据来说，key+pointer 更容易装进缓存，这大大提高了缓存命中率，从而提高了基础算法的效率。这对于上层应用是完全透明的，可以充分享受缓存友好带来的性能提升。 堆外内存Flink 基于堆内存的内存管理机制已经可以解决很多 JVM 现存问题了，为什么还要引入堆外内存？ 启动超大内存（上百 GB）的 JVM 需要很长时间，GC 停留时间也会很长（分钟级）。使用堆外内存的话，可以极大地减小堆内存（只需要分配 Remaining Heap 那一块），使得 TaskManager 扩展到上百 GB 内存不是问题。 高效的 IO 操作。堆外内存在写磁盘或网络传输时是 zero-copy，而堆内存的话，至少需要 copy 一次。 堆外内存是进程间共享的。也就是说，即使 JVM 进程崩溃也不会丢失数据。这可以用来做故障恢复（Flink 暂时没有利用起这个，不过未来很可能会去做）。 Flink用通过ByteBuffer.allocateDirect(numBytes)来申请堆外内存，用 sun.misc.Unsafe 来操作堆外内存。 基于 Flink 优秀的设计，实现堆外内存是很方便的。Flink 将原来的 MemorySegment 变成了抽象类，并生成了两个子类。HeapMemorySegment 和 HybridMemorySegment。从字面意思上也很容易理解，前者是用来分配堆内存的，后者是用来分配堆外内存和堆内存的。是的，你没有看错，后者既可以分配堆外内存又可以分配堆内存。为什么要这样设计呢？ 首先假设HybridMemorySegment只提供分配堆外内存。在上述堆外内存的不足中的第二点谈到，Flink 有时需要分配短生命周期的 buffer，这些 buffer 用HeapMemorySegment会更高效。那么当使用堆外内存时，为了也满足堆内存的需求，我们需要同时加载两个子类。这就涉及到了 JIT 编译优化的问题。因为以前 MemorySegment 是一个单独的 final 类，没有子类。JIT 编译时，所有要调用的方法都是确定的，所有的方法调用都可以被去虚化（de-virtualized）和内联（inlined），这可以极大地提高性能（MemroySegment的使用相当频繁）。然而如果同时加载两个子类，那么 JIT 编译器就只能在真正运行到的时候才知道是哪个子类，这样就无法提前做优化。实际测试的性能差距在 2.7 被左右。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Apache HBase]]></title>
    <url>%2F2020%2F02%2F27%2FApache-HBase%2F</url>
    <content type="text"><![CDATA[本文将全局介绍 HBase 的整体架构。]]></content>
      <categories>
        <category>NoSql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Apache Kafka]]></title>
    <url>%2F2020%2F02%2F18%2FApache-Kafka%2F</url>
    <content type="text"><![CDATA[本文将全局介绍 Kafka 的整体架构。 概念 消息 消息是Kafka中最基本的数据单元。消息由一串字节构成，主要由key和value构成，key和value也都是byte数组。 Topic Topic是用于存储消息的逻辑概念，可以看作一个消息集合。每个Topic可以有多个生产者向其推送消息，也可以有多个Consumer消费其中的消息。 Partition 每个Topic可以划分为多个Partition，同一Topic下的不同分区包含的消息是不同的。每个消息在被添加到分区时，都会被分配一个offset，它是消息在此分区中的唯一编号，Kafka通过offset保证消息在分区内的顺序。 Log 分区在逻辑上对应一个Log，当生产者将消息写入分区时，实际上是写入到分区对应的Log中。Log是一个逻辑概念，可以对应到磁盘上的一个目录。Log由多个Segment组成，每个Segment对应一个日志文件和索引文件。 保留策略(Retention Policy)和日志压缩(Log Compaction) 一种是设置消息的保留时间，另一种是设置日志文件大小阈值。Kafka后台启动线程，定期将相同的key进行合并。 Replica 每个Partition可以有多个副本，所有的读写请求都由选举出的Leader副本处理，其他都作为Follower副本，Follower副本仅仅是从Leader副本处把数据拉取到本地，同步更新到自己的Log中。 ISR集合 ISR集合是由Leader副本管理的， 表示的是目前可用(alive)且消息量与Leader相差不多的副本集合，是整个副本集合的一个子集。 HW &amp; LEO HW（High Watermark）是由Leader副本管理的，HW之后的消息对消费者不可见。当ISR集合中的全部Follower副本都拉取HW指定消息进行同步之后，Leader副本递增HW的值。 LEO（Log End Offset）是所有副本都会管理的一个offset标记，指向追加到当前副本的最后一个消息的offset。 Broker 一个单独的Kafka Server就是一个Broker。Broker的主要工作就是接收生产者发送的消息，分配offset，之后保存到磁盘；同时接收消费者、其他Broker的请求，根据请求类型进行相应处理并返回响应。一般生产环境中，一个Broker独占一台物理机。 Controller Controller是Kafka集群的指挥中心，而其他Broker则听从Controller指挥实现相应地功能。Controller负责管理分区的状态、管理每个分区的副本状态、监听Zookeeper中的数据变化等工作。当Leader Controller出现故障时则重新选举新的Controller。 Producer 生产者的主要工作是生产消息，并将消息按照一定的规则推送到Topic。选择分区的规则可以有多种，根据消息的key的Hash值选择分区，或按序轮询全部分区的方式，或自定义路由规则。 Consumer 消费者的主要工作是从Topic拉取消息，并对消息进行消费。每个消费者消费到Partition的哪个位置（offset）的相关信息，是Consumer自己维护的。 Consumer Group 在Kafka中，多个Consumer可以组成一个Consumer Group，一个Consumer只能属于一个Consumer Group。 Broker模块功能Kafka源码包结构： Kafka源码组成： LogManager定位LogSegment： 定位index项及position： LogManager模块实现： 此模块中的定时线程任务： kafka-log-retention 负责LogSegment的清理工作，一是LogSegment的存活时长log.retention.hours，二是Log的大小log.retention.bytes。 kafka-log-flusher 定时刷写页面缓存中的日志到磁盘。log.flush.interval.messages，log.flush.interval.ms kafka-recovery-point-checkpoint 定时将所有数据目录的所有日志的检查点写到“recovery-point-offset-checkpoint”文件中。检查点是指日志已经刷新到磁盘的位置，用于故障恢复。 Cleaner activeSegment不参与。 log.clenear.enable Clean部分表示已经被压缩过的部分，dirty部分表示未压缩的部分。“迫切程度”是通过cleanableRation(ditry部分占整个Log的比例)决定的。 ReplicaManager KafkaControllerKafka在zookeeper上记录的信息： /brokers/ids/[id]:可用broker的id /brokers/topics/[topic]/partitions:topic的分区以及AR /brokers/topics/[topic]/partitions/[partition_id]/state:分区的Leader、ISR等 /controller_epoch:Controller Leader的选举次数 /controller:当前Controller Leader的id /admin/reassign_partitions:需要进行副本重新分配的分区 /admin/preferred_replica_election:需要进行“优先副本”选举的分区 /admin/delete_topics:待删除的Topic /isr_change_notification:ISR集合发生变化的分区 /config: 配置信息 KafkaController模块实现： GroupCoordinatorConsumerCoordinator的实现： GroupCoordinator的实现： Consumer加入Consumer Group的过程： 查找GroupCoordinator 发送JoinGroupRequest 发送SyncGroupRequest 问题1：集群规模大，KafkaConsumer数量多导致频繁Rebalance解决：0.11.0版本中添加了” group.initial.rebalance.delay.ms”配置，让GroupCoordinator推迟空消费组接收到成员加入请求后本应立即开启的rebalance。 问题2：Kafka0.10.0版本的Consumer心跳和next接口在同一个线程中，如果处理时间长，会导致超时解决：0.10.1版本中在ConsumerCoordinator中添加了单独的HeartbeatThread线程。 网络层实现Selector选择器Selector是SelectableChannel的多路复用器，可以同时监控多个SelectableChannel的IO状况，即利用Selector可使一个单独的线程管理多个SelectableChannel，SelectableChannel设置为非阻塞。服务端为ServerSocketChannel，客户端为SocketChannel。 通道使用register（Selector sel，int ops）方法将通道注册到选择器，第二个参数指定监听的事件类型。SelectionKey.OP_READ (1) 读SelectionKey.OP_WRITE (4) 写SelectionKey.OP_CONNECT (8) 连接SelectionKey.OP_ACCEPT (16) 接收 Selector.open() 创建一个Selectorselector.select() 阻塞等待监听的事件就绪，返回大于0说明有事件就绪selector.selectedKeys() 获得就绪的事件，使用其iterator遍历SelectionKey，根据key判断事件类型 key.isAcceptable()、key.isReadable()，处理完移除wakeUp() 唤醒由于调用select()方法而阻塞的线程 网络线程模型 broker端网络线程模型： Reactor单线程模型： Reactor多线程模型： 主从Reactor多线程模型： 与Netty网络线程模型的对比：Netty使用的是Reactor多线程模型：因为服务器端的 ServerSocketChannel 只绑定到了 bossGroup 中的一个线程, 因此在调用 Java NIO 的 Selector.select() 处理客户端的连接请求时, 实际上是在一个线程中的；而且workerGroup必须创建。Netty服务器端需要绑定多个端口时，才会用到主从Reactor多线程模型。Kafka网络层默认也是使用Reactor多线程模型，当”listeners”配置多个，监听不同的端口时，才用到主从Reactor多线程模型。Kafka网络层有独立的Handler处理线程池，而Netty的业务处理是在work线程调用的Handler中完成。 Kafka新老版本网络模型对比 写流程消息格式 V1 Message &amp; Message Set 12345678910111213141516171819202122V1的消息格式：也叫“浅层shallow消息”CRC：4字节，消息的校验码magic: 1字节，魔数标识，与消息格式有关，取值为0或1。当magic为0时，消息的offset使用绝对offset且 消息格式中没有timestamp部分；当magic为1时，消息的offset使用相对offset且消息格式中存在timestamp部分。attributes:1字节，消息的属性。其中第0-2位的组合表示使用的压缩类型，0表示无压缩，1表示gzip压缩，2表示snappy压缩，3表示lz4压缩。第3位表示时间戳类型，0表示创建时间，1表示追加时间。timestamp：时间戳。key length：消息key的长度。key: 消息的key。value length：消息的value长度。value：消息的value。V1消息长度举例： 消息头部开销为22字节，即消息的长度至少为22字节。 假设有一条Kafka消息，key是“key”，value是“hello”，那么key的长度就是3，value的长度就是5，因此这条Kafka消息需要占用22 + 3 + 5 = 30字节； 假设另一条Kafka消息未指定key，而value依然是“hello”，那么Kafka会往key长度字段中写入-1表明key是空，因而不再需要保存key信息，故总的消息长度= 22 + 5 = 27字节。V1的消息集合：offset：保存浅层消息的位移size：浅层消息的字节数V1消息集合长度举例： 计算消息集合大小，还是拿之前的两条Kafka消息为例。第一条消息被封装进一个消息集合，那么该消息集合总的长度 = 12 + 30 = 42字节，而包含第二条未指定key消息的消息集合总长度 = 12 + 27 = 39字节。此时，两条消息总共占81字节。 V2 Record &amp; RecordBatch 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051V2的消息格式：length:消息总长度。直接计算出消息的总长度并保存在第一个字段中，而不需要像v1版本时每次需要重新计算。这样做的好处在于提升解序列化的效率——拿到总长度后，Kafka可以直接new出一个等长度的ByteBuffer，然后装填各个字段。同时有了这个总长度，在遍历消息时可以实现快速地跳跃，省去了很多copy的工作。attributes: int8 bit 0~7: unusedtimestampDelta: varint 消息时间戳与所属record batch起始时间戳的差值，保存差值可以进一步节省消息字节数offsetDelta: varint 消息位移与所属record batch起始位移的差值，保存差值可以进一步节省消息字节数keyLength: varintkey: byte[]valueLen: varintvalue: byte[]Headers =&gt; [Header] headerKeyLength: varintheaderKey: StringheaderValueLength: varintValue: byte[]v2版本不在对每条消息执行CRC校验，而是针对整个batchv2版本的属性字节不再使用，原先保存在属性字段中的诸如压缩类型、时间戳类型等信息都统一保存在外层的batch中查看MemoryRecordsBuilder.writeDefaultBatchHeader()方法封装RecordBatch再调用DefaultRecordBatch.writeHeader()方法V2的消息长度举例： 假设这条Kafka消息的key是“key”，value是“hello”，同时假设这是batch中的第一条消息，因此时间戳增量和位移增量都是0，另外我们还假设没有指定任何header，因此header数组个数是0。结合上图我们可以计算这条消息的长度 = 总长度值占用的字节数 + 1 + 1 + 1 + 1 + 3 + 1 + 5 + 1 = 总长度值占用的字节数 + 14，由于14小于64，因此总长度值只需1个字节，故消息总长度是15字节。同时消息的第一个字节保存的值就是15。V2的消息集合：baseOffset: int64 起始位移batchLength: int32 RecordBatch总长度partitionLeaderEpoch: int32 分区leader版本号magic: int8 (current magic value is 2) 版本crc: int32 crc被移动到batch这一层，而非消息这一层attributes: int16 bit 0~2: 0: no compression 1: gzip 2: snappy 3: lz4 bit 3: timestampType bit 4: isTransactional (0 means not transactional) bit 5: isControlBatch (0 means not a control batch) bit 6~15: unusedlastOffsetDelta: int32 最大唯一增量firstTimestamp: int64 起始时间戳maxTimestamp: int64 最大时间戳producerId: int64 producerEpoch: int16baseSequence: int32records: [Record]V2消息集合长度举例： 和v2版本一样，我们来看下如何计算消息集合大小，还是以上面的两条Kafka消息为例。第一条消息被封装进一个batch，那么该batch总的长度 = 61 + 15 = 76字节。 二者比较从上面的情况来看，似乎V2占用的磁盘空间反而增加了，这是因为我们测试的时候，Producer向Kafka一次发送的batch中只包含一条消息。如果我们改用java API程序来批量发送消息的话，我们就会发现两者的不同。在未有任何调优的情况下，v2版本消息格式在一定程度上减少了网络IO和磁盘IO的开销。展示那两张性能对比图。 Producer客户端Producer java客户端实现： Producer客户端与broker端交互的协议格式： 请求数据从生产者发送到服务端的流转过程： Delay机制TimingWheelDelayQueue本质上是封装了一个PriorityQueue。PriorityQueue内部使用最小堆来实现排序队列。最小堆在插入和获取时，时间复杂度随着DelayQueue中的元素个数呈对数级别增长，都是O(logn)。Kafka这类分布式框架有大量延迟操作并且对性能要求高， DelayQueue不能满足Kafka的性能要求，因此Kafka字节实现了时间轮。 TimingWheel：底层使用数组实现，数组中的每个元素存放一个TimerTaskList对象，同一个TimerTaskList中的任务到期时间相近，但不一定相同。expiration记录整个TimerTaskList的超时时间，expirationMs记录TimerTask的超时时间戳。 TimingWheel时间格数据结构： 到期时间和bucket选择 1234virtualId = expiration/tickMsbucket = virtualId%wheelSize ----- 计算延时任务的桶号bucket.expiration = virtualId*tickMs ----- 计算桶的到期时间时间轮的expiration范围 = [currentTime+tickMs , currentTime+interval] 单层时间轮添加任务及执行过程 123外部的Purgatory添加任务，Repear线程轮询一次timeoutMs=200ms,加入A、B、C、D4个任务，到期时间戳：[A=0s,B=1s,C=1s,D=3s]，currentTime=0s，tickMs=1s，wheelSize=8，interval=8 多层时间轮的任务流转过程 123456789101112假设有两个时间轮，分别为Level0、Level1Level0 ： currentTime=Time0，tickMs=1s，wheelSize=8，interval=8sLevel1 ： currentTime=Time0，tickMs=8s，wheelSize=8，interval=64s加入了⑦⑧⑨三个任务，任务⑦在Level0的7-8之间，任务⑧⑨在Level1的8-16之间currentTime=Time7Level0 的指针指向了7~8之间，任务⑦到期Level1 的指针还是没有变化currentTime=Time8Level0 的指针移动到了8~9之间Level1 的指针移动到了8~16之间，任务⑧⑨到期，从Level1弹出，重新添加到低层级的更细粒度的Level0中执行 DelayedOperation组件DelayedOperationPurgatory组件实现： DelayedProduce： DelayedFetch： DelayedProduce和DelayedFetch之间的关联：在处理ProduceRequest过程中可能会向Log中添加数据，可能会后移Leader副本的LEO，Follower副本就可以读取到足量的数据（FetchRequest的min_bytes），所以会尝试DelayedFetch；在处理来自Follower副本的FetchRequest过程中，可能会后移HW，所以会尝试完成DelayedProduce（ProduceRequest的acks为-1），这样两者可以很好地协同工作了。 问题Kafka实现高吞吐的原因？ 读写文件依赖OS文件系统的页缓存，而不是在JVM内部缓存数据，利用OS来缓存，内存利用率高 sendfile技术（零拷贝），避免了传统网络IO四步流程 支持End-to-End的批量发送与压缩发送 顺序IO以及常量时间get、put消息，时间轮数据结构 Partition 可以很好的横向扩展和提供高并发处理 消息中间件的比较选型？ kafka中副本的概念？kafka副本如何进行leader选举？kafka中consumer group与consumer的关系？新加入一个consumer后，kafka内部如何做rebalance的？kafka的事务实现？Kafka怎样保证不重复消费消息？此问题其实等价于保证消息队列消费的幂等性 主要需要结合实际业务来操作: 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 Kafka怎样保证数据不丢失，不重复？ 关闭自动提交offset 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 Kafka多数据中心部署灾备？一个Kafka数据中心由于灾难性硬件故障、软件故障、停电 – 导致一个装有Apache Kafka集群的数据中心完全失效。不过，另一个数据中心的Kafka继续运行中，它已经拥有原始数据中心的数据副本，这些数据是从相同topic上复制过来的。客户端应用程序从故障集群切换到正在运行的集群，并根据在原始数据中心中停止的位置自动恢复在新数据中心的数据消费。企业最大限度地减少灾难导致的停机时间和数据丢失，并继续运行它们的任务关键型应用程序。 采取三项措施来进行灾难规划： 设计多数据中心解决方案 制定故障转移和故障恢复手册 多测试]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2020%2F02%2F10%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本文介绍Java开发中常见的设计模式。 参考：《设计模式之禅》 23种设计模式创造类单例模式 懒汉模式12345678910public class Singleton&#123; private static Singleton instance = null; private Singleton()&#123;&#125; public static Singleton newInstance()&#123; if(null == instance)&#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 饿汉模式1234567public class Singleton&#123; private static Singleton instance = new Singleton(); private Singleton()&#123;&#125; public static Singleton newInstance()&#123; return instance; &#125;&#125; 双重检测123456789101112131415public class Singleton &#123; private static volatile Singleton instance = null; private Singleton()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; // Single Checked synchronized (Singleton.class) &#123; if (instance == null) &#123; // Double checked instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 静态内部类123456789public class Singleton&#123; private static class SingletonHolder&#123; public static Singleton instance = new Singleton(); &#125; private Singleton()&#123;&#125; public static Singleton newInstance()&#123; return SingletonHolder.instance; &#125;&#125; 枚举123456789101112class Resource&#123;&#125;public enum SomeThing &#123; INSTANCE; private Resource instance; private SomeThing() &#123; instance = new Resource(); &#125; public Resource getInstance() &#123; return instance; &#125;&#125; 工厂方法模式 抽象工厂模式 模版方式模式 建造者模式 原型模式 迭代器模式 结构类代理模式 装饰者模式 适配器模式 组合模式 门面模式 享元模式 桥梁模式 行为类中介者模式 命令模式 责任链模式 策略模式 观察者模式 备忘录模式 访问者模式 状态模式 解释器模式 6大设计原则单一职责原则Single Responsibility Principle，SRP，应该有且只有一个原因引起类的变更。一个方法尽可能做一件事但是职责和变化原因是不可度量的，因项目而异，因环境而异。建议接口一定要做到单一职责，类的设计尽量做到只有一个原因引起变更。 类的复杂性降低，实现什么职责都有清晰明确的定义。可读性提高，可维护性提高。 里氏替换原则LiskovSubstitutionPrinciple，LSP，所有引用基类的地方必须能透明地使用其子类的对象。 最佳实践： 子类必须完全实现父类的方法 子类可以有自己的个性 覆盖或实现父类的方法时输入参数可以被放大，子类中方法的输入参数必须与父类中被覆写的方法的输入参数类型相同或者更宽松。 覆写或实现父类的方法时输出结果可以被缩小，父类的一个方法返回值是一个类型T，子类的相同方法（重载或覆写）的返回类型为S，要求S类型必须小于等于T类型。 依赖倒置原则DependenceInversionPrinciple，DIP，模块间的依赖通过抽象发生，实现类之间不发生直接的依赖关系，其依赖关系是通过接口或抽象类产生的；接口或抽象类不依赖于实现类；实现类依赖接口或抽象类。精简的定义就是“面向接口编程”。 构造函数传递依赖对象 1234567891011121314151617public interface IDriver&#123; void drive();&#125;public class Driver implements IDriver&#123; private ICar car; // 构造函数注入 public Driver(ICar car)&#123; this.car = car; &#125; @Override public void drive()&#123; this.car.run(); &#125;&#125; Setter方法传递依赖对象 1234567891011121314151617181920public interface IDriver&#123; // 接口声明依赖对象 void setCar(ICar car); void drive();&#125;public class Driver implements IDriver&#123; private ICar car; // Setter依赖注入 public void setCar(ICar car)&#123; this.car = car; &#125; @Override public void drive()&#123; this.car.run(); &#125;&#125; 最佳实践： 每个类都尽量都有接口或抽象类，或者抽象类或接口两者都具备。 变量的表面类型尽量是接口或者抽象类。 任何类都不应该从具体类派生。 尽量不要覆写基类的方法，如果基类是一个抽象类，而且这个方法已经实现了，子类尽量不要覆写。 结合里氏替换原则使用。 接口隔离原则实例接口（Object Interface），在Java中声明一个类，然后用new关键字产生一个实例，它是对一个类型事物的描述，这是一种接口。类接口（Class Interface），使用interface关键字定义的接口。 两种类型的美女定义： 12345678public interface IGoodBodyGirl&#123; void goodLooking(); void niceFigure();&#125;public interface IGreatTemperamentGirl&#123; void greatTemperament();&#125; 最标准的美女： 123456789101112131415161718192021public class PrettyGirl implements IGoodBodyGirl,IGreatTemperamentGirl&#123; private String name; public PrettyGirl(String name)&#123; this.name = name; &#125; @Override public void goodLooking()&#123; System.out.println("She has good looking."); &#125; @Override public void niceFigure()&#123; System.out.println("She has nice figure."); &#125; @Override public greatTemperament()&#123; System.out.println("She has great temperament."); &#125;&#125; 最佳实践： 一个接口只服务于一个子模块或业务逻辑，通过业务逻辑压缩接口中的public方法，接口时长去回顾，尽量让接口达到“满身筋骨肉”，而不是“肥嘟嘟”的一大堆方法。 已经被污染的接口，尽量去修改，若变更的风险较大，则采用适配器模式去转化处理。 迪米特法则LawofDemeter，LoD，一个类应该对自己需要耦合或调用的类知道的最少，你的内部是如何复杂都和我没关系，那是你的事情，我就知道你提供的那么多public方法，我就调用这么多，其他我一概不管。核心概念就是类间解藕，弱耦合，只有弱耦合了以后，类的复用率才可以提高。 开闭原则类、模块和函数应该对扩展开放，对修改关闭。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2020%2F02%2F10%2FRedis%2F</url>
    <content type="text"><![CDATA[本文将全局介绍 Redis 的整体架构。 问题缓存穿透缓存雪崩缓存击穿缓存与数据库双写一致性问题先更新数据库，再删除缓存（推荐） 通过读取binlog的方式，采用mq异步淘汰缓存，提供一个保障的重试机制!!!]]></content>
      <categories>
        <category>NoSql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2F2020%2F02%2F09%2FJVM%2F</url>
    <content type="text"><![CDATA[本文介绍JVM内存结构、类加载、GC、JVM调优等方面。《深入理解Java虚拟机: JVM高级特性与最佳实践》 JVM内存结构JVM = 类加载器(classloader) + 执行引擎(execution engine) + 运行时数据区域(runtime data area) 运行时数据区域Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些数据区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则是依赖用户新城的启动和结束而建立和销毁。 Java堆思考：为什么要使用分代模型？？？ 被所有线程共享，在虚拟机启动时创建，用来存放对象实例，几乎所有的对象实例都在这里分配内存。对于大多数应用来说，Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是垃圾收集器管理的主要区域，因此很多时候也被称为“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；新生代又有Eden空间、From Survivor空间、To Survivor空间三部分。Java堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出OOM异常。 控制参数 作用 -Xms 设置堆的最小空间大小 -Xmx 设置堆的最大空间大小 -XX:NewSize 设置新生代最小空间大小 -XX:MaxNewSize 设置新生代最大空间大小 -Xss 设置每个线程的堆栈大小 年轻代有将近98%的对象都是朝生夕死，所以针对这一情况，对象会在新生代Eden中进行分配，当Eden区没有足够空间的时候，虚拟机会触发Minor GC。Minor GC的回收速度很快，通过Minor GC后，Eden区会被清空。Eden区的绝大部分对象在这个时候都会被回收，剩下的那些无需被回收的对象会被放到Survior的from区，如果from区放不下就会直接被放到Old区。等到再次触发Minor GC后会将Eden区和from区存活的对象放到to区。同样的，如果to区放不下就会被放到Old区。Minor GC会将年轻代的存活的对象在from区和to区来回存放。在Survivor区中存活的对象，每经历一次Minor GC，对象的年龄就会加1，当长期存活的对象年龄达到一定数字 15 就会被移到老年代。另外还有一个机制，虚拟机并不一定要对象年龄达到 15 岁时才会放入老年代，如果Survivor区中相同年龄对象的大小的和大于Suvivor空间的一半，年龄大于等于改年龄的对象就可以直接进入老年代，无需等待“成年”，类似于负载均衡。 老年代老年代占据着，2/3的堆内存空间，只有在Major GC时才会被清理，每次Major GC都会出发”Stop The World”。内存越大，STW的时间就越长，所以内存也不是越大越好。 除了年轻代那里的对象会进入老年代以外，还有一种特殊情况：大对象。大对象是指大量连续内存空间的对象，这部分对象不管其生命周期有多短，都会直接进入老年代。这样做的目的是为了避免在Eden区和两个Survivor区发生大量的内存复制。 方法区用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。和Java堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出OOM异常。对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载。方法区逻辑上属于堆的一部分，为了与堆进行区分，通常又叫“非堆”。 运行时常量池Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池。运行时常量池是方法区的一部分。Class文件中的常量池（编译器生成的各种字面量和符号引用）会在类加载后被放入这个区域。除了在编译期生成的常量，还允许动态生成，例如String的intern（）。这部分常量也会被放入运行时常量池。 注：在JDK1.7之前， HotSpot 使用永久代实现方法区；HotSpot 使用GC分代实现方法区带来了很大便利；从JDK1.7开始 HotSpot 开始移除永久代。其中符号引用（Symbols）被移动到 Native Heap中，字符串常量和类引用被移动到 Java Heap 中。在JDK1.8中，永久代已经被完全被元空间（Metaspace）取代。元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 Java虚拟机栈线程私有，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈桢用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。局部变量表存放了编译器可知的各种基本数据类型（boolean、byte、char、short、int、float、double、long）、对象引用（reference），它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置和returnAddress的类型。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 局部变量表 - 存储方法参数，内部使用的变量 操作栈数 - 在变量进行存储时，需要进行入栈和出栈 动态链接 - 引用类型的指针 方法出口 - 方法的返回 一段源程序代码： 123456789101112public class Demo &#123; static int hello() &#123; int a = 1; int b = 2; int c = a + b; return c; &#125; public static void main(String[] args) &#123; System.out.println(hello()); &#125;&#125; 生成字节码文件： 1javac demo.java 对class文件反汇编： 1javap -c demo.class &gt; demo.txt 通过文件编译工具来查看demo.txt的内容： 12345678910111213141516171819202122232425262728Compiled from &quot;Demo.java&quot;public class Demo &#123; public Demo(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return static int hello(); Code: 0: iconst_1 // 把第一个整型变量推到操作数栈 1: istore_0 // 让它出栈，并存储到局部变量表 2: iconst_2 3: istore_1 4: iload_0 // 加载第一个数据入操作数栈 5: iload_1 // 加载第二个数据入操作数栈 6: iadd // 将两个数相加，结束入操作数栈 7: istore_2 // 将结果出栈，存到局部变量表 8: iload_2 // 加载它入操作数栈 9: ireturn // 返回 public static void main(java.lang.String[]); Code: 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: invokestatic #3 // Method hello:()I 6: invokevirtual #4 // Method java/io/PrintStream.println:(I)V 9: return&#125; 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出StackOverflowError异常 栈进行动态扩展时如果无法申请到足够内存，会抛出OutOfMemoryError异常 本地方法栈与虚拟机栈非常类似，虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。有的虚拟机（Sun Hotpot）直接把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 程序计数器线程私有，它的生命周期与线程相同。可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器的工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，如：分支、循环、跳转、跳转、异常处理、线程恢复等基础功能。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果执行的是Native方法，这个计数器值则为空（undefined）。程序计数器中存储的数据所占空间的大小不会随程序的执行而发生改变，所以此区域不会出现OOM的情况。 直接内存（Direct Memory）直接内存并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但是这部分内存也被频繁的使用，而且也可能导致OOM异常。在JDK1.4中新加入了NIO类，引入了一种基于 Channel 与 Buffer 的I/O方式，可以直接使用Native函数库分配堆外内存，然后通过一个存储在Java堆里的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存（包括RAM以及SWAP区或者分页文件）大小以及处理器寻址空间的限制，也会抛出OOM异常。 HotSpot虚拟机对象对象的创建 虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。 虚拟机将为新生对象分配内存：Serial、ParNew带Compact过程 -&gt; 绝对规整的内存 -&gt; “指针碰撞” Bump the pointerCMS Mark-Sweep -&gt; 不规整的内存 -&gt; “空闲列表” Free List并发创建对象如何保证线程安全：同步处理 -&gt; CAS + 失败重试线程划分 -&gt; TLAB 虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。 对象的内存布局在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（InstanceData）和对齐填充（Padding）。 实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。 对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpotVM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说，就是对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或者2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象的访问定位 使用句柄来访问的最大好处就是reference中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要修改。 使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本。 OOM java示例java堆溢出1234567891011121314/** * -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError */public class HeapOOM &#123; static class OOMObject&#123; &#125; public static void main(String[] args)&#123; List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while(true)&#123; list.add(new OOMObject()); &#125; &#125;&#125; 运行结果: 虚拟机栈和本地方法栈溢出在单个线程下，无论是由于栈帧太大还是虚拟机栈容量太小，当内存无法分配的时候，虚拟机抛出的都是StackOverflowError异常。 1234567891011121314151617181920/** * -Xss 128k */public class JavaVMStackSOF &#123; private int stackLength = 1; public void stackLeak()&#123; stackLength++; stackLeak(); &#125; public static void main(String[] args)&#123; JavaVMStackSOF oom = new JavaVMStackSOF(); try &#123; oom.stackLeak(); &#125;catch(Throwable e)&#123; System.out.println("stack length:"+oom.stackLength); throw e; &#125; &#125;&#125; 运行结果: 创建线程导致内存溢出异常，每个线程都会分配到一定的栈容量，每个线程分配到的栈容量越大，可以创建的线程数就越少，建立线程时就容易把剩下的内存耗尽 123456789101112131415161718192021222324252627/** * -Xss 2m */public class JavaVMStackOOM &#123; private void dontstop()&#123; while(true)&#123; &#125; &#125; public void stackLeakByThread()&#123; while(true)&#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; dontstop(); &#125; &#125;); thread.start(); &#125; &#125; public static void main(String[] args) throws Throwable&#123; JavaVMStackOOM oom = new JavaVMStackOOM(); oom.stackLeakByThread(); &#125;&#125; 运行结果: 方法区和运行时常量池溢出借助CGLib直接操作字节码运行生成大量的动态类，撑爆方法区 123456789101112131415161718192021222324/** * JDK7：-XX:PermSize=10M -XX:MaxPermSize=10M * JDK8：-XX:MetaspaceSize=10M -XX:MaxMetaspaceSize=10M */public class JavaMethodAreaOOM &#123; public static void main(String[] args) &#123; while (true) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; return methodProxy.invoke(o, objects); &#125; &#125;); enhancer.create(); &#125; &#125; static class OOMObject &#123; &#125;&#125; 运行结果: GC算法 垃圾收集器对象存活判断可达性分析：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。 GC Roots包括： 虚拟机栈引用的对象 方法区中类静态属性实体引用的对象 方法区中常量引用的对象 本地方法栈中JNI引用的对象 垃圾回收算法 复制：将内存缩小为原来的一半，代价略高。YGC 标记-清除：标记和清除的两个过程效率都不高；产生大量不连续的内存碎片。OGC 标记整理：如果不想浪费50%的空间，又不想要内存碎片。OGC 分代收集算法根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾回收都发现只有少量对象死去，选用复制算法，只需要付出少量存活对象的肤质成本就可以完成收集。而老年代中因为对象存活率高，没有额外空间对它进行分配担保，就必须使用标记清除或标记整理来回收。 垃圾回收器分类标准 分类标准 描述 线程数 串行和并行，串行一次只使用一个线程进行垃圾回收，并行一次将开启多个线程同时进行垃圾回收。在并行能力较强的CPU上，使用并行垃圾回收器可以缩短GC的停顿时间 工作模式 并发式和独占式，并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间，独占式垃圾回收器(Stop the world)一旦运行，就停止应用程序的其他所有线程 碎片处理方式 压缩式和非压缩式，压缩式垃圾回收器在回收完成后，对存活对象进行压缩整理，消除回收后的碎片 工作的内存区间 新生代垃圾回收器和老年代垃圾回收器 Serial - 新生代串行收集器 使用单线程进行垃圾回收，独占式，使用-XX:+UseSerialGC指定使用，JVM在Client模式下默认垃圾收集器 复制算法 优点：实现简单，处理高效缺点：Stop The World [GC (Allocation Failure) [DefNew: 1937K-&gt;320K(3072K), 0.0021596 secs] 1937K-&gt;1645K(9920K), 0.0023185 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] ParNew - 新生代并行收集器 实现和Serial相同，仅将GC线程改成多线程，使用-XX:+UseParNewGC指定使用 复制算法 优点：在多CPU情况下优于Serial缺点：Stop The World GC线程数， -XX:+ParallelGCThreads当CPU个数&lt;8，ParallelGCThreads = CPU个数当CPU个数&gt;8，ParallelGCThreads = 3 + （（5*CPU个数/8）） [GC (Allocation Failure) [ParNew: 1937K-&gt;320K(3072K), 0.0023399 secs] 1937K-&gt;1656K(9920K), 0.0025229 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Parallel Scavenge - 新生代并行收集器缺点：Stop The World 线程模型和ParNew相同，区别在于Parallel Scavenge收集器的目标是达到一个可以控制的吞吐量 复制算法 1234-XX:+UseParallelGC，指定使用 -XX:MaxGCPauseMillis，设置最大停顿时间，大于0的整数 -XX:GCTimeRatio，设置吞吐量大小，0~100整数，即运行用户代码时间 / 垃圾收集时间，默认值为99 -XX:+UseAdaptiveSizePolicy 自适应GC策略开关，在自适应模式下，新生代大小，survivor和eden区的比例、晋升老年代对象年龄等参数会被自动调整，以达到最合适的停顿时间，或最大吞吐量 [GC (Allocation Failure) –[PSYoungGen: 1998K-&gt;1998K(2560K)] 8142K-&gt; Serial Old（MSC） - 老年代串行收集器缺点：Stop The World 使用标记-整理算法，和Serial一样是串行独占式回收器，可以和Serial、ParNew搭配使用。缺点是停顿时间可能会比较长。 [Full GC (Allocation Failure) [Tenured: 33389K-&gt;33377K(34176K), 0.0128073 secs] 47726K-&gt;47714K(49536K), [Metaspace: 2501K-&gt;2501K(1056768K)], 0.0136535 secs] [Times: user =0.02 sys=0.00, real=0.01 secs] Parallel Old - 老年代并行收集器缺点：Stop The World 使用标记-整理算法，和Parallel Scavenge一样是并行多线程收集器，关注于吞吐量和CPU资源敏感的场合。-XX:+UseParallelOldGC缺点是停顿时间可能会比较长。 [Full GC (Ergonomics) [PSYoungGen: 1957K-&gt;1526K(2560K)] [ParOldGen: 6248K-&gt;6261K(7168K)] 8205K&gt;7788K(9728K), [Metaspace: 2496K-&gt;2496K(1056768K)], 0.0072201 secs] [Times : user=0.01 sys=0.00, real=0.01 secs] CMS - 老年代并发收集器 全称：Concurrent Mark Sweep基于标记-清除算法一种以获取最短停顿时间为目标的收集器。CMS并非没有暂停，而是用两次短暂停来替代串行标记整理算法的长暂停，它的收集周期如下： 初始标记Stop The World仅仅标记GC Roots内直接关联的对象。这个阶段速度很快。 并发标记进行GC Tracing，从GC Roots开始对堆进行可达性分析，找出存活对象。 重新标记修正并发期间由于用户进行运作导致的标记变动的那一部分对象的标记记录。这个阶段的停顿时间一般会比初始标记阶段稍长点，但远比并发标记的时间短。 并发清除清除垃圾对象，标记-清除算法。 并发重置状态等待下一次CMS的触发 在整个过程中，CMS 回收器的内存回收基本上和用户线程并发执行。 缺点： 对CPU资源非常依赖。过分依赖于多线程环境，默认开启的线程数为(CPU的数量*3)/4，当CPU的数量小于4时，CMS对用户查询的影响将会很大，因为他们要分出一半的运算能力去执行回收器线程。 无法清除浮动垃圾。CMS并发清除阶段清除已标记的垃圾，但用户线程还在运行，因此会有新的垃圾产生，但是这部分垃圾未被标记，在下一次GC才能清除，因此被称为浮动垃圾。 垃圾收集结束后残余大量空间碎片。因为采用的标记-清除算法。 G1 - 整堆回收器JDK1.7中正式投入使用，用于取代 CMS 的压缩回收器。 G1 首先将 堆 分为 大小相等 的 Region，避免 全区域 的垃圾回收。然后追踪每个 Region 垃圾 堆积的价值大小，在后台维护一个 优先列表，根据允许的回收时间优先回收价值最大的 Region。同时 G1采用 Remembered Set 来存放 Region 之间的 对象引用 ，其他回收器中的 新生代 与 老年代 之间的对象引用，从而避免 全堆扫描。 初始标记Stop The World仅仅标记GC Roots内直接关联的对象。这个阶段速度很快。 并发标记进行GC Tracing，从GC Roots开始对堆进行可达性分析，找出存活对象。 重新标记修正并发期间由于用户进行运作导致的标记变动的那一部分对象的标记记录。这个阶段的停顿时间一般会比初始标记阶段稍长点，但远比并发标记的时间短。 筛选回收首先对各个 Region 的回收价值和成本进行排序，根据用户所期望的 GC停顿时间来制定回收计划。这个阶段可以与用户程序并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高回收效率。 与其他 GC回收相比，G1具备以下4个特点： 并行与并发使用多个 CPU 来缩短 Stop-the-World 的 停顿时间，部分其他回收器需要停顿 Java 线程执行的 GC 动作，G1 回收器仍然可以通过 并发的方式 让 Java 程序继续执行。 分代回收与其他回收器一样，分代概念 在 G1 中依然得以保留。虽然 G1 可以不需要 其他回收器配合 就能独立管理 整个GC堆，但它能够采用 不同的策略 去处理 新创建的对象 和 已经存活 一段时间、熬过多次 GC 的旧对象，以获取更好的回收效果。新生代 和 老年代 不再是 物理隔离，是多个 大小相等 的独立 Region。 空间整合与 CMS 的 标记—清理 算法不同，G1 从 整体 来看是基于 标记—整理 算法实现的回收器。从 局部（两个 Region 之间）上来看是基于 复制算法 实现的。但无论如何，这 两种算法 都意味着 G1 运作期间 不会产生内存空间碎片，回收后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象 时不会因为无法找到 连续内存空间 而提前触发 下一次 GC。 可预测的停顿这是 G1 相对于 CMS 的另一大优势，降低停顿时间 是 G1 和 CMS 共同的关注点。G1 除了追求 低停顿 外，还能建立 可预测 的 停顿时间模型，能让使用者明确指定在一个 长度 为 M 毫秒的 时间片段 内，消耗在 垃圾回收 上的时间不得超过 N 毫秒。（后台维护的 优先列表，优先回收 价值大 的 Region）。 类加载机制什么是类的加载类的加载是指将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的借口。 类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError），如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误。 类的生命周期 加载 通过一个类的全限定名来获取此定义此类的二进制字节流 将这个字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表此类的java.lang.Class对象，作为方法区这个类的各种数据结构的访问入口 加载.class文件的方式： 从本地系统中直接加载 通过网络下载.class文件 从zip,jar等归档文件中加载.class文件 从专有数据库中提取.class文件 从Java源文件动态编译为.class文件 验证目的在于确保class文件的字节流中包含的信息符合当前虚拟机的要求，不会危害虚拟机自身安全。 文件格式验证：验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理。 元数据验证：对字节码描述的信息进行语义分析，以保证其描述的信息符合Java语言规范的要求。是否有父类（除了java.lang.Object都应该有父类），是否继承了被final修饰的类，是否实现了接口中的所有方法。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：符号引用中通过字符串描述的全限定名是否能找到对应的类；符号引用中通过字符串描述的全限定名是否能找到对应的类；符号引用中的类、字段、方法的访问性（private、protected、public、default）是否可被当前类访问。符号引用中的类、字段、方法的访问性（private、protected、public、default）是否可被当前类访问。 准备进行内存分配，仅包括类变量（被static修饰的变量），而不包括实例变量。类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中。 1public static int value=123； 变量value在准备阶段过后的初始值为0而不是123，因为这时候尚未开始执行任何Java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器＜clinit＞()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行。 而用final修饰的static，在编译的时候就会分配了。 1public static final int value=123； 编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据ConstantValue的设置将value赋值为123。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，符号引用在前一章讲解Class文件格式的时候已经出现过多次，在Class文件中它以CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等类型的常量出现，就是一组符号来描述目标，可以是任何字面量。而直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 类或接口解析 字段解析 类方法解析 接口方法解析 初始化到了初始化阶段，才真正执行类中定义的Java代码。初始化阶段是执行类构造器＜clinit＞()方法的过程。类加载的最后阶段，若该类有超类，则对其进行初始化，执行静态初始化器和静态初始化成员变量（如前面只初始化了默认值的static变量将会在这个阶段赋值，成员变量也将被初始化）。 初始化类中的静态变量，并执行类中的static代码、构造函数。JVM规范严格定义了何时需要对类进行初始化： 通过new 关键字、反射、clone、反序列化机制实例化对象时 调用类的静态方法时 使用类的静态字段或对其赋值时 通过反射调用类的方法时 初始化该类的子类时（初始化子类前其父类必须已经被初始化） JVM启动时被标记为启动类的类（简单理解为具有main方法的类） 类加载器类加载器的任务是根据一个类的全限定名来读取此类的二进制字节流到JVM中，然后转换为一个与目标类对应的java.lang.Class对象实例。 站在Java虚拟机的角度，只存在两种不同的类加载器: 启动类加载器，是虚拟机自身的一部分；所有其它的类加载器，独立于虚拟机之外，并且全部继承自java.lang.ClassLoader，这些类加载器需要由启动类加载器加载到内存中之后才能去加载器他的类。 启动类加载器 负责加载存放在 JDK\jre\lib 下，或被 -Xbootclasspath 参数指定的路径下的，并且能够被虚拟机识别的类库（如rt.jar，所有java.开头的类均被Bootstrap ClassLoader加载）。启动类加载器是无法被Java程序直接引用的，主要加载的是JVM自身需要的类，这个类加载使用C++语言实现的， 扩展类加载器 该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载JDK\jre\lib\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.开头的类），开发者可以直接使用扩展类加载器。 应用程序类加载器 该加载器由sun.misc.Launcher$AppClassLoader实现，它负责加载用户类路径 java -classpath 或 -Djava.class.path所指定的类库，也就是我们经常用到的classpath路径。开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。通过 ClassLoader#getSystemClassLoader() 可以直接获取到该类加载器。 类加载的几种方式： 命令行启动应用时由JVM初始化加载 Class.forName()方式动态加载，将类.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块 ClassLoader.loadClass()方法动态加载，就是将.class文件中加载到jvm中，不会执行static中的内容，只有在newInstance才会去执行static块 Class.forName(name,intialize,loader)，带参函数也可以控制是否加载static块，并且只有调用了newInstance()方法才调用构造函数，创建类的对象 双亲委派模型如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父类加载器去完成，依次向上，因此，所有类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 当 AppClassLoader 加载一个class时，它首先不会自己去尝试加载这个类，而是被类加载请求委派给父类加载器 ExtClassLoader 去完成。当 ExtClassLoader 加载一个class时，它首先不会自己去尝试加载这个类，而是被类加载请求委派给父类加载器 BootstrapClassLoader 去完成。如果 BootstrapClassLoader 加载失败，会使用 ExtClassLoader 来尝试加载如果 ExtClassLoader 也加载失败，则会使用 AppClassLoader 来加载，如果 AppClassLoader 也加载失败，则会报出异常 ClassNotFoundException 双亲委派模型的意义：系统类防止内存中出现多份同样的字节码，避免类的重复加载保证Java程序安全稳定运行，Java核心api中定义类型不会被随意替换 JVM调优命令jstat：虚拟机统计信息监视工具123jstat -gcutil pid [interval] [count]jstat -gcutil 2764 250 20 jinfo: Java配置信息工具123jinfo -heap pidjinfo -flag CMSInitiatingOccupancyFraction pid : 查询参数值，-XX:CMSInitiatingOccupancyFraction=85 jmap：Java内存映像工具123jmap -histo:live pidjmap -dump:live,format=b,file=dump.hprof pid jhat: 虚拟机堆转储快照工具1jhat dump.hprof 屏幕显示”Serverisready.”的提示后，用户在浏览器中键入http://localhost:7000/就可以看到分析结果。 jstack: Java堆栈跟踪工具1jstack -l pid 工具JConsole VisualVM MAT GC参数调优 排除Cache内容过多的问题，如果Cache内容过多会导致JVM老年代容易被用满导致频繁GC，使用jstat命令查看GC情况 调整GC时间点如果GC需要处理的内存量比较大，执行的时间也就比较长，STW时间也就更长。按照这个思路调整CMS启动的时间点，希望提早GC，也就是让GC变得更加频繁但是期望每次执行的时间较少。 -XX:+UseCMSInitiatingOccupancyOnly-XX:+CMSInitiatingOccupancyFraction=50 在Old区使用了50%的时候触发GC，实验后发现GC的频率有所增加。 调整对象在年轻代中驻留的时间，尝试提升这些对象在年轻代GC时被销毁的概率，-XX:MaxTenuringThreshold=31 Full GC之前先再进行一次YGC -XX:+ScavengeBeforeFullGC-XX:+CMSScavengeBeforeRemark Young区对象引用了Old区的对象，如果Old区进行清理之前不进行Young区清理就会导致Old区被Young区引用的对象无法释放]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Netty内存池化管理]]></title>
    <url>%2F2020%2F02%2F09%2FNetty%E5%86%85%E5%AD%98%E6%B1%A0%E5%8C%96%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文介绍Netty内存池化管理。 Netty内存池化管理参考：一文读懂Netty的内存管理 PooledBufferAllocatorPoolThreadCacheNetty自己实现了类似LocalThread的类来充当线程缓存 PoolThreadLocalCache 继承自 FastThreadLocal JEMalloc分配算法JEMalloc分配算法 PoolArenaNetty内存主要分为两种：DirectByteBuf 和 HeapByteBuf。Netty作为服务器架构技术，拥有大量的网络数据传输，当我们进行网络传输时，必须将数据拷贝至直接内存，合理利用好直接内存，能够显著提高性能。 Pool 和 Unpool的区别 池化内存的管理方式是首先申请一大块内存，当使用完成释放后，再将该部分内存放入池子中，等待下一次的使用，这样的话，可以减少垃圾回收的次数，提高处理性能。非池化内存就是普通的内存使用，需要时直接申请，释放时直接释放。目前netty针对pool做了大量的支持，这样内存使用直接交给了netty管理，减轻了直接内存回收的压力。 这样的话，内存分为4种： PoolDireBuf、UnpoolDireBuf、PoolHeapBuf、UnpoolHeapBuf。Netty底层默认使用PoolDireBuf类型的内存，这些内存主要由PoolArena管理。 PoolArena PoolArena作为Netty底层内存池核心管理类，主要原理是首先申请一些内存块，不同的成员变量来完成不同大小的内存块分配。下图描述了PoolArena最重要的成员变量： Tiny解决 16b～498b 之间的内存分配，Small解决 512b~4kb 的内存分配，Normal解决 8kb～16mb 的内存分配。 PoolArena的内存分配 线程分配内存主要从两个地方分配：PoolThreadCache 和 PoolArena 其中 PoolThreadCache 线程独享，PoolArena为几个线程共享。 Netty真正申请内存时的调用过程： PoolArena.allocate() 分配内存主要考虑先尝试从缓存中，然后再尝试从PoolArena分配。Tiny 和 Small 的申请过程一样，以Tiny申请为例，具体过程如下： 1）对申请的内存进行规范化，就是说只能申请某些固定大小的内存，比如Tiny范围的是16b倍数的内存，Small为512b、1k、2k、4k 的内存，Normal为8k、16k … 16m 范围的内存，始终是2的幂次方。申请的内存不足16b的，按照16b去申请。 2) 判断是否是小于8k的内存申请，若是申请Tiny|Small级别的内存： 首先尝试从cache中申请，申请不到的话，接着会尝试从 tinySubPagePools 中申请，首先计算出该内存在 tinySubPagePools 中对应的下标。 检查对应链串是否已经有PoolSubpage可用, 若有的话, 直接进入PoolSubpage.allocate进行内存分配 若没有可分配的内存, 则会进入allocateNormal进行分配 3）若分配normal类型的类型, 首先也会尝试从缓存中分配, 然后再考虑从allocateNormal进行内存分配。 4）若分配大于16m的内存, 则直接通过allocateHuge()从内存池外分配内存。 PoolChunkList对于在q050、q025、q000、qInit、q075这些PoolChunkList里申请内存的流程图如下： 按照以上顺序，这样安排的考虑是： 将PoolChunk分配维持在较高的比例上 保存一些空闲较大的内存，以便大内存的分配 PoolChunkPoolSubpageNetty中大于8k的内存都是通过PoolChunk来分配的，小于8k的内存是通过PoolSubpage分配的。当申请小于8k的内存时，会分配一个8k的叶子节点，若用不完的话，存在很大的浪费，所以通过 双向链表 添加节点： 1234567private void addToPool(PoolSubpage&lt;T&gt; head) &#123; assert prev == null &amp;&amp; next == null; prev = head; next = head.next; next.prev = this; head.next = this;&#125; 双向列表的插入： 第一步：首先找到插入位置，节点 s 将插入到节点 p 之前第二步：将节点 s 的前驱指向节点 p 的前驱，即 s-&gt;prior = p-&gt;prior;第三步：将节点 p 的前驱的后继指向节点 s 即 p-&gt;prior-&gt;next = s;第四步：将节点 s 的后继指向节点 p 即 s-&gt;next = p;第五步：将节点 p 的前驱指向节点 s 即 p-&gt;prior = s; 移除节点： 1234567private void removeFromPool() &#123; assert prev != null &amp;&amp; next != null; prev.next = next; next.prev = prev; next = null; prev = null;&#125; 双向列表的删除： 第一步：找到即将被删除的节点 p第二步：将 p 的前驱的后继指向 p 的后继，即 p-&gt;prior-&gt;next = p-&gt;next;第三步：将 p 的后继的前驱指向 p 的前驱，即 p-&gt;next-&gt;prior = p-&gt;prior;第四步：删除节点 p 即 delete p; PoolSubpage 管理8k的内存，如下图： 每一个PoolSubpage都会与PoolChunk里面的一个叶子节点映射起来。 1.首次请求Arena分配，Arena中的双向链表为空，不能分配； 2.传递给Chunk分配，Chunk找到一个空闲的Page，然后均等切分并加入到Arena链表中，最后分配满足要求的大小。之后请求分配同样大小的内存，则直接在Arena中的PoolSubpage双向链表进行分配；如果链表中的节点都没有空间分配，则重复1步骤。 Netty使用一个long整型表示在 PoolSubpage 中的分配结果，高32位表示均等切分小块的块号，其中的低6位用来表示64位即一个long的分配信息，其余位用来表示long数组的索引。低32位表示所属Chunk号。 以下是PoolSubpage的init及allocate流程图: DirectByteBuffer我们知道, 在使用IO传输数据时, 首先会将数据传输到堆外直接内存中, 然后才通过网络发送出去。这样的话, 数据多了次中间copy, 能否不经过copy而直接将数据发送出去呢, 其实是可以的, 存放的位置就是本文要讲的主角:DirectByteBuffer 。 JVM内存主要分为heap内存和堆外内存(一般我们也会称呼为直接内存), heap内存我们不用care, jvm能自动帮我们管理, 而堆外内存回收不受JVM GC控制, 因此, 堆外内存使用必须小心。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Netty线程模型]]></title>
    <url>%2F2020%2F02%2F09%2FNetty%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本文介绍Netty的线程模型以及服务端、客户端启动、客户端接入等流程。 Netty Reactor![](Netty Reactor工作架构图.png) ServerBootstrap启动流程 Client接入流程 Bootstrap启动流程 TCP粘包/拆包问题TCP粘包/拆包的基础知识TCP是一个”流”协议，在业务上认为，一个完整的包可能会被拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包/拆包问题。 由于底层的TCP协议无法理解上层的业务数据，所以在底层不能保证数据包不被拆分和重组，这个问题只能通过上层的应用协议栈设计来解决： 消息定长，不够补空格 在包尾增加回车换行符进行分割 将消息分为消息头和消息体，消息头中包含消息总长度或消息体长度的字段 更复杂的应用层协议 没考虑TCP粘包/拆包的问题案例TCP粘包导致的读半包问题查看example模块中的\ TimeServerTcpStickyException 和 TimeClientTcpStickyException: 服务端只收到2条消息，说明客户端发送的消息发生了TCP粘包： 服务端只收到2条消息，因此只发送2条应答，但实际上客户端值收到一条包含2个”BAD ORDER”的消息，说明服务端返回的应答消息也发生了TCP粘包： 使用Netty解决读半包问题为了解决TCP粘包/拆包导致的问题，Netty默认提供了多种编解码器用于处理半包。 查看example模块中的 TimeServerFixTcpStickyException 和 TimeClientFixTcpStickyException: 分别在服务端和客户端添加 LineBasedFrameDecoder 和 StringDecoder 解决问题。 服务端正常收到客户端的100次请求： 客户端正常收到服务端的100次应答消息： 问题Netty的消息可靠性机制 网络通信类故障 客户端指定连接超时时间 TCP心跳机制 故障定制：客户端的断连重连机制，消息的缓存重发，接口日志中详细记录故障细节，运维相关功能，例如告警、触发邮件/短信等 select、poll 与 epoll 的区别IO多路复用：I/O是指网络I/O,多路指多个TCP连接(即socket或者channel）,复用指复用一个或几个线程。意思说一个或一组线程处理多个TCP连接。最大优势是减少系统开销小，不必创建过多的进程/线程，也不必维护这些进程/线程。 IO多路复用使用两个系统调用(select/poll/epoll和recvfrom)，blocking IO只调用了recvfrom；select/poll/epoll 核心是可以同时处理多个connection，而不是更快，所以连接数不高的话，性能不一定比多线程+阻塞IO好,多路复用模型中，每一个socket，设置为non-blocking,阻塞是被select这个函数block，而不是被socket阻塞的。 select机制基本原理： 客户端操作服务器时就会产生这三种文件描述符(简称fd)：writefds(写)、readfds(读)、和exceptfds(异常)。select会阻塞住监视3类文件描述符，等有数据、可读、可写、出异常 或超时、就会返回；返回后通过遍历fdset整个数组来找到就绪的描述符fd，然后进行对应的IO操作。一个连接对应一个fd。优点： 几乎在所有的平台上支持，跨平台支持性好缺点： 由于是采用轮询方式全盘扫描，会随着文件描述符FD数量增多而性能下降。 每次调用 select()，需要把 fd 集合从用户态拷贝到内核态，并进行遍历(消息传递都是从内核到用户空间) 默认单个进程打开的FD有限制是1024个，可修改宏定义，但是效率仍然慢。 select的调用过程如下： 123456789101112131415（1）使用copy_from_user从用户空间拷贝fd_set到内核空间（2）注册回调函数__pollwait（3）遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。（5）__pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk-&gt;sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。（8）把fd_set从内核空间拷贝到用户空间。 poll机制 基本原理与select一致，也是轮询+遍历；唯一的区别就是poll没有最大文件描述符限制（使用链表的方式存储fd），使用 pollfd 结构而不是 select 的 fd_set 结构。 epoll机制基本原理： 没有fd个数限制，用户态拷贝到内核态只需要一次，使用时间通知机制来触发。通过epoll_ctl注册fd，一旦fd就绪就会通过callback回调机制来激活对应fd，进行相关的io操作。epoll之所以高性能是得益于它的三个函数 1)epoll_create()系统启动时，在Linux内核里面申请一个B+树结构文件系统，返回epoll对象，也是一个fd 2)epoll_ctl() 每新建一个连接，都通过该函数操作epoll对象，在这个对象里面修改添加删除对应的链接fd, 绑定一个callback函数 3)epoll_wait() 轮训所有的callback集合，并完成对应的IO操作优点： 没fd这个限制，所支持的FD上限是操作系统的最大文件句柄数，1G内存大概支持10万个句柄 效率提高，使用回调通知而不是轮询的方式，不会随着FD数目的增加效率下降 内核和用户空间mmap同一块内存实现(mmap是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间) 例子：100万个连接，里面有1万个连接是活跃，我们可以对比 select、poll、epoll 的性能表现 select： 不修改宏定义默认是1024,l则需要100w/1024=977个进程才可以支持 100万连接，会使得CPU性能特别的差。 poll： 没有最大文件描述符限制,100万个链接则需要100w个fd，遍历都响应不过来了，还有空间的拷贝消耗大量的资源。 epoll: 请求进来时就创建fd并绑定一个callback，主需要遍历1w个活跃连接的callback即可，即高效又不用内存拷贝。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Monitor with Prometheus And Grafana]]></title>
    <url>%2F2020%2F02%2F04%2FMonitor-with-Prometheus-And-Grafana%2F</url>
    <content type="text"><![CDATA[Prometheus 是一套开源的系统监控、报警、时间序列数据库的组合。Prometheus 基本原理是通过Http协议周期性抓取被监控组件的状态，而输出这些被监控的组件的Http接口为 Exporter。Grafana 是一个可视化仪表盘，它拥有美观的图标和布局展示，默认支持 CloudWatch、Graphite、ES、InfluxDB、Mysql、PostgreSQL、Prometheus、OpenTSDB等作为数据源。我们可以将 Prometheus抓取的数据，通过 Grafana 优美直观的展示出来。 Prometheus安装prometheus下载地址：Prometheus 12345$ cd /usr/local$ tar -zxvf prometheus-2.12.0.darwin-amd64.tar.gz$ ln -s prometheus-2.12.0.darwin-amd64 prometheus $ cd prometheus$ ./prometheus premethus启动初始界面如下： mysql-exportermysql-exporter下载地址: mysql-exporter 准备连接mysql的配置文件my.cnf: 12345[client]host=localhostport=3306user=rootpassword=123456 12345$ cd /usr/local/prometheus_exporter$ tar -zxvf mysqld_exporter-0.12.1.darwin-amd64.tar.gz$ ln -s mysqld_exporter-0.12.1.darwin-amd64 mysqld_exporter$ cd mysqld_exporter$ ./mysqld_exporter --config.my-cnf ./my.cnf 在/usr/local/prometheus/prometheus.yml中添加： 123- job_name: &apos;mysql&apos; static_configs: - targets: [&apos;localhost:9104&apos;] 命令行启动mysql-exporter如下： 打开http://localhost:9104/metrics, 可以看到返回了mysql的瞬时metrics: node-exporternode-exporter下载地址： node-exporter 12345$ cd /usr/local/prometheus_exporter$ tar -zxvf node_exporter-0.18.1.darwin-amd64.tar.gz$ ln -s node_exporter-0.18.1.darwin-amd64 node_exporter$ cd node_exporter$ ./node_exporter 在/usr/local/prometheus/prometheus.yml中添加： 123- job_name: &apos;node&apos; static_configs: - targets: [&apos;localhost:9100&apos;] 命令行启动node-exporter如下： Prometheus界面搜索‘node_load1’，简单的图形化展示: pushgatewaypushgateway下载地址：pushgateway 12345$ cd /usr/local$ tar -zxvf pushgateway-0.9.1.darwin-amd64.tar.gz$ ln -s pushgateway-0.9.1.darwin-amd64 pushgateway$ cd pushgateway$ ./pushgateway 在/usr/local/prometheus/prometheus.yml中添加： 12345- job_name: &apos;pushgateway&apos; static_configs: - targets: [&apos;localhost:9091&apos;] labels: instances: pushgateway 命令行启动pushgateway如下： Prometheus上展示的targets如下: 其他exporterkafka-exporter git地址：Kafka-exporterjmx-exporter git地址：JMX-exporter Grafana安装123456$ cd /usr/local$ wget https://dl.grafana.com/oss/release/grafana-6.3.5.darwin-amd64.tar.gz$ tar -zxvf grafana-6.3.5.darwin-amd64.tar.gz$ ln -s grafana-6.3.5.darwin-amd64 grafana$ cd /usr/local/grafna/bin$ ./grafana-server 命令行启动grafana如下： grafana启动初始界面如下： Grafana-dashboards开源的grafana-dashboards模板： 123git clone https://github.com/percona/grafana-dashboards.gitgit clone https://github.com/finn-no/grafana-dashboards.gitgit clone https://github.com/rfrail3/grafana-dashboards.git 在grafana界面添加premethus数据源： 上传dashboard的开源json模板，从上方的git工程中获取： mysql监控的dashboard如下： node监控的dashboard如下： Flink metrics1.7.2基于flink 1.7.2版本做监控试验。 1.需要在 /usr/local/flink-1.7.2/lib 下添加 flink-metrics-prometheus_2.11-1.7.2.jar 2.修改flink配置文件: /usr/local/flink-1.7.2/conf/flink-conf.yaml： 123456metrics.reporter.promgateway.class: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReportermetrics.reporter.promgateway.host: localhostmetrics.reporter.promgateway.port: 9091metrics.reporter.promgateway.jobName: myJobmetrics.reporter.promgateway.randomJobNameSuffix: truemetrics.reporter.promgateway.deleteOnShutdown: false 上传一个flink任务jar包： 在Grafana界面上传flink metrics的模板json：Flink Metrics JSON 界面上选择指定的source、sink，即可显示出输入输出records： 1.9.0Grafana可以展示以下几种监控界面：]]></content>
      <categories>
        <category>Monitor</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[行式存储与列式存储的区别]]></title>
    <url>%2F2020%2F01%2F20%2F%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[XXX 我们知道，当今的数据处理大致可分为两大类，联机事务处理OLTP（on-line transaction processing） 和联机分析处理OLAP（on-line analytical processing). OLTP与OLAP的区别OLTP是传统关系型数据库的主要应用，用来执行一些基本的、日常的事务处理，比如数据库记录的增、删、改、查等等。而OLAP则是分布式数据库的主要应用，它对实时性要求不高，但处理的数据量大，通常应用于复杂的动态报表系统上。 OLTP与OLAP的主要区别：| 数据处理类型 | OLTP | OLAP || :——–: | :—-: | :—: || 主要的面向对象 | 业务开发人员 | 分析决策人员 || 功能实现 | 日常事务处理| 面向分析决策 || 数据模型 | 关系模型 | 多维模型 || 处理的数据量 | 通常为几条或几十条记录 | 通常达到百万千万条记录 || 操作类型 | 查询、插入、更新、删除 | 查询为主 | 行式存储与列式存储传统的关系型数据库采用行式存储法（Row-based），一行中的数据在存储介质中以连续存储形式存在。 列式存储（Column-based）是相对于行式存储来说的，新兴的HBase，GP等分布式数据库均采用列式存储，一列中的数据在存储介质中以连续存储形式存在。 行式存储的适用场景 适合随机的增删改查操作 需要在行中选取所有属性的查询操作 需要频繁插入或更新的操作，其操作与索引和行的大小更为相关 实操中我们会发现，行式数据库在读取数据的时候，会存在一个固有的“缺陷”。比如所选择查询的目标即使只涉及少数几项属性，但由于这些目标数据埋藏在各行数据单元中，而行单元往往又特别大，应用程序必须读取每一条完整的行记录，从而使得读取效率大大降低。对此，行式数据库给出的优化方案是加“索引”。在OLTP类型的应用中，通过索引机制或给表分区等手段，可以简化查询操作步骤，并提升查询效率。 列式存储的适用场景但针对海量数据背景的OLAP应用，(例如分布式数据库、数据仓库等等)，行式存储的数据库就有些“力不从心”了。行式数据库建立索引和物化视图，需要花费大量时间和资源，因此还是得不偿失，无法从根本上解决查询性能和维护成本等问题。也不适用于数据仓库等应用场景，所以后来出现了基于列式存储的数据库。 对于数据仓库和分布式数据库来说，大部分情况下它会从各个数据源汇总数据，然后进行分析和反馈。其操作大多是围绕同一列属性的数据进行的，而当查询某属性的数据记录时，列式数据库只需返回与列属性相关的值。在大数据量查询场景中，列式数据库可在内存中高效组装各列的值，最终形成关系记录集，因此可以显著减少IO消耗，并降低查询响应时间。非常适合数据仓库和分布式的应用 查询过程中，可针对各列的运算并发执行(SMP)，最后在内存中聚合完整记录集，最大可能降低查询响应时间; 可在数据列中高效查找数据，无需维护索引(任何列都能作为索引)，查询过程中能够尽量减少无关IO，避免全表扫描; 因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率;如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。 不适用的场景： 数据需要频繁更新的交易场景 表中列属性比较少的小量数据库场景 不适合做含有删除和更新的实时操作]]></content>
      <categories>
        <category>DB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2019年终个人总结]]></title>
    <url>%2F2020%2F01%2F20%2F2019%E5%B9%B4%E7%BB%88%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Hey, password is required here. 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]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring-SpringBoot-MyBatis]]></title>
    <url>%2F2020%2F01%2F10%2FSpring-SpringBoot-MyBatis%2F</url>
    <content type="text"><![CDATA[本文将全局介绍 MyBatis 的整体架构。 参考：《MyBatis技术内幕》 MyBatis整体架构 基础支持层反射模块该模块对Java原生的反射进行了良好的封装，提供了更加简洁易用的API，方便上层使调用，并且对反射操作进行了一系列优化，例如缓存了类的元数据，提高了反射操作的性能。 类型转换模块MyBatis为简化配置文件提供了别名机制，该机制是类型转换模块的主要功能之一。类型转换模块的另一个功能是实现JDBC类型与Java类型之间的转换，该功能在为SQL语句绑定实参以及映射查询结果集时都会涉及。在为SQL语句绑定实参时，会将数据由Java类型转换成JDBC类型；而在映射结果集时，会将数据由JDBC类型转换成Java类型。 TypeHandlersetParameter(): 将数据由JdbcType类型转换成Java类型 getResult(): 将数据由Java类型转成JdbcType TypeHandler的实现子类： IntegerTypeHandler实现： TypeHandlerRegistry 除了MyBatis本身提供的TypeHandler实现，我们也可以添加自定义的TypeHandler接口实现，添加方式是在mybatis-config.xml配置文件中的＜typeHandlers＞节点下，添加相应的＜typeHandler＞节点配置，并指定自定义的TypeHandler接口实现类。在MyBatis初始化时会解析该节点，并将该TypeHandler类型的对象注册到TypeHandlerRegistry中，供MyBatis后续使用。 TypeAliasRegistryMyBatis可以为一个类添加一个别名，之后就可以通过别名引用该类。 MyBatis通过TypeAliasRegistry类完成别名注册和管理的功能，TypeAliasRegistry的结构比较简单，它通过TYPE_ALIASES字段（Map＜String, Class＜？＞＞类型）管理别名与Java类型之间的对应关系，通过TypeAliasRegistry.registerAlias（）方法完成注册别名。 日志模块MyBatis作为一个设计优良的框架，除了提供详细的日志输出信息，还要能够集成多种日志框架，其日志模块的一个主要功能就是集成第三方日志框架。 日志适配器 Log接口定义了日志模块功能，LogFactory工厂类负责创建对应的日志组件适配器。在LogFactory类加载时会执行其静态代码块，其逻辑是按序加载并实例化对应日志组件的适配器，然后使用LogFactory.logConstructor这个静态字段，记录当前使用的第三方日志组件的适配器。 Jdbc调试 使用了代理模式， 资源加载模块资源加载模块主要是对类加载器进行封装，确定类加载器的使用顺序，并提供了加载类文件以及其他资源文件的功能。单例模式。 解析器模块解析器模块的主要提供了两个功能：一个功能是对XPath进行封装，为MyBatis初始化时解析mybatis-config.xml配置文件以及映射配置文件提供支持；另一个功能是为处理动态SQL语句中的占位符提供支持。 XML处理方式 DOM DOM是基于树形结构的XML解析方式，它会将整个XML文档读入内存并构建一个DOM树，基于这棵树形结构对各个节点（Node）进行操作。XML 文档中的每个成分都是一个节点：整个文档是一个文档节点，每个XML标签对应一个元素节点，包含在XML标签中的文本是文本节点，每一个 XML 属性是一个属性节点，注释属于注释节点。 经过DOM解析后得到的树形结构如下： SAX SAX是基于事件模型的XML解析方式，它并不需要将整个XML文档加载到内存中，而只需将XML文档的一部分加载到内存中，即可开始解析，在处理过程中并不会在内存中记录XML中的数据，所以占用的资源比较小。当程序处理过程中满足条件时，也可以立即停止解析过程，这样就不必解析剩余的XML内容。 StAX XPathXPath之于XML就好比SQL语言之于数据库。 XPath中常用的表达式： 查找所有书籍的XPath表达式是：”//book”。查找作者为Neal Stephenson的所有图书需要指定＜author＞节点的值，得到表达式：”//book[author=’Neal Stephenson’]”。为了找出这些图书的标题，需要选取＜title＞节点，得到表达式：”//book[author=’Neal Stephenson’]/title”。最后，真正需要的信息是＜title＞节点中的文本节点，得到的完整XPath表达式是：”//book[author=”Neal Stephenson”]/title/text（）”。 XPathParser 数据源模块现在开源的数据源都提供了比较丰富的功能，例如，连接池功能、检测连接状态等，选择性能优秀的数据源组件对于提升ORM框架乃至整个应用的性能都是非常重要的。MyBatis自身提供了相应的数据源实现，当然MyBatis也提供了与第三方数据源集成的接口，这些功能都位于数据源模块之中。 事务模块MyBatis对数据库中的事务进行了抽象，其自身提供了相应的事务接口和简单实现。在很多场景中，MyBatis会与Spring框架集成，并由Spring框架管理事务， JdbcTransaction依赖于JDBC Connection控制事务的提交和回滚。 缓存模块在优化系统性能时，优化数据库性能是非常重要的一个环节，而添加缓存则是优化数据库时最有效的手段之一。正确、合理地使用缓存可以将一部分数据库请求拦截在缓存这一层，这就能够减少相当一部分数据库的压力。MyBatis中提供了一级缓存和二级缓存，而这两级缓存都是依赖于基础支持层中的缓存模块实现的。这里需要读者注意的是，MyBatis中自带的这两级缓存与MyBatis以及整个应用是运行在同一个JVM中的，共享同一块堆内存。如果这两级缓存中的数据量较大，则可能影响系统中其他功能的运行，所以当需要缓存大量数据时，优先考虑使用Redis、Memcache等缓存产品。 装饰器模式 Binding模块开发人员无须编写自定义Mapper接口的实现，MyBatis会自动为其创建动态代理对象。在有些场景中，自定义Mapper接口可以完全代替映射配置文件，但有的映射规则和SQL语句的定义还是写在映射配置文件中比较方便，例如动态SQL语句的定义。 Mapper接口 核心处理层配置解析在MyBatis初始化过程中，会加载mybatis-config.xml配置文件、映射配置文件以及Mapper接口中的注解信息，解析后的配置信息会形成相应的对象并保存到Configuration对象中。例如，示例中定义的＜resultMap＞节点（即ResultSet的映射规则）会被解析成ResultMap对象；示例中定义的＜result＞节点（即属性映射）会被解析成ResultMapping对象。之后，利用该Configuration对象创建SqlSessionFactory对象。待MyBatis初始化之后，开发人员可以通过初始化得到SqlSessionFactory创建SqlSession对象并完成数据库操作。 建造者模式 BaseBuilder 12345678// All-In-One配置对象protected final Configuration configuration;// 在mybatis-config.xml配置文件中可以使用&lt;typeAliases&gt;标签定义别名，在Configuration对象初始化时创建的protected final TypeAliasRegistry typeAliasRegistry;// 在mybatis-config.xml配置文件中可以使用&lt;typeHandlers&gt;标签添加自定义TypeHandler器，在Configuration对象初始化时创建的protected final TypeHandlerRegistry typeHandlerRegistry; XMLConfigBuilder负责解析 mybatis-config.xml 文件 解析 节点生成 java.util.Properties 对象，设置到 XPathParser 和 Configuration 的 variables 字段中。 解析 节点MyBatis全局性配置，如 解析 、 节点 解析 节点插件是MyBatis提供的扩展机制之一，用户可以通过添加自定义插件在SQL语句执行过程中的某一点进行拦截。MyBatis中的自定义插件只需要实现Interceptor接口。Configuration 的 interceptorChain 字段。 解析 节点 解析 节点在实际生产中，同一项目可能分为开发、测试和生产多个不同的环境，每个环境的配置可能不尽相同，每个 节点对应一种环境的配置。根据 XMLConfigBuilder.environment 字段值确定要使用的 配置，之后创建对应的 TransactionFactory 和 DataSource 对象，并封装进 Environment 对象中。 解析 节点告诉 MyBatis 去哪些位置查找映射配置文件以及使用了配置注解标识的接口。会创建 XMLMapperBuilder 加载映射文件，加载相应的Mapper接口，解析其中的注解并完成向 MapperRegistry 的注册。 XMLMapperBuilder负责解析 mapper.xml 文件 解析 节点 解析 节点 解析 节点定义了结果集与结果对象之间的映射规则。 解析 节点定义了可重用的SQL语句片段。 XMLStatementBuilder负责解析定义的SQL语句。 12345678// 节点中的id属性（包括命名空间前缀）private String resource;// 对应一条SQL语句，getBoundSqlprivate SqlSource sqlSource;// SQL的类型，INSERT、UPDATE、DELETE、SELECTprivate SqlCommandType sqlCommandType; 解析 节点在解析SQL节点之前，首先通过XMLIncludeTransformer 解析SQL语句中节点，该过程会将节点替换成节点中定义的SQL片段，并将其中的${}占位符替换成真实的参数，可能涉及多层递归。 12345678910&lt;include id="someinclude"&gt; from $&#123;tablename&#125;&lt;/include&gt;&lt;select id="" resultType=""&gt; select B.id as blog_id,B.title as blog_title,B.author_id as blog_author_id &lt;include refid="someinclude"&gt; &lt;property name="tablename" value="Blog"/&gt; &lt;/include&gt;&lt;/select&gt; 解析 节点 解析 SQL 节点 绑定Mapper接口每个映射配置文件中的命名空间可以绑定一个Mapper接口，并注册到 MapperRegistry 中。在 XMLMapperBuilder.bindMapperForNamespace()中完成。 SQL解析与scripting模块拼凑SQL语句是一件烦琐且易出错的过程，为了将开发人员从这项枯燥无趣的工作中解脱出来，MyBatis实现动态SQL语句的功能，提供了多种动态SQL语句对应的节点，例如，＜where＞节点、＜if＞节点、＜foreach＞节点等。通过这些节点的组合使用，开发人员可以写出几乎满足所有需求的动态SQL语句。 MyBatis中的scripting模块会根据用户传入的实参，解析映射文件中定义的动态SQL节点，并形成数据库可执行的SQL语句。之后会处理SQL语句中的占位符，绑定用户传入的实参。 组合模式。 SQL执行SQL语句的执行涉及多个组件，其中比较重要的是Executor、StatementHandler、ParameterHandler和ResultSetHandler。Executor主要负责维护一级缓存和二级缓存，并提供事务管理的相关操作，它会将数据库相关操作委托给StatementHandler完成。StatementHandler首先通过ParameterHandler完成SQL语句的实参绑定，然后通过java.sql.Statement对象执行SQL语句并得到结果集，最后通过ResultSetHandler完成结果集的映射，得到结果对象并返回。 模板模式。 插件接口层 接口层相对简单，其核心就是SqlSession接口，该接口中定义了MyBatis暴露给应用程序调用的API，也就是上层应用与MyBatis交互的桥梁。 接口层在接收到调用请求时，会调用核心处理层的相应模块来完成具体的数据库操作。 策略模式。 Spring问题IoC 控制反转 类A和类B的依赖关系通过配置文件告诉IoC容器，由IoC容器创建对象A和对象B并维护两者之间的关系，客户在使用对象A时，可以直接从IoC容器中获取。 BeanFactory 与 ApplicationContext 的区别都可以当作 Spring 容器，其中 ApplicationContext 是 BeanFactory 的字接口。 BeanFactory：是 Spring 里面最底层的接口，包含了各种Bean的定义，读取bean配置文件，管理bean的加载、实例化，控制bean的生命周期，维护bean之间的依赖关系。ApplicationContext：作为 BeanFactory 的派生，除了提供 BeanFactory 所具有的功能外，还提供了更完整的框架功能： 继承 MessageSource，因此支持国际化 统一的资源文件访问方式 提供在监听器中注册bean的事件 同时加载多个配置文件 载入多个（有继承关系）上下文，使得每一个上下文都专注于一个特定的层次，比如应用的web层 BeanFactory：采用的是延迟加载形式来注入 Bean 的，即只有在使用到某个Bean时（调用getBean()），才对该 Bean 进行加载实例化。这样，我们就不能发现一些存在的Spring配置问题。如果Bean的某一个属性没有注入，BeanFactory加载后，直到第一次使用调用getBean方法才会抛出异常。ApplicationContext：在容器启动时，一次性创建了所有bean。这样，在容器启动时，就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。ApplicationContext 启动后预载入所有的单实例 Bean。ApplicationContext 唯一的不足是占用内存空间，当应用程序配置bean较多时，程序启动较慢。 BeanFactory 通常以编程的方式被创建，ApplicationContext 还能以声明的方式创建，如使用ContextLoader。 BeanFactory 和 ApplicationContext 都支持 BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory 需要手动注册，而 ApplicationContext 是自动注册的。 refresh 函数中包含了几乎 ApplicationContext 中提供的全部功能，而且此函数中逻辑非常清晰明了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Override public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. // 准备刷新的上下文 // 对系统属性及环境变量的初始化及验证 prepareRefresh(); // Tell the subclass to refresh the internal bean factory. // 初始化BeanFactory，并进行XML文件的读取 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. // 对BeanFactory进行各种功能填充 // @Qualifier 和 @Autowired 应该是大家非常熟悉的注解，那么这两个注解正是在这一步骤中增加的支持。 prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. // 子类覆盖方法做额外的处理 postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. // 激活各种BeanFactory处理器 invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. // 注册拦截Bean创建的Bean处理器，这里只是注册，真正的调用是在getBean的时候 registerBeanPostProcessors(beanFactory); // Initialize message source for this context. // 为上下文初始化Message源，即不同语言的消息体，国际化处理 initMessageSource(); // Initialize event multicaster for this context. // 初始化应用消息广播器，并放入 “applicationEventMulticaster” bean中 initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. // 留给子类来初始化其他的bean onRefresh(); // Check for listener beans and register them. // 在所有注册的bean中查找 Listener bean，注册到消息广播器中 registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. // 初始化剩下的单实例（非惰性的） finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. // 完成刷新过程，通知生命周期处理器lifeCycleProcessor 刷新过程，同时发出 ContextRefreshEvent 通知别人 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125; &#125; Bean 的生命周期首先说说一下Servlet的生命周期：实例化，初始init，接收请求service，销毁destory。Spring 上下文中的Bean生命周期也类似： 实例化Bean 对于 BeanFactory 容器，当客户向容器请求一个尚未初始化的bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，容器就会调用createBean进行实例化。对于 ApplicationContext 容器，当容器启动结束后，通过获取 BeanDefinition 对象中的信息，实例化所有的bean。 设置对象属性（依赖注入） 实例化后的对象被封装在 BeanWrapper 对象中，紧接着，Spring 根据 BeanDefinition 中的信息以及通过 BeanWrapper 提供的设置属性的接口完成依赖注入。 处理Aware接口 BeanFactoryAware、ApplicationContextAware、ResourceLoaderAware、ServletContextAware等 ①如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String beanId)方法，此处传递的就是Spring配置文件中Bean的id值； ②如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory()方法，传递的是Spring工厂自身。 ③如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文 BeanPostProcessor的before 对Bean初始化前的一些自定义前置处理 init-method 激活自定义的init方法，如果Bean在Spring配置文件中配置了init-method属性，则会自动调用其配置的初始化方法。 BeanPostProcessor的after 对Bean初始化后的一些自定义后置处理。此时，Bean已经被正确创建了，可以使用了。 DisposableBean Sprin中不但提供了对于初始化方法的扩展入口，同样也提供了销毁方法的扩展入口，除了熟知的配置属性 destory-method 外，用户还可以注册后处理器 DestructionAwareBeanPostProcessor 来统一处理 bean 的销毁方法。当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法； destory-method 最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。 autowiring 的实现原理自动装配，减少用户配置Bean的工作量。对属性 autowire 的处理是 populateBean 处理过程的一部分，在处理一般的Bean之前，先对 autowiring 属性进行处理。 autowire_by_name，autowire_by_type 在Spring框架xml配置中有5种自动装配： no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean byName：通过bean的名称进行自动装配，如果一个bean的property与另一个bean的name相同，就进行自动装配 byType：通过参数的数据类型进行自动装配 constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。 autodetect：自动探测，如果有构造方法，通过construct的方式自动装配，否则使用byType的方式自动装配 基于注解的方式：使用@Autowired注解来自动装配指定的bean，需要在Spring配置文件中进行配置，context:annotation-config/。在启动Spring IOC时，容器自动装载了一个 AutowiredAnnotationBeanPostProcessor 处理器，当容器扫描到@Autowired、@Resource、@Inject 时，就会在Ioc容器自动查找需要的bean,并装配该对象的属性。在使用@Autowired的时候，首先在容器中查询对应类型的bean：如果查询的结果刚好为一个，就将该bean装配给@Autowired指定的数据；如果查询的结果不止一个，那么@Autowired会根据名称来查找；如果上述查找的结果为空，那么会抛出异常，解决办法是使用required=false @Autowired可用于：构造函数、成员变量、Setter方法@Autowired 和 @Resource 之间的区别： @Autowired 默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它的属性为false） @Resource 默认是按名称装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入 Spring支持的几种bean的作用域 singleton：默认，每个容器中只有一个bean的实例，单例的模式由BeanFactory自身来维护 prototype：为每一个bean请求提供一个实例 request：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收 session：与request范围类似，确保每个 session 中有一个bean的实例，在 session 过期后，bean 会随之失效 global-session：全局作用域，global-session 和 Portlet 应用相关。当应用部署在 Portlet 容器中工作时，它包含很多 portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。全局作用域与Servlet中的session作用域效果相同。 AOP Advice通知 定义在连接点做什么，为切面增强提供织入接口。 MethodBeforeAdvice.before() AfterReturningAdvice.afterReturning() ThrowsAdvice.afterThrowing() Pointcut切点 需要增强的地方可以是被某个正则表达式进行标识，或根据某个方法名进行匹配等。 在面向对象编程 OOP 中，开发人员可以通过封装、继承、多态等概念建立对象的层次结构。在系统中，除了核心的业务逻辑，还会有权限检测、日志输出、事务管理等相关的代码，它们会散落在多个对象中，横跨整个对象层次结构，但是这些功能与核心业务逻辑并无直接关系。AOP利用“横切”技术将那些影响了多个类的公共代码抽取出来，封装到一个可重用的模块中，并将其称为Aspect（切面）。这样就可以减少重复的代码，降低模块之间的耦合度，提高了系统的可维护性。 AOP实现的关键在于代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为 AspectJ ，动态代理则以 Spring AOP 为代表： AspectJ 是静态代理的增强，所谓静态代理，就是 AOP 框架会在编译阶段生成 AOP 代理类，因此也称为编译时增强，它会在编译阶段将 AspectJ(切面) 织入到Java字节码中，运行的时候就是增强之后的AOP对象。 Spring AOP 使用的是动态代理，所谓的动态代理就是说 AOP 框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个 AOP 对象，这个 AOP 对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 Spring AOP 中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理： JDK 动态代理只提供接口的代理，不支持类的代理。核心 InvocationHandler 接口和 Proxy 类，InvocationHandler 通过 invoke() 方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy 利用 InvocationHandler 动态创建一个符合某一接口的实例，生成目标类的代理对象。 InvocationHandler 的 invoke(Object proxy, Method method, Object[] args)：proxy是最终生成的代理实例；method是被代理目标实现的某个具体方法；args是被代理目标实现的某个方法的具体入参，在方法反射调用时使用。 如果代理类没有实现 InvocationHandler 接口，那么 Spring AOP 会选择使用 CGLIB 来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并增加增强代码，从而实现AOP。CGLIB 是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 静态代理与动态代理区别在于生成 AOP 代理对象的时机不同，相对来说 AspectJ 的静态代理方式具有更好的性能，但是 AspectJ 需要特定的编译器进行处理，而 Spring AOP 则无需特定的编译器处理。 SpringMVC 用户发送请求至前端控制器 DispatcherServlet DispatcherServlet 收到请求调用处理器映射器 HandlerMapping 处理器映射器根据请求 url 找到具体的处理器，生成处理器执行链 HandlerExecutionChain (包括处理器对象和处理器拦截器) 一并返回给 DispatcherServlet DispatcherServlet 根据处理器 Handler 获取处理器适配器 HandlerAdapter 执行 HandlerAdapter 处理一系列操作，如：参数封装、数据格式转换、数据验证等操作 执行处理器Handler（Controller，也叫页面控制器） Handler 执行完返回 ModelAndView HanlerAdapter 将 Handler 执行结果 ModeAndView 返回到 DispatcherServlet DispatcherServlet 将 ModelAndView 传给 ViewResolver 视图解析器 ViewResolver 解析后返回具体 View DispatcherServlet 对 View 进行渲染视图（即将模型数据model填充至视图中） DispatcherServlet 响应用户 Spring事务实现Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。 Spring事务的种类 编程式事务管理使用 TransactionTemplate @Transactional 声明式事务管理建立在AOP之上的，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，在目标方法开始之前加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。 Spring的事务传播行为Spring 事务的传播行为说的是，当多个事务同时存在的时候，Spring 如何处理这些事务的行为。 PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当时存在事务，就加入该事务，该设置是最常用的设置。 PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。 PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按 required 属性执行。 Spring中的隔离级别 ISOLATION_DEFAULT：这是个 PlatfromTransactionManager 默认的隔离级别，使用数据库默认的事务隔离级别。 ISOLATION_READ_UNCOMMITTED：读未提交，允许另外一个事务可以看到这个事务未提交的数据。 ISOLATION_READ_COMMITTED：读已提交，保证一个事务修改的数据提交后才能被另一事务读取，而且能看到该事务对已有记录的更新。 ISOLATION_REPEATABLE_READ：可重复读，保证一个事务修改的数据提交后才能被另一事务读取，但是不能看到该事务对已有记录的更新。 ISOLATION_SERIALIZABLE：一个事务在执行的过程中完全看不到其他事务对数据库所做的更新。 Spring 框架中的不同类型的事件Spring 提供了5种标准的事件： 上下文更新事件（ContextRefreshedEvent）：在调用 ConfigurableApplicationContext 接口中的refresh()方法时被触发。 上下文开始事件（ContextStartedEvent）：当容器调用 ConfigurableApplicationContext 的 start() 方法开始/重新开始容器时触发该事件。 上下文停止事件 (ContextStoppedEvent)：当容器调用 ConfigurableApplicationContext 的 stop() 方法开始/重新开始容器时触发该事件。 上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。 请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。 如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。 DI 依赖注入对象之间的依赖关系是由容器在运行期决定的，也就是说，由容器动态地确定并维持两个对象之间的某个依赖关系。通过依赖注入机制，开发人员只需要通过简单的配置（XML或注解），就可以确定依赖关系，实现组件的重用。 SpringBoot问题什么是 SpringBoot ？SpringBoot 是 Spring 开源组织下的子项目，是 Spring 组件一站式解决方案，主要简化了使用 Spring 的难度，简省了繁重的配置，提供了各种启动器，开发者能快速上手。 什么是Spring 为什么要用 SpringBoot ？ 独立运行 简化配置 自动配置 无代码生成和XML配置 应用监控 上手容易 SpringBoot组件 名称 描述 spring-boot-starter 核心 Spring Boot starter，包括自动配置支持，日志和 YAML spring-boot-starter-actuator 生产准备的特性，用于帮我们监控和管理应用 spring-boot-starter-amqp 对”高级消息队列协议”的支持，通过 spring-rabbit 实现 spring-boot-starter-aop 对面向切面编程的支持，包括 spring-aop 和 AspectJ spring-boot-starter-batch 对 Spring Batch 的支持，包括 HSQLDB 数据库 spring-boot-starter-cloudconnectors 对 Spring Cloud Connectors 的支持，简化在云平台下（例如，Cloud Foundry 和 Heroku）服务的连接 spring-boot-starter-dataelasticsearch 对 Elasticsearch 搜索和分析引擎的支持，包括 spring-data-elasticsearch spring-boot-starter-datagemfire 对 GemFire 分布式数据存储的支持，包括 spring-data-gemfire spring-boot-starter-data-jpa 对”Java 持久化 API”的支持，包括 spring-data-jpa，spring-orm 和 Hibernate spring-boot-starter-datamongodb 对 MongoDB NOSQL 数据库的支持，包括 spring-data-mongodb spring-boot-starter-data-rest 对通过 REST 暴露 Spring Data 仓库的支持，通过 spring-data-rest-webmvc 实现 spring-boot-starter-data-solr 对 Apache Solr 搜索平台的支持，包括 spring-data-solr spring-boot-starter-freemarker 对 FreeMarker 模板引擎的支持 spring-boot-starter-groovytemplates 对 Groovy 模板引擎的支持 spring-boot-starter-hateoas 对基于 HATEOAS 的 RESTful 服务的支持，通过 spring-hateoas 实现 spring-boot-starter-hornetq 对”Java 消息服务 API”的支持，通过 HornetQ 实现 spring-boot-starter-integration 对普通 spring-integration 模块的支持 spring-boot-starter-jdbc 对 JDBC 数据库的支持 spring-boot-starter-jersey 对 Jersey RESTful Web 服务框架的支持 spring-boot-starter-jta-atomikos 对 JTA 分布式事务的支持，通过 Atomikos 实现 spring-boot-starter-jta-bitronix 对 JTA 分布式事务的支持，通过 Bitronix 实现 spring-boot-starter-mail 对 javax.mail 的支持 spring-boot-starter-mobile 对 spring-mobile 的支持 spring-boot-starter-redis 对 REDIS 键值数据存储的支持，包括 spring-redis spring-boot-starter-security 对 spring-security 的支持 spring-boot-starter-socialfacebook 对 spring-social-facebook 的支持 spring-boot-starter-sociallinkedin 对 spring-social-linkedin 的支持 spring-boot-starter-socialtwitter 对 spring-social-twitter 的支持 spring-boot-starter-test 对常用测试依赖的支持，包括 JUnit, Hamcrest 和 Mockito，还有 spring-test 模块 spring-boot-starter-thymeleaf 对 Thymeleaf 模板引擎的支持，包括和 Spring 的集成 spring-boot-starter-velocity 对 Velocity 模板引擎的支持 spring-boot-starter-web 对全栈 web 开发的支持， 包括 Tomcat 和 spring-webmvc spring-boot-starter-websocket 对 WebSocket 开发的支持 spring-boot-starter-ws 对 Spring Web 服务的支持 spring-boot-starter-remote-shell 添加远程 ssh shell支持 spring-boot-starter-jetty 导入 Jetty HTTP 引擎（作为 Tomcat 的替代） spring-boot-starter-log4j 对 Log4J 日志系统的支持 spring-boot-starter-logging 导入 Spring Boot 的默认日志系统 spring-boot-starter-tomcat 导入 Spring Boot 的默认 HTTP 引擎 spring-boot-starter-undertow 导入 Undertow HTTP 引擎（作为 Tomcat 的替代） Spring的启动流程 组装 SpringApplication resourceLoader：设置resourceload 设置 primarySources：可以把启动类加载进入Spring容器 webApplicationType：判断当前 application 应该运行在什么环境下 mainApplicationClass：找出main方法启动的class 执行 SpringApplication 的 run 获取 SpringApplicationRunListeners 从 META-INF/spring.factories 获取 SpringApplicationRunListeners 的集合，并依次调用 SpringApplicationRunListener 的 starting 方法，即最终调用 ApplicationListener 的 onApplicationEvent 方法，发布 springboot 启动事件 prepareEnvironment（目前profile功能已经被maven取代了） ConfigurableEnvironment：代表两种意义：一种是profiles，用来描述哪些beandefinitions是可用的；一种是properties，用来描述系统的配置，其来源可能是配置文件、JVM属性文件、操作系统环境变量等等 getOrCreateEnvironment()：根据webApplicationType创建不同的Environment configureEnvironment(XX)：通过configurePropertySources(environment, args)设置properties，通过configureProfiles(environment, args)设置profiles listeners.environmentPrepared(environment);发布environmentPrepared事件，即调用ApplicationListener的onApplicationEvent事件 bindToSpringApplication：即把当前的environment和当前的springApplication绑定 ConfigurationPropertySources.attach(environment)：将ConfigurationPropertySource放入environment的propertysource中的第一个 createApplicationContext：创建Spring的容器 根据不同的 webApplicationType 设置不同的 contextClass (容器的class类型)，然后生成不同的容器实例对象 生成容器实例的时候，对于Kotlin类使用’primary’构造函数实例化一个类，如果不是就使用默认构造函数，根据得到构造函数生成实例对象，如果构造函数不是公共的，我们尝试去改变并访问 prepareContext：准备容器，在准备刷新容器前准备好容器 context.setEnvironment(environment)：设置spring容器的environment postProcessApplicationContext(context)：设置beanNameGenerator和resourceLoader applyInitializers(context)：调用ApplicationContextInitializer的initialize来初始化context，其中还检测各个ApplicationContextInitializer是否接受该类型的容器 listeners.contextPrepared(context);即调用SpringApplicationRunListener的contextPrepared方法，但目前是个空实现。 分别注册springApplicationArguments和springBootBanner这两个bean getAllSources就是获取我们的primarySources和sources load(context, sources.toArray(new Object[0]))：首先创建BeanDefinitionLoader，设置该loader的sources，annotatedReader，xmlReader，scanner，以及添加scanner的ExcludeFilter（即过滤springboot的启动类），若用户启动的时候设置了beanNameGenerator，resourceLoader，environment的话就替代我们自身设置的属性。同时根据source的类型选择不同的load方法，这边我们是load（class），最终判断是否是component注解，是的话就通过annotatedReader将启动类注册成bean listeners.contextLoaded(context):首先判断ApplicationListener是否属于ApplicationContextAware，如果是的话就将spring容器赋值给该listener，然后将该ApplicationListener赋值给spring容器，然后调用ApplicationListener的onApplicationEvent方法 context.registerShutdownHook()：注册一个线程，该线程主要指向doclose方法，doClose方法的逻辑就是：从applicationContexts集合中删除当前容器，MBeanServer卸载MBean，发布容器关闭事件，调用了实现了lifecycleProcessor接口的bean，destroyBeans，closeBeanFactory，onClose：关闭内置tomcat，active设置为false refreshContext(context)：真正的刷新spring容器 prepareRefresh():设置些初始的操作比如，开启激活，启动日期，初始化propertySource。 获取beanFactory prepareBeanFactory(beanFactory)：设置beanFactory的classloader，BeanExpressionResolver，PropertyEditorRegistrar，ApplicationContextAwareProcessor和忽略xxxxAware，注 册依赖，还有ApplicationListenerDetectorApplicationContextAwareProcessor：只是将applicationContext传递给ApplicationContextAwareProcessor，方便后面的xxxAware调用忽略xxxxAware：忽略这些Aware接口实现类中与接口set方法中入参类型相同的属性的的自动注入这样就保证了关键的类是由spring容器自己产生的而不是我们注入的，自动注入不是指的@AutoWire 而是指的是beans的default-autowire=”byType” 或在bean的autowire=”byType” ，这样spring 回去ioc容器寻找类型相似的类型给其注入，如果实现了spring 的xxaware接口，就不会自动注入记载filterPropertyDescriptorsForDependencyCheck删除与入参类型相同的属性注册依赖：即指定一些类自动注入的实例是spring指定的实例对象ApplicationListenerDetector：检测实现了ApplicationListener的实现类，因为有些实现类，无法通过getBeanNamesForType获取到。 postProcessBeanFactory(beanFactory)：继续设置ignoreDependencyInterface（ServletContextAware）还有annotatedClasses，basePackages如果存在就设置。 invokeBeanFactoryPostProcessors(beanFactory)：从beanFactoryPostProcessors获取BeanFactoryPostProcessor，然后先执行BeanDefinitionRegistryPostProcessor类型的postProcessBeanDefinitionRegistry，继续从beanFactory获取BeanDefinitionRegistryPostProcessor类型的bean然后执行postProcessBeanDefinitionRegistry，执行的过程按照PriorityOrdered，Ordered，普通的类型进行执行，然后优先执行registryProcessors的postProcessBeanFactory在执行regularPostProcessors的postProcessBeanFactory，再从BeanFactory获取PriorityOrdered，Ordered，普通的类型三种类型的BeanFactoryPostProcessor，并按照顺序执行。总结：从之前加入的beanFactoryPostProcessor先执行postProcessBeanDefinitionRegistry（假如是BeanDefinitionRegistryPostProcessor）然后在执行postProcessBeanFactory方法，然后从beanFactory获取BeanFactoryPostProcessor 然后执行postProcessBeanFactory，执行过程中都要按照PriorityOrdered，Ordered，普通的类型三种类型的顺序执行。 registerBeanPostProcessors：从beanFactory获取BeanPostProcessor分别按照PriorityOrdered，Ordered，普通的类型注册BeanPostProcessor BeanPostProcessor和BeanFactoryPostProcessor:前者是对bean初始化前后进行设置，后者可以对beanFactory进行修改 或者，可以对beanDefinition进行修改或者增加或者初始化渴望提前初始化的bean initMessageSource()：一般是我们用来初始化我们国际化文件的 initApplicationEventMulticaster():设置applicationEventMulticaster，spring发布各种事件就依靠他，这个和springboot发布事件使用相同的类 onRefresh()：初始化其他的子容器类中的bean，同时创建spring的内置tomcat，这在后期Springboot内嵌式tomcat中详细阐述 registerListeners()：添加用户设置applicationListeners，然后从beanFactory获取ApplicationListener，然后发布需要earlyApplicationEvents事件 finishBeanFactoryInitialization(beanFactory)：实例化非懒加载的剩余bean finishRefresh：清理资源缓存，初始化lifecycle，调用lifecycle的onrefresh，发布ContextRefreshedEvent的事件,激活JMX,启动tomcat afterRefresh(context, applicationArguments)：目前是空实现 listeners.started(context)：发布started事件 callRunners(context, applicationArguments) 从 spring 容器中获取 ApplicationRunner 和 CommandLineRunner 对象，然后按照顺序排序，循环调用他们的run 方法 handleRunFailure(context, ex, exeptionReporters, listeners) 处理不同的异常状态，然后调用 listeners.failed(context, exception)，并关闭spring容器 listeners.running(context)：发布running事件 SpringCloud 问题熔断、限流 MyBatis问题什么是 Mybatis ?（1）Mybatis是一个半ORM（对象关系映射）框架，它内部封装了JDBC，开发时只需要关注SQL语句本身，不需要花费精力去处理加载驱动、创建连接、创建statement等繁杂的过程。程序员直接编写原生态sql，可以严格控制sql执行性能，灵活度高。 （2）Mybatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 （3）通过xml 文件或注解的方式将要执行的各种 statement 配置起来，并通过java对象和 statement中sql的动态参数进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射为java对象并返回。（从执行sql到返回result的过程）。 Mybatis的优点（1）基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用。（2）与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接；（3）很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）。（4）能够与Spring很好的集成；（5）提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护。 Mybatis的$和#的区别？1）#{}是预编译处理，${}是字符串替换。2）Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值；3）Mybatis在处理${}时，就是把${}替换成变量的值。4）使用#{}可以有效的防止SQL注入，提高系统安全性。 讲下Mybatis的缓存 Mybatis缓存分为一级缓存和二级缓存，一级缓存放在session里面，二级缓存放在它的命名空间里，默认是不打开的，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态)，可在它的映射文件中配置。 Mybatis是如何分页的？分页插件的原理是什么？ 1）Mybatis使用RowBounds对象进行分页，可以直接编写sql实现分页，也可以使用Mybatis的分页插件。 2）分页插件的原理：实现Mybatis提供的接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql。 举例：select * from student，拦截sql后重写为select t.* from (select * from student) t limit 0,10 简述Mybatis的运行原理，以及如何编写一个插件？ 1）Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis通过动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 2）实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。 Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？ 1）Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能。 2）Mybatis提供了9种动态sql标签：trim|where|set|foreach|if|choose|when|otherwise|bind。 3）其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。 简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？ Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中，标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。标签会被解析为ResultMap对象，其每个子元素会被解析为ResultMapping对象。每一个、、、标签均会被解析为MappedStatement对象，标签内的sql会被解析为BoundSql对象。 MyBatis中遇到的设计模式？ Builder模式，例如SqlSessionFactoryBuilder、XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder、CacheBuilder； 工厂模式，例如SqlSessionFactory、ObjectFactory、MapperProxyFactory； 单例模式，例如ErrorContext和LogFactory；代理模式，Mybatis实现的核心，比如MapperProxy、ConnectionLogger，用的jdk的动态代理；还有executor.loader包使用了cglib或者javassist达到延迟加载的效果；组合模式，例如SqlNode和各个子类ChooseSqlNode等；模板方法模式，例如BaseExecutor和SimpleExecutor，还有BaseTypeHandler和所有的子类例如IntegerTypeHandler；适配器模式，例如Log的Mybatis接口和它对jdbc、log4j等各种日志框架的适配实现；装饰者模式，例如Cache包中的cache.decorators子包中等各个装饰者的实现；迭代器模式，例如迭代器模式PropertyTokenizer；]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[About Hadoop]]></title>
    <url>%2F2020%2F01%2F10%2FAbout-Hadoop%2F</url>
    <content type="text"><![CDATA[XXX 问题HadoopYarn架构 ResourceManager（RM）RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成： 调度器：Scheduler；应用程序管理器：Applications Manager，ASM。调度器调度器根据容量、队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。 应用程序管理器应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。 NodeManager（NM）NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。 ApplicationMaster（AM）提交的每个作业都会包含一个 AM，主要功能包括： 与 RM 协商以获取资源（用 container 表示）；将得到的任务进一步分配给内部的任务；与 NM 通信以启动/停止任务；监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。 ContainerContainer 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。 HDFS写流程 Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象； 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息； 通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block； DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点； DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功； 完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入； 调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。 HDFS读流程 Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息； NameNode 返回存储的每个块的 DataNode 列表； Client 将连接到列表中最近的 DataNode； Client 开始从 DataNode 并行读取数据； 一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。 HDFS创建一个文件的流程 客户端通过ClientProtocol协议向RpcServer发起创建文件的RPC请求。 FSNamesystem封装了各种HDFS操作的实现细节，RpcServer调用FSNamesystem中的相关方法以创建目录。 进一步的，FSDirectory封装了各种目录树操作的实现细节，FSNamesystem调用FSDirectory中的相关方法在目录树中创建目标文件，并通过日志系统备份文件系统的修改。 最后，RpcServer将RPC响应返回给客户端。 Hadoop1.x与Hadoop2.x的区别Hadoop1.x的缺点：JobTracker存在单点故障的隐患任务调度和资源管理全部是JobTracker来完成,单点负担过重TaskTracker以Map/Reduce数量表示资源太过简单TaskTracker 分Map Slot 和 Reduce Slot, 如果任务只需要map任务可能会造成资源浪费 资源调度方式的改变在1.x, 使用Jobtracker负责任务调度和资源管理,单点负担过重,在2.x中,新增了yarn作为集群的调度工具.在yarn中,使用ResourceManager进行 资源管理, 单独开启一个Container作为ApplicationMaster来进行任务管理. HA模式在1.x中没有HA模式,集群中只有一个NameNode,而在2.x中可以启用HA模式,存在一个Active NameNode 和Standby NameNode. HDFS FederationHadoop 2.0中对HDFS进行了改进，使NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性 Hadoop HA的介绍 Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务； ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）； Zookeeper 集群：为主备切换控制器提供主备选举支持； 共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 描述如何安装配置Hadoop 使用root账户登录 修改IP 修改Host主机名 配置SSH免密码登录 关闭防火墙 安装JDK 解压hadoop安装包 配置hadoop的核心文件hadoop-env.sh：用于定义Hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为Hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等；core-site.xml：用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置；mapred-site.xml：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置；hdfs-site.xml：HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置； 格式化 hadoop namenode -format 启动节点start-all.sh 启动Hadoop集群会分别启动哪些进程，各自的作用？ NameNode：维护文件系统树及整棵树内所有的文件和目录。这些信息永久保存在本地磁盘的两个文件中：命名空间镜像文件、编辑日志文件记录每个文件中各个块所在的数据节点信息，这些信息在内存中保存，每次启动系统时重建这些信息负责响应客户端的 数据块位置请求 。也就是客户端想存数据，应该往哪些节点的哪些块存；客户端想取数据，应该到哪些节点取接受记录在数据存取过程中，datanode节点报告过来的故障、损坏信息 SecondaryNameNode(非HA模式)：实现namenode容错的一种机制。定期合并编辑日志与命名空间镜像，当namenode挂掉时，可通过一定步骤进行上顶。(注意 并不是NameNode的备用节点) DataNode：根据需要存取并检索数据块定期向namenode发送其存储的数据块列表 ResourceManager：负责Job的调度,将一个任务与一个NodeManager相匹配。也就是将一个MapReduce之类的任务分配给一个从节点的NodeManager来执行。 NodeManager：运行ResourceManager分配的任务，同时将任务进度向application master报告 JournalNode(HA下启用):高可用情况下存放namenode的editlog文件 小文件过多会有什么危害？Hadoop上大量HDFS元数据信息存储在NameNode内存中,因此过多的小文件必定会压垮NameNode的内存.每个元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间.显而易见的解决这个问题的方法就是合并小文件,可以选择在客户端上传时执行一定的策略先合并,或者是使用Hadoop的CombineFileInputFormat&lt;K,V&gt;实现小文件的合并 HiveHive内部表和外部表的区别建表时带有external关键字为外部表，否则为内部表内部表和外部表建表时都可以自己指定location删除表时，外部表不会删除对应的数据，只会删除元数据信息，内部表则会删除其他用法是一样的 Hive四种排序方式的区别 order byorder by 是要对输出的结果进行全局排序，这就意味着只有一个reducer才能实现（多个reducer无法保证全局有序）但是当数据量过大的时候，效率就很低。如果在严格模式下（hive.mapred.mode=strict）,则必须配合limit使用 sort bysort by 不是全局排序，只是在进入到reducer之前完成排序，只保证了每个reducer中数据按照指定字段的有序性，是局部排序。配置mapred.reduce.tasks=[nums]可以对输出的数据执行归并排序。可以配合limit使用，提高性能 distribute bydistribute by 指的是按照指定的字段划分到不同的输出reduce文件中，和sort by一起使用时需要注意， distribute by必须放在前面 cluster bycluster by 可以看做是一个特殊的distribute by+sort by，它具备二者的功能，但是只能实现倒序排序的方式,不能指定排序规则为asc 或者desc Hive的Metastore的三种模式 内嵌Derby方式这个是Hive默认的启动模式，一般用于单元测试，这种存储方式有一个缺点：在同一时间只能有一个进程连接使用数据库。 Local方式本地MySQL Remote方式远程MySQL,一般常用此种方式 Hive中的join有哪些？Hive中除了支持和传统数据库中一样的内关联（JOIN）、左关联（LEFT JOIN）、右关联（RIGHT JOIN）、全关联（FULL JOIN），还支持左半关联（LEFT SEMI JOIN） 内关联（JOIN）只返回能关联上的结果。 左外关联（LEFT [OUTER] JOIN）以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。 右外关联（RIGHT [OUTER] JOIN）和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。 全外关联（FULL [OUTER] JOIN）以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。 LEFT SEMI JOIN以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY也在副表中的记录 笛卡尔积关联（CROSS JOIN）返回两个表的笛卡尔积结果，不需要指定关联键。 Impala和Hive的查询有哪些区别？Impala是基于Hive的大数据实时分析查询引擎，直接使用Hive的元数据库Metadata,意味着impala元数据都存储在Hive的metastore中。并且impala兼容Hive的sql解析，实现了Hive的SQL语义的子集，功能还在不断的完善中。 Impala相对于Hive所使用的优化技术 没有使用 MapReduce进行并行计算，虽然MapReduce是非常好的并行计算框架，但它更多的面向批处理模式，而不是面向交互式的SQL执行。与 MapReduce相比：Impala把整个查询分成一执行计划树，而不是一连串的MapReduce任务，在分发执行计划后，Impala使用拉式获取 数据的方式获取结果，把结果数据组成按执行树流式传递汇集，减少的了把中间结果写入磁盘的步骤，再从磁盘读取数据的开销。Impala使用服务的方式避免 每次执行查询都需要启动的开销，即相比Hive没了MapReduce启动时间。 使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。 充分利用可用的硬件指令（SSE4.2）。 更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。 通过选择合适的数据存储格式可以得到最好的性能（Impala支持多种存储格式）。 最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。 Hive中大表join小表的优化在小表和大表进行join时，将小表放在前边，效率会高，hive会将小表进行缓存 Hive Sql是怎样解析成MR job的？主要分为6个阶段: Hive使用Antlr实现语法解析.根据Antlr制定的SQL语法解析规则,完成SQL语句的词法/语法解析,将SQL转为抽象语法树AST. 遍历AST,生成基本查询单元QueryBlock.QueryBlock是一条SQL最基本的组成单元，包括三个部分：输入源，计算过程，输出. 遍历QueryBlock,生成OperatorTree.Hive最终生成的MapReduce任务，Map阶段和Reduce阶段均由OperatorTree组成。Operator就是在Map阶段或者Reduce阶段完成单一特定的操作。QueryBlock生成Operator Tree就是遍历上一个过程中生成的QB和QBParseInfo对象的保存语法的属性. 优化OperatorTree.大部分逻辑层优化器通过变换OperatorTree，合并操作符，达到减少MapReduce Job，减少shuffle数据量的目的 OperatorTree生成MapReduce Job.遍历OperatorTree,翻译成MR任务. 对输出表生成MoveTask 从OperatorTree的其中一个根节点向下深度优先遍历 ReduceSinkOperator标示Map/Reduce的界限，多个Job间的界限 遍历其他根节点，遇过碰到JoinOperator合并MapReduceTask 生成StatTask更新元数据 剪断Map与Reduce间的Operator的关系 优化任务. 使用物理优化器对MR任务进行优化,生成最终执行任务 Hive UDF简单介绍在Hive中，用户可以自定义一些函数，用于扩展HiveQL的功能，而这类函数叫做UDF（用户自定义函数）。UDF分为两大类：UDAF（用户自定义聚合函数）和UDTF（用户自定义表生成函数）。Hive有两个不同的接口编写UDF程序。一个是基础的UDF接口，一个是复杂的GenericUDF接口。org.apache.hadoop.hive.ql. exec.UDF 基础UDF的函数读取和返回基本类型，即Hadoop和Hive的基本类型。如，Text、IntWritable、LongWritable、DoubleWritable等。org.apache.hadoop.hive.ql.udf.generic.GenericUDF 复杂的GenericUDF可以处理Map、List、Set类型。 按照学生科目取每个科目的TopN原始数据： 123456789id,name,subject,score1,小明,语文,872,张三,语文,273,王五,语文,694,李四,语文,995,小明,数学,866,马六,数学,337,李四,数学,448,小红,数学,50 sql： 123select a.* from(select id,name,subject,score,row_number() over(partition by subject order by score desc) rank from student) awhere a.rank &lt;= 3 Spark讲一下Spark的运行架构Cluster Manager(Master)：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。Driver： 运行Application 的main()函数Executor：执行器，是为某个Application运行在worker node上的一个进程 一个Spark程序的执行流程 A -&gt; 当 Driver 进程被启动之后,首先它将发送请求到Master节点上,进行Spark应用程序的注册B -&gt; Master在接受到Spark应用程序的注册申请之后,会发送给Worker,让其进行资源的调度和分配.C -&gt; Worker 在接受Master的请求之后,会为Spark应用程序启动Executor, 来分配资源D -&gt; Executor启动分配资源好后,就会想Driver进行反注册,这是Driver已经知道哪些Executor为他服务了E -&gt; 当Driver得到注册了Executor之后,就可以开始正式执行spark应用程序了. 首先第一步,就是创建初始RDD,读取数据源,再执行之后的一系列算子. HDFS文件内容被读取到多个worker节点上,形成内存中的分布式数据集,也就是初始RDDF -&gt; Driver就会根据 Job 任务任务中的算子形成对应的task,最后提交给 Executor, 来分配给task进行计算的线程G -&gt; task就会去调用对应的任务数据来计算,并task会对调用过来的RDD的partition数据执行指定的算子操作,形成新的RDD的partition,这时一个大的循环就结束了后续的RDD的partition数据又通过Driver形成新的一批task提交给Executor执行,循环这个操作,直到所有的算子结束 Spark的shuffle#### #### #### #### #### #### #### #### #### #### #### #### #### HBaseHBase的架构和基本原理Hbase以表的方式组织数据，表由行（Row）以及列（Column）组成，行由row key和一个或多个列及其值组成（存储是按照row key的字典顺序排序，row key的设计非常重要！！），列必须属于某一列族（Column family），一个列族可以有一各或多个列（一列由列簇和列修饰符组成，他们通常由冒号（：） 分隔），其在存储架构中就是一个Hfile。Hbase中的列可以达到百万级，列中的数据可以是稀疏的，空值并不占用存储空间。数据按主键排序，同时表按主键划分为多个Region。底层是LSM树（Long-Structed Merge Tree）。 对于以上叙述，表的简略结构（逻辑模型）： HBase简略架构图： 接下来对Zookeeper、HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog做简单叙述。 Zookeeper 保证任何时候，集群中只有一个master（负责多HMaster的选举） 存贮所有Region的寻址入口 实时监控RegionServer的状态，将RegionServer的上线和下线信息实时通知给Master（服务器之间状态同步） 存储HBase的schema（元数据信息）。包括有哪些table、每个table有哪些column family等 HMaster主要负责table和region的管理工作。 Region分裂后，为RegionServer分配新的Region 负责RegionServer的负载均衡，调整region的分配 发现失效的RegionServer，并重新分配其上的region 管理用户对table的增、删、改、查操作 监听zk，基于zookeeper感应rs的上下线 监听zk，基于zookeeper来保证HA 处理schema更新请求（管理用户对表的增删改） 不参与对表的读写访问 负载很低 无单点故障 在一个RegionServer死机后，负责失效节点的Region迁移 HRegionServer主要负责响应用户对其上region的I/O请求，向HDFS读写数据。 HRegionServer维护HMaster分配给它的region 维护region的cache 处理region的flush、compact、split 内部管理一系列的HRegion对象 一个HRegionServer会有多个HRegion和一个HLog HRegion也就是指一个Table的分区。 每一个HRegion又由很多的Store组成，每一个Store存储的实际上是一个列簇（ColumnFamily）下所有的数据。此外，在每一个Store（又名HStore）中有包含一块MemStore。MemStore驻留在内存中，数据到来时首先更新到MemStore中，当到达阈值之后再flush（默认64M）到对应的StoreFile（又名HFile）中，所以每一个Store包含多个StoreFile，StoreFile负责的是实际数据存储，为HBase中最小的存储单元。 达到某个阈值时，分裂（默认256M）。所以一个HRegionServer管理多个表，一个表下有多个Region，一个HRegion有多少个列族就有多少个Store,Store下有多个StoreFile文件，是HBase中最小的存储单元。 以Region为单位管理, region(startKey,endKey)；【默认情况下，刚创建一个表时，并不知道startkey和endkey】 每个Column Family单独存储：storeFile；（ storefile的数量一多（到达阀值），就合并（同时合并版本以及删除之前要删除的数据）；合并后大小到达阀值就split） 当某个Column Family累积的大小（具体的数据量） &gt; 某阈值时，自动分裂成两个Region；合并之后，旧数据也不是立即删除，而是复制一份并同内存中的数据一起写到磁盘，在之后，LSM-Tree会提供一些机制来回收这些空间。 如何找到某行属于哪个region呢？两张特殊的表：-NAMESPACE- 和.META. StoreFile底层存储格式是HFile，HBase中最小的存储单元。memStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。 HFile HBase中KeyValue数据的存储格式，是hadoop的二进制格式文件。 HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。 Trailer中有指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。 - Data Block是hbase io的基本单元，为了提高效率，HRegionServer中有基于LRU的block cache机制。 每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB），大号的Block有利于顺序Scan，小号的Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成 Magic内容就是一些随机数字，目的是防止数据损坏 HLogHLog是WAL的核心实现类。 WAL意为write ahead log，HBase中的预写日志，用来做灾难恢复使用，底层实现是HLog，HLog记录数据的所有变更。使用WAL的原因：因为MemStore存储的数据是驻留在内存中的，是不稳定的（比如宕机时），所以采用了WAL预写日志来解决这个问题。（运行MApReduce作业时，可以通过关闭WAL功能来获得性能的提升——setWriteToWAL(boolean)） 其实HLog文件就是一个普通的Hadoop Sequence File， Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，sequence number的起始值为0，或者是最近一次存入文件系统中的sequence number。 Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。[1] 与传统关系型数据库的区别数据类型：没有数据类型，都是字节数组（有一个工具类Bytes，将java对象序列化为字节数组）。数据操作：HBase只有很简单的插入、查询、删除、清空等操作，表和表之间是分离的，没有复杂的表和表之间的关系，而传统数据库通常有各式各样的函数和连接操作。存储模式：Hbase适合于非结构化数据存储，基于列存储而不是行。数据维护：HBase的更新操作不应该叫更新，它实际上是插入了新的数据，而传统数据库是替换修改时间版本：Hbase数据写入cell时，还会附带时间戳，默认为数据写入时RegionServer的时间，但是也可以指定一个不同的时间。数据可以有多个版本。可伸缩性，Hbase这类分布式数据库就是为了这个目的而开发出来的，所以它能够轻松增加或减少硬件的数量，并且对错误的兼容性比较高。而传统数据库通常需要增加中间层才能实现类似的功能 HBase的应用场景 半结构化或非结构化数据:对于数据结构字段不够确定或杂乱无章非常难按一个概念去进行抽取的数据适合用HBase，因为HBase支持动态添加列。 记录很稀疏：RDBMS的行有多少列是固定的。为null的列浪费了存储空间。而如上文提到的，HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。 多版本号数据：依据Row key和Column key定位到的Value能够有随意数量的版本号值，因此对于须要存储变动历史记录的数据，用HBase是很方便的。比方某个用户的Address变更，用户的Address变更记录也许也是具有研究意义的。 仅要求最终一致性：对于数据存储事务的要求不像金融行业和财务系统这么高，只要保证最终一致性就行。（比如HBase+elasticsearch时，可能出现数据不一致） 高可用和海量数据以及很大的瞬间写入量：WAL解决高可用，支持PB级数据，put性能高索引插入比查询操作更频繁的情况。比如，对于历史记录表和日志文件。（HBase的写操作更加高效） 业务场景简单：不需要太多的关系型数据库特性，列入交叉列，交叉表，事务，连接等。 HBase 宕机如何处理？宕机分为HMaster宕机和HRegionServer宕机，如果是HRegionServer宕机，HMaster会将其所管理的region重新分布到其他活动的RegionServer上，由于数据和日志都持久在HDFS中，该操作不会导致数据丢失。所以数据的一致性和安全性是有保障的。 如果是HMaster宕机，HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。即Zookeeper会保证总会有一个HMaster在对外提供服务。 HRegionServer宕机如何处理？ ZooKeeper 会监控 HRegionServer 的上下线情况，当 ZK 发现某个 HRegionServer 宕机之后会通知 HMaster 进行失效备援； HRegionServer 会停止对外提供服务，就是它所负责的 region 暂时停止对外提供服务 HMaster 会将该 HRegionServer 所负责的 region 转移到其他 HRegionServer 上，并且会对 HRegionServer 上存在 memstore 中还未持久化到磁盘中的数据进行恢复; 这个恢复的工作是由 WAL重播 来完成，这个过程如下：wal实际上就是一个文件，存在/hbase/WAL/对应RegionServer路径下宕机发生时，读取该RegionServer所对应的路径下的wal文件，然后根据不同的region切分成不同的临时文件recover.edits当region被分配到新的RegionServer中，RegionServer读取region时会进行是否存在recover.edits，如果有则进行恢复 HBase写数据的流程 Client先访问zookeeper，从.META.表获取相应region信息，然后从meta表获取相应region信息 根据namespace、表名和rowkey根据meta表的数据找到写入数据对应的region信息 找到对应的regionserver 把数据先写到WAL中，即HLog，然后写到MemStore上 MemStore达到设置的阈值后则把数据刷成一个磁盘上的StoreFile文件。 当多个StoreFile文件达到一定的大小后(这个可以称之为小合并，合并数据可以进行设置，必须大于等于2，小于10——hbase.hstore.compaction.max和hbase.hstore.compactionThreshold，默认为10和3)，会触发Compact合并操作，合并为一个StoreFile，（这里同时进行版本的合并和数据删除。） 当Storefile大小超过一定阈值后，会把当前的Region分割为两个（Split）【可称之为大合并，该阈值通过hbase.hregion.max.filesize设置，默认为10G】，并由Hmaster分配到相应的HRegionServer，实现负载均衡 HBase读数据的流程 首先，客户端需要获知其想要读取的信息的Region的位置，这个时候，Client访问hbase上数据时并不需要Hmaster参与（HMaster仅仅维护着table和Region的元数据信息，负载很低），只需要访问zookeeper，从meta表获取相应region信息(地址和端口等)。【Client请求ZK获取.META.所在的RegionServer的地址。】 客户端会将该保存着RegionServer的位置信息的元数据表.META.进行缓存。然后在表中确定待检索rowkey所在的RegionServer信息（得到持有对应行键的.META表的服务器名）。【获取访问数据所在的RegionServer地址】 根据数据所在RegionServer的访问信息，客户端会向该RegionServer发送真正的数据读取请求。服务器端接收到该请求之后需要进行复杂的处理。 先从MemStore找数据，如果没有，再到StoreFile上读(为了读取的效率)。 HBase和Hive有什么区别？Hive与HBase的底层存储是什么？Hive产生的原因是什么？HBase是为了弥补Hadoop的什么缺陷？共同点： 都是架构在hadoop之上的，都是用Hadoop作为底层存储 区别： - Hive是建立在Hadoop之上为了减少MapReducejobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。 - 想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop，如果是索引访问，就用HBase+Hadoop； - Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多； - Hive本身不存储和计算数据，它完全依赖于 HDFS 和 MapReduce，Hive中的表纯逻辑； - hive借用hadoop的MapReduce来完成一些hive中的命令的执行； - hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作； - hbase是列存储； - hdfs 作为底层存储，hdfs 是存放文件的系统，而 Hbase 负责组织文件； - hive 需要用到 hdfs 存储文件，需要用到 MapReduce 计算框架。 解释下HBase实时查询的原理实时查询，可以认为是从内存中查询，一般响应时间在 1 秒内。HBase 的机制是数据先写入到内存中，当数据量达到一定的量（如 128M），再写入磁盘中， 在内存中，是不进行数据的更新或合并操作的，只增加数据，这使得用户的写操作只要进入内存中就可以立即返回，保证了 HBase I/O 的高性能。 描述HBase的rowkey的设计联系region和rowkey关系说明，设计可参考以下三个原则： rowkey长度原则 rowkey 是一个二进制码流，可以是任意字符串，最大长度 64kb，实际应用中一般为 10-100bytes，以 byte[] 形式保存，一般设计成定长。建议越短越好，不要超过 16 个字节， 原因如下： 数据的持久化文件 HFile 中是按照 KeyValue 存储的，如果 rowkey 过长会极大影响 HFile 的存储效率 MemStore 将缓存部分数据到内存，如果 rowkey 字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。 rowkey散列原则 如果 rowkey 按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将 rowkey 的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个 RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个 RegionServer 上，这样在数据检索的时候负载会集中在个别的 RegionServer 上，造成热点问题，会降低查询效率。 rowkey唯一原则 必须在设计上保证其唯一性，rowkey 是按照字典顺序排序存储的，因此， 设计 rowkey 的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。 描述HBase中scan和get的功能以及实现的异同 按指 定 RowKey 获 取 唯 一 一 条 记 录 ， get 方法（ org.apache.hadoop.hbase.client.Get ） Get的方法处理分两种 ： 设置了ClosestRowBefore和没有设置的 rowlock 主要是用来保证行的事务性，即每个get 是以一个 row 来标记的.一个 row 中可以有很多 family 和 column。 按指定的条件获取一批记录，scan 方法(org.apache.Hadoop.hbase.client.Scan)实现条件查询功能使用的就是 scan 方式scan 可以通过 setCaching 与 setBatch 方法提高速度(以空间换时间)；scan 可以通过 setStartRow 与 setEndRow 来限定范围([start，end]start? 是闭区间，end 是开区间)。范围越小，性能越高；scan 可以通过 setFilter 方法添加过滤器，这也是分页、多条件查询的基础。 3.全表扫描，即直接扫描整张表中所有行记录。 简述HBase中的compact的用途？什么时候触发？分为哪两种？在 HBase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile， 当 storeFile 的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。Compact 的作用： 合并文件清除过期，多余版本的数据提高读写数据的效率 Minor操作只合并小文件，对TTL过期数据设置过期清理，不会对文件内容进行清除操作。 Major操作对 Region 下同一个 Column family 的 StoreFile 合并为一个大文件，并且清除删除、过期、多余版本的数据。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库-MySQL]]></title>
    <url>%2F2020%2F01%2F10%2F%E6%95%B0%E6%8D%AE%E5%BA%93-MySQL%2F</url>
    <content type="text"><![CDATA[XXX 问题EXPLAIN你的Select查询使用 EXPLAIN 关键字可以让你知道 MySQL 是如何处理你的 SQL 语句的。这可以帮你分析你的查询语句或是表结构的性能瓶颈。EXPLAIN 的查询结果还会告诉你你的索引主键被如何利用的，你的数据表是如何被搜索和排序的…… 当只有一行数据时，使用LIMIT 1当你查询表的有些时候，你已经知道结果只会有一条结果，但因为你可能需要去 fetch 游标，或是你也许会去检查返回的记录数。在这种情况下，加上 LIMIT 1 可以增加性能。这样一来， MySQL 数据库引擎会在找到一条数据后停止搜索，而不是继续往后查少下一条符合记录的数据。 为搜索字段建索引索引并不一定就是给主键或是唯一字段的，如果表中的某个字段会经常用来做搜索，那么，请为其建立索引吧。 在Join表的时候使用相当类型的列，并将其索引如果你的应用程序有很多Join查询，应该确认两个表中Join的字段是被建过索引的，这样，MySQL内部会启动为你优化join的SQL语句的机制。而且，这些被用来join的字段，应该是相同类型的。例如：如果你要把Decimal字段和一个int字段join在一起，MySQL就无法使用它们的索引。对于那些String类型，需要有相同的字符集才行。 千万不要 order by rand()避免 select *从数据库里读出越多的数据，那么查询就会越慢。并且，如果你的数据库服务器和web服务器是两台独立的服务器的话，这还会增加网络传输的负载。所以，你应该养成一个需要什么就取什么的好习惯。 永远为每张表设置一个ID我们应该为数据库里的每张表都设置一个 ID 做为其主键，而且最好的是一个 INT 型的(推荐使用 UNSIGNED)，并设置上自动增加的 AUTO_INCREMENT 标志。就算是你 users 表有一个主键叫 “email”的字段，你也别让它成为主键。使用 VARCHAR 类型来当主键会使用得性能下降。另外，在你的程序中，你应该使用表的 ID 来构造你的数据结构。而且，在 MySQL 数据引擎下，还有一些操作需要使用主键，在这些情况下，主键的性能和设置变得非常重要，比如，集群，分区…… 使用 enum 而不是 varchar无缓冲的查询把 IP 地址换成 unsigned int固定长度的表会更快垂直分割拆分大的 delete 或 insert 语句如果你需要在一个在线的网站上去执行一个大的 delete 或 insert 查询，你需要非常小心，要避免你的操作让你的整个网站停止相应。因为这两个操作是会锁表的，表一锁住了，别的操作都进不来了。Apache 会有很多的子进程或线程。所以，其工作起来相当有效率，而我们的服务器也不希望有太多的子进程，线程和数据库链接，这是极大的占服务器资源的事情，尤其是内存。如果你把你的表锁上一段时间，比如 30 秒钟，那么对于一个有很高访问量的站点来说，这 30 秒所积累的访问进程/线程，数据库链接，打开的文件数，可能不仅仅会让你的 web 服务 Crash，还可能会让你的整台服务器马上掛了。所以，如果你有一个大的处理，你定你一定把其拆分，使用 LIMIT 条件是一个好的方法。 越小的列会越快如果一个表只会有几列罢了(比如说字典表，配置表)，那么，我们就没有理由使用 INT 来做主键，使用 MEDIUMINT, SMALLINT 或是更小的 TINYINT 会更经济一些。如果你不需要记录时间，使用 DATE 要比 DATETIME 好得多。 选择正确的存储引擎MyISAM 适合于一些需要大量查询的应用，但其对于有大量写操作并不是很好。甚至你只是需要 update 一个字段，整个表都会被锁起来，而别的进程，就算是读进程都无法操作直到读操作完成。另外， MyISAM 对于 select count(*) 这类的计算是超快无比的。 InnoDB 的趋势会是一个非常复杂的存储引擎，对于一些小的应用，它会比MyISAM 还慢。他是它支持“行锁” ，于是在写操作比较多的时候，会更优秀。并且，他还支持更多的高级应用，比如：事务 小心“永久链接”“永久链接”的目的是用来减少重新创建 MySQL 链接的次数。当一个链接被创建了，它会永远处在连接的状态，就算是数据库操作已经结束了。而且，自从我们的 Apache 开始重用它的子进程后——也就是说，下一次的 HTTP 请求会重用 Apache 的子进程，并重用相同的 MySQL 链接。 PHP 手册： mysql_pconnect() 在理论上来说，这听起来非常的不错。但是从个人经验(也是大多数人的)上来说，这个功能制造出来的麻烦事更多。因为，你只有有限的链接数，内存问题，文件句柄数，等等。而且， Apache 运行在极端并行的环境中，会创建很多很多的了进程。这就是为什么这种“永久链接”的机制工作地不好的原因。在你决定要使用“永久链接”之前，你需要好好地考虑一下你的整个系统的架构。 尽可能的使用 not null除非你有一个很特别的原因去使用 NULL 值，你应该总是让你的字段保持NOT NULL。这看起来好像有点争议，请往下看。 首先，问问你自己“Empty”和“NULL”有多大的区别(如果是 INT，那就是 0 和 NULL)?如果你觉得它们之间没有什么区别，那么你就不要使用 NULL。(你知道吗?在 Oracle 里， NULL 和 Empty 的字符串是一样的!) 不要以为 NULL 不需要空间，其需要额外的空间，并且，在你进行比较的时候，你的程序会更复杂。 当然，这里并不是说你就不能使用 NULL 了，现实情况是很复杂的，依然会有些情况下，你需要使用 NULL 值。 MySQL中有哪几种锁？MyISAM 支持表锁， InnoDB 支持表锁和行锁，默认为行锁表级锁：开销小，加锁快，不会出现死锁。锁定粒度大，发生锁冲突的概率最高，并发量最低行级锁：开销大，加锁慢，会出现死锁。锁力度小，发生锁冲突的概率小，并发度最高 数据库设计范式第一范式（1NF）强调的是列的原子性，即列不能够再分成其他几列。考虑这样一个表：【联系人】（姓名，性别，电话）如果在实际场景中，一个联系人有家庭电话和公司电话，那么这种表结构设计就没有达到 1NF。要符合 1NF 我们只需把列（电话）拆分，即：【联系人】（姓名，性别，家庭电话，公司电话）。1NF 很好辨别，但是 2NF 和 3NF 就容易搞混淆。 说明：在任何一个关系数据库中，第一范式（1NF）是对关系模式的设计基本要求，一般设计中都必须满足第一范式（1NF）。不过有些关系模型中突破了1NF的限制，这种称为非1NF的关系模型。换句话说，是否必须满足1NF的最低要求，主要依赖于所使用的关系模型。 第二范式（2NF）首先是 1NF，另外包含两部分内容，一是表必须有一个主键；二是没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。 考虑一个订单明细表：【OrderDetail】（OrderID，ProductID，UnitPrice，Discount，Quantity，ProductName）。因为我们知道在一个订单中可以订购多种产品，所以单单一个 OrderID 是不足以成为主键的，主键应该是（OrderID，ProductID）。显而易见 Discount（折扣），Quantity（数量）完全依赖（取决）于主键（OderID，ProductID），而 UnitPrice，ProductName 只依赖于 ProductID。所以 OrderDetail 表不符合 2NF。不符合 2NF 的设计容易产生冗余数据。可以把【OrderDetail】表拆分为【OrderDetail】（OrderID，ProductID，Discount，Quantity）和【Product】（ProductID，UnitPrice，ProductName）来消除原订单表中UnitPrice，ProductName多次重复的情况。 第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的唯一标识。简而言之，第二范式就是在第一范式的基础上属性完全依赖于主键。 第三范式（3NF）在1NF基础上，任何非主属性不依赖于其它非主属性[在2NF基础上消除传递依赖]。 第三范式（3NF）是第二范式（2NF）的一个子集，即满足第三范式（3NF）必须满足第二范式（2NF）。 首先是 2NF，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。 考虑一个订单表【Order】（OrderID，OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity）主键是（OrderID）。 其中 OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity 等非主键列都完全依赖于主键（OrderID），所以符合 2NF。不过问题是 CustomerName，CustomerAddr，CustomerCity 直接依赖的是 CustomerID（非主键列），而不是直接依赖于主键，它是通过传递才依赖于主键，所以不符合 3NF。通过拆分【Order】为【Order】（OrderID，OrderDate，CustomerID）和【Customer】（CustomerID，CustomerName，CustomerAddr，CustomerCity）从而达到 3NF。 第二范式（2NF）和第三范式（3NF）的概念很容易混淆，区分它们的关键点在于，2NF：非主键列是否完全依赖于主键，还是依赖于主键的一部分；3NF：非主键列是直接依赖于主键，还是直接依赖于非主键列。 列举常见的关系型数据库和非关系型数据库？关系型数据库：Oracle、DB2、Microsoft SQL Server、Microsoft Access、MySQL非关系型数据库：NoSql、Cloudant、MongoDb、redis、HBase 关系型数据库关系型数据库的特性1、关系型数据库，是指采用了关系模型来组织数据的数据库；2、关系型数据库的最大特点就是事务的一致性；3、简单来说，关系模型指的就是二维表格模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。关系型数据库的优点1、容易理解：二维表结构是非常贴近逻辑世界一个概念，关系模型相对网状、层次等其他模型来说更容易理解；2、使用方便：通用的SQL语言使得操作关系型数据库非常方便；3、易于维护：丰富的完整性(实体完整性、参照完整性和用户定义的完整性)大大减低了数据冗余和数据不一致的概率；4、支持SQL，可用于复杂的查询。关系型数据库的缺点1、为了维护一致性所付出的巨大代价就是其读写性能比较差；2、固定的表结构；3、高并发读写需求；4、海量数据的高效率读写； 非关系型数据库非关系型数据库的特性1、使用键值对存储数据；2、分布式；3、一般不支持ACID特性；4、非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合。非关系型数据库的优点1、无需经过sql层的解析，读写性能很高；2、基于键值对，数据没有耦合性，容易扩展；3、存储数据的格式：nosql的存储格式是key,value形式、文档形式、图片形式等等，文档形式、图片形式等等，而关系型数据库则只支持基础类型。非关系型数据库的缺点1、不提供sql支持，学习和使用成本较高；2、无事务处理，附加功能bi和报表等支持也不好； 常见的MySQL数据库引擎是MyiSAM和inmoDB比较：MyISAM：1、它不支持事务，也不支持外键，其优势是访问的速度快innoDB：1、InnoDB支持事务安全，对比MyISAM引擎，InnoDB写的效率差一些，并且会占据更多的磁盘空间。2、MySQL支持外键的存储引擎只有InnoDB 什么是事务？什么是锁？MySQL如何支持事务？事务：主要用于处理操作量大，复杂度高的数据。一般来说，事务是必须满足4个条件（ACID）：原子性：所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行，而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 锁：是实现事务的关键，所可以保证事务的完整性和并发行，与现实生活中的锁一样，可以使某些数据的拥有者，在某段时间内不能使用某些数据或者数据结构。 注意：MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 简述触发器、函数、视图、存储过程触发器:使用触发器可以定制用户对表进行【增、删、改】操作时前后的行为,触发器无法由用户直接调用，而知由于对表的【增/删/改】操作被动引发的函数:是MySQL数据库提供的内部函数(当然也可以自定义函数)。这些内部函数可以帮助用户更加方便的处理表中的数据视图:视图是虚拟表或逻辑表，它被定义为具有连接的SQL SELECT查询语句。存储过程:存储过程是存储在数据库目录中的一坨的声明性SQL语句，数据库中的一个重要对象,有效提高了程序的性能 MySQL索引种类普通索引、唯一索引、联合索引、全文索引、空间索引 索引在什么情况下遵循最左前缀的规则？在建立了联合索引的前提条件下，数据库会一直从左向右的顺序依次查找，直到遇到了范围查询(&gt;,&lt;,between,like等) 列举创建索引但是无法命中索引的8种情况 查询条件中有or、not in、not exist等 小表查询 like查询是以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 没有使用索引字段查询 对索引列进行运算，需要建立函数索引 单独引用联合索引中的非第一位置的索引 没有查询条件 如何开启慢日志查询？参数说明：slow_query_log： 慢查询开启状态slow_query_log_file：慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）long_query_time：查询超过多少秒才记录开启慢日志查询方法一：全局变量设置1、将 slow_query_log 全局变量设置为“ON”状态。指令示例：mysql&gt; set global slow_query_log=’ON’;2、设置慢查询日志存放的位置：mysql&gt; set global slow_query_log_file=’/usr/local/mysql/data/slow.log’;3、查询超过1秒就记录：mysql&gt; set global long_query_time=1;开启慢日志查询方法二： 配置文件设置修改配置文件my.cnf，在[mysqld]下的下方加入：[mysqld]slow_query_log = ONslow_query_log_file = /usr/local/mysql/data/slow.loglong_query_time = 1修改完参数后重启数据库服务就可以了 数据库导入导出命令（结构和数据）？导出数据库结构和数据：mysqldump -u用户名 -p密码 数据库名&gt; 路径+导出的文件名.sql导出数据库所有表结构：mysqldump -u用户名 -p密码 -d 数据库名&gt;路径+文件名.sql导出数据表结构和数据：mysqldump -u用户名 -p密码 数据库名 表名&gt;路径+文件名.sql导出数据表结构：mysqldump -u用户名 -p密码 -d 数据库名 表名&gt;路径+文件名.sql 数据库优化思路SQL语句优化1）应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。2）应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=03）很多时候用 exists 代替 in 是一个好的选择4）用Where子句替换HAVING 子句 因为HAVING 只会在检索出所有记录之后才对结果集进行过滤 索引优化合理使用：普通索引、唯一索引(主键索引、唯一索引)、联合索引、全文索引、空间索引 MySQL的最大索引个数不能超过5个，为啥？？？ MySQL一张表的数据量不能超过2000万，为啥？？？ 数据库结构优化1）范式优化：比如消除冗余（节省空间。。）2）反范式优化：比如适当加冗余等（减少join）3）拆分表：分区将数据在物理上分隔开，不同分区的数据可以制定保存在处于不同磁盘上的数据文件里。这样，当对这个表进行查询时，只需要在表分区中进行扫描，而不必进行全表扫描，明显缩短了查询时间，另外处于不同磁盘的分区也将对这个表的数据传输分散在不同的磁盘I/O，一个精心设置的分区可以将数据传输对磁盘I/O竞争均匀地分散开。对数据量大的时时表可采取此方法。可按月自动建表分区。4）拆分其实又分垂直拆分和水平拆分：案例： 简单购物系统暂设涉及如下表：1.产品表（数据量10w，稳定）2.订单表（数据量200w，且有增长趋势）3.用户表 （数据量100w，且有增长趋势） 以mysql为例讲述下水平拆分和垂直拆分，mysql能容忍的数量级在百万静态数据可以到千万。垂直拆分：解决问题：表与表之间的io竞争 。不解决问题：单表中数据量增长出现的压力。方案： 把产品表和用户表放到一个server上 订单表单独放到一个server上水平拆分： 解决问题：单表中数据量增长出现的压力 不解决问题：表与表之间的io争夺。方案： 用户表通过性别拆分为男用户表和女用户表，订单表通过已完成和完成中拆分为已完成订单和未完成订单， 产品表、未完成订单放一个server上；已完成订单表和男用户表放一个server上；女用户表放一个server上(女的爱购物 哈哈) 服务器硬件优化limit 1在对name做了唯一索引前提下，简述以下区别：select * from tb where name = ‘Oldboy-Wupeiqi’ # 全局遍历找所有select * from tb where name = ‘Oldboy-Wupeiqi’ limit 1 # 锁定一条就结束 什么是数据库约束，常见的约束有哪几种？数据库约束用于保证数据库、表数据的完整性（正确性和一致性）。可以通过定义约束\索引\触发器来保证数据的完整性。总体来讲,约束可以分为:主键约束：primary key；外键约束：foreign key；唯一约束：unique；检查约束：check；空值约束：not null；默认值约束：default； 从数据库中随机取50条数据select * from 表 order by rand() limit 50; 什么是sql注入？SQL注入攻击指的是通过构建特殊的输入作为参数传入Web应用程序，而这些输入大都是SQL语法里的一些组合，通过执行SQL语句进而执行攻击者所要的操作，其主要原因是程序没有细致地过滤用户输入的数据，致使非法数据侵入系统。 关于sql语句应该考虑哪些安全性？ 防止sql注入，对特殊字符进行转义，过滤或者使用预编译的sql语句绑定变量。 最小权限原则，特别是不要用root账户，为不同的类型的动作或者组建使用不同的账户。 当sql运行出错时，不要把数据库返回的错误信息全部显示给用户，以防止泄漏服务器和数据库相关信息。 一张表，里面有ID自增主键，当insert了17条记录之后，删除了第15，16，17条记录，再把MySQL重启，再insert一条记录，这条记录的ID是18还是15？如果表的类型是MyISAM，那么是18。因为MyISAM表会把自增主键的最大ID记录到数据文件里，重启MySQL自增主键的最大ID也不会丢失。如果表的类型是InnoDB，那么是15。InnoDB表只是把自增主键的最大ID记录到内存中，所以重启数据库或者是对表进行OPTIMIZE操作，都会导致最大ID丢失。 简述数据库的读写分离？读写分离为了确保数据库产品的稳数据定性，很多数据库拥有双机热备功能。也就是，第一台数据库服务器，是对外提供增删改业务的生产服务器；第二台数据库服务器，主要进行读的操作。 简述数据库分库分表？（水平、垂直）垂直分库：就是按照功能的不同，把没有关联的数据放到不同的数据库和服务器中水平分库：根据一定的规则将一个表的数据划分到不同的数据库中，两个数据库的表结构一样 mysql的锁类型行锁、表锁、页面锁 排他锁和共享锁mysql索引是怎么实现的？大致描述下B+树的大致结构事务及其特性事务拥有四个重要的特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability），人们习惯称之为 ACID 特性。下面我逐一对其进行解释。 原子性（Atomicity）事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。例如，如果一个事务需要新增 100 条记录，但是在新增了 10 条记录之后就失败了，那么数据库将回滚对这 10 条新增的记录。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 一致性（Consistency）指事务将数据库从一种状态转变为另一种一致的的状态。事务开始前和结束后，数据库的完整性约束没有被破坏。例如工号带有唯一属性，如果经过一个修改工号的事务后，工号变的非唯一了，则表明一致性遭到了破坏。 隔离性（Isolation）要求每个读写事务的对象对其他事务的操作对象能互相分离，即该事务提交前对其他事务不可见。 也可以理解为多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。这指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。例如一个用户在更新自己的个人信息的同时，是不能看到系统管理员也在更新该用户的个人信息（此时更新事务还未提交）。 注：MySQL 通过锁机制来保证事务的隔离性。 持久性（Durability）事务一旦提交，则其结果就是永久性的。即使发生宕机的故障，数据库也能将数据恢复，也就是说事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。这只是从事务本身的角度来保证，排除 RDBMS（关系型数据库管理系统，例如 Oracle、MySQL 等）本身发生的故障。 注：MySQL 使用 redo log 来保证事务的持久性。 mysql事务隔离级别READ UNCOMMITTED（读未提交）该隔离级别的事务会读到其它未提交事务的数据，此现象也称之为脏读。 准备两个终端，在此命名为 mysql 终端 1 和 mysql 终端 2，再准备一张测试表 test，写入一条测试数据并调整隔离级别为 READ UNCOMMITTED，任意一个终端执行即可。 12345SET @@session.transaction_isolation = 'READ-UNCOMMITTED';create database test;use test;create table test(id int primary key);insert into test(id) values(1); 登录 mysql 终端 1，开启一个事务，将 ID 为 1 的记录更新为 2。 123begin;update test set id = 2 where id = 1;select * from test; -- 此时看到一条ID为2的记录 登录 mysql 终端 2，开启一个事务后查看表中的数据。 123use test;begin;select * from test; -- 此时看到一条 ID 为 2 的记录 最后一步读取到了 mysql 终端 1 中未提交的事务（没有 commit 提交动作），即产生了脏读，大部分业务场景都不允许脏读出现，但是此隔离级别下数据库的并发是最好的。 READ COMMITTED (读提交)一个事务可以读取另一个已提交的事务，多次读取会造成不一样的结果，此现象称为不可重复读问题，Oracle 和 SQL Server 的默认隔离级别。 准备两个终端，在此命名为 mysql 终端 1 和 mysql 终端 2，再准备一张测试表 test，写入一条测试数据并调整隔离级别为 READ COMMITTED，任意一个终端执行即可。 12345SET @@session.transaction_isolation = 'READ-COMMITTED';create database test;use test;create table test(id int primary key);insert into test(id) values(1); 登录 mysql 终端 1，开启一个事务，将 ID 为 1 的记录更新为 2，并确认记录数变更过来。 123begin;update test set id = 2 where id = 1;select * from test; -- 此时看到一条记录为 2 登录 mysql 终端 2，开启一个事务后，查看表中的数据。 123use test;begin;select * from test; -- 此时看一条 ID 为 1 的记录 登录 mysql 终端 1，提交事务。 1commit; 切换到 mysql 终端 2。 1select * from test; -- 此时看到一条 ID 为 2 的记录 mysql 终端 2 在开启了一个事务之后，在第一次读取 test 表（此时 mysql 终端 1 的事务还未提交）时 ID 为 1，在第二次读取 test 表（此时 mysql 终端 1 的事务已经提交）时 ID 已经变为 2，说明在此隔离级别下已经读取到已提交的事务。 REPEATABLE READ (可重复读)该隔离级别是 MySQL 默认的隔离级别，在同一个事务里，select 的结果是事务开始时时间点的状态，因此，同样的 select 操作读到的结果会是一致的，但是，会有幻读现象。MySQL 的 InnoDB 引擎可以通过 next-key locks 机制（参考下文”行锁的算法”一节）来避免幻读。 准备两个终端，在此命名为 mysql 终端 1 和 mysql 终端 2，准备一张测试表 test 并调整隔离级别为 REPEATABLE READ，任意一个终端执行即可。 1234SET @@session.transaction_isolation = 'REPEATABLE-READ';create database test;use test;create table test(id int primary key,name varchar(20)); 登录 mysql 终端 1，开启一个事务。 12begin;select * from test; -- 无记录 登录 mysql 终端 2，开启一个事务。 12begin;select * from test; -- 无记录 切换到 mysql 终端 1，增加一条记录并提交。 12insert into test(id,name) values(1,'a');commit; 切换到 msyql 终端 2。 1select * from test; --此时查询还是无记录 通过这一步可以证明，在该隔离级别下已经读取不到别的已提交的事务，如果想看到 mysql 终端 1 提交的事务，在 mysql 终端 2 将当前事务提交后再次查询就可以读取到 mysql 终端 1 提交的事务。我们接着实验，看看在该隔离级别下是否会存在别的问题。 此时接着在 mysql 终端 2 插入一条数据。1insert into test(id,name) values(1,'b'); -- 此时报主键冲突的错误 也许到这里您心里可能会有疑问，明明在第 5 步没有数据，为什么在这里会报错呢？其实这就是该隔离级别下可能产生的问题，MySQL 称之为幻读。注意我在这里强调的是 MySQL 数据库，Oracle 数据库对于幻读的定义可能有所不同。 SERIALIZABLE (序列化)在该隔离级别下事务都是串行顺序执行的，MySQL 数据库的 InnoDB 引擎会给读操作隐式加一把读共享锁，从而避免了脏读、不可重读复读和幻读问题。 准备两个终端，在此命名为 mysql 终端 1 和 mysql 终端 2，分别登入 mysql，准备一张测试表 test 并调整隔离级别为 SERIALIZABLE，任意一个终端执行即可。 1234SET @@session.transaction_isolation = 'SERIALIZABLE';create database test;use test;create table test(id int primary key); 登录 mysql 终端 1，开启一个事务，并写入一条数据。 12begin;insert into test(id) values(1); 登录 mysql 终端 2，开启一个事务。 12begin;select * from test; -- 此时会一直卡住 立马切换到 mysql 终端 1,提交事务。 1commit; 一旦事务提交，msyql 终端 2 会立马返回 ID 为 1 的记录，否则会一直卡住，直到超时，其中超时参数是由 innodb_lock_wait_timeout 控制。由于每条 select 语句都会加锁，所以该隔离级别的数据库并发能力最弱，但是有些资料表明该结论也不一定对，如果感兴趣，您可以自行做个压力测试。 隔离级别 脏读 不可重复读 幻读 读未提交 可以出现 可以出现 可以出现 读提交 不允许出现 可以出现 可以出现 可重复读 不允许出现 不允许出现 可以出现 序列化 不允许出现 不允许出现 不允许出现 mysql中的锁锁也是数据库管理系统区别文件系统的重要特征之一。锁机制使得在对数据库进行并发访问时，可以保障数据的完整性和一致性。对于锁的实现，各个数据库厂商的实现方法都会有所不同。本文讨论 MySQL 中的 InnoDB 引擎的锁。 锁的类型InnoDB 实现了两种类型的行级锁： 共享锁（S锁）：允许事务读取一行数据。可以使用 SQL 语句 select * from tableName where … lock in share mode; 手动加 S 锁。 独占锁（X锁）：允许事务删除或更新一行数据。可以使用 SQL 语句 select * from tableName where … for update; 手动加 X 锁。 S 锁和 S 锁是兼容的，X 锁和其它锁都不兼容，举个例子，事务 T1 获取了一个行 r1 的 S 锁，另外事务 T2 可以立即获得行 r1 的 S 锁，此时 T1 和 T2 共同获得行 r1 的 S 锁，此种情况称为锁兼容，但是另外一个事务 T2 此时如果想获得行 r1 的 X 锁，则必须等待 T1 对行 r 锁的释放，此种情况也成为锁冲突。 为了实现多粒度的锁机制，InnoDB 还有两种内部使用的意向锁，由 InnoDB 自动添加，且都是表级别的锁。 意向共享锁（IS）：事务即将给表中的各个行设置共享锁，事务给数据行加 S 锁前必须获得该表的 IS 锁。 意向排他锁（IX）：事务即将给表中的各个行设置排他锁，事务给数据行加 X 锁前必须获得该表 IX 锁。 意向锁的主要目的是为了使得行锁和表锁共存。表 2 列出了行级锁和表级意向锁的兼容性。 行级锁和表级意向锁的兼容性： 锁类型 X IX S IS X 冲突 冲突 冲突 冲突 IX 冲突 兼容 冲突 兼容 S 冲突 冲突 兼容 兼容 IS 冲突 兼容 兼容 兼容 行锁的算法InnoDB 存储引擎使用三种行锁的算法用来满足相关事务隔离级别的要求。 Record Locks该锁为索引记录上的锁，如果表中没有定义索引，InnoDB 会默认为该表创建一个隐藏的聚簇索引，并使用该索引锁定记录。 Gap Locks该锁会锁定一个范围，但是不括记录本身。可以通过修改隔离级别为 READ COMMITTED 或者配置 innodb_locks_unsafe_for_binlog 参数为 ON。 Next-key Locks该锁就是 Record Locks 和 Gap Locks 的组合，即锁定一个范围并且锁定该记录本身。InnoDB 使用 Next-key Locks 解决幻读问题。需要注意的是，如果索引有唯一属性，则 InnnoDB 会自动将 Next-key Locks 降级为 Record Locks。举个例子，如果一个索引有 1, 3, 5 三个值，则该索引锁定的区间为 (-∞,1], (1,3], (3,5], (5,+ ∞)。 死锁死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。 InnoDB 引擎采取的是 wait-for graph 等待图的方法来自动检测死锁，如果发现死锁会自动回滚一个事务。 下面我们通过一个示例来了解死锁。 准备两个终端，在此命名为 mysql 终端 1 和 mysql 终端 2，分别登入 mysql，再准备一张测试表 test 写入两条测试数据，并调整隔离级别为 SERIALIZABLE，任意一个终端执行即可。 12345SET @@session.transaction_isolation = 'REPEATABLE-READ';create database test;use test;create table test(id int primary key);insert into test(id) values(1),(2); 登录 mysql 终端 1，开启一个事务，手动给 ID 为 1 的记录加 X 锁。 12begin;select * from test where id = 1 for update; 登录 mysql 终端 2，开启一个事务，手动给 ID 为 2 的记录加 X 锁。 12begin;select * from test where id = 2 for update; 切换到 mysql 终端 1，手动给 ID 为 2 的记录加 X 锁，此时会一直卡住，因为此时在等待第 3 步中 X 锁的释放，直到超时，超时时间由 innodb_lock_wait_timeout 控制。 1select * from test where id = 2 for update; 在锁超时前立刻切换到 mysql 终端 2，手动给 ID 为 1 的记录加 X 锁，此时又会等待第 2 步中 X 所的释放，两个终端都在等待资源的释放，所以 InnoDB 引擎会立马检测到死锁产生，自动回滚一个事务，以防止死锁一直占用资源。 12select * from test where id = 1 for update;ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction 此时，通过 show engine innodb status\G 命令可以看到 LATEST DETECTED DEADLOCK 相关信息，即表明有死锁发生；或者通过配置 innodb_print_all_deadlocks（MySQL 5.6.2 版本开始提供）参数为 ON 将死锁相关信息打印到 MySQL 的错误日志。 锁的优化建议锁如果利用不好，会给业务造成大量的卡顿现象，在了解了锁相关的一些知识点后，我们可以有意识的去避免锁带来的一些问题。 合理设计索引，让 InnoDB 在索引键上面加锁的时候尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定而影响其他 Query 的执行。 尽可能减少基于范围的数据检索过滤条件，避免因为间隙锁带来的负面影响而锁定了不该锁定的记录。 尽量控制事务的大小，减少锁定的资源量和锁定时间长度。 在业务环境允许的情况下，尽量使用较低级别的事务隔离，以减少 MySQL 因为实现事务隔离级别所带来的附加成本。 SQL语法SQL查询用户的最长连续登录天数有一个用户登录表格，表有两个字段 uid, time分别表示用户id与登录时间， 表名为 user_login,登录时间的格式为 yyyy-MM-dd HH:mm:ss请书写一段脚本，来统计每个用户的最近3个月内的最长连续登录天数。 首先要思考，怎么才算连续登录呢？就是一号登录，2号也登录了，这样就连续2天登录，那我们怎么知道2号他有没有登录呢？ 目标是最长，连续。有序 -&gt; 连续 -&gt; 最大 让用户分组排序，计算出序号 123456-- 1.根据用户id分组，对登录时间time做topN排序SELECT UID, date1, row_number() OVER(PARTITION BY UID order by date1) as sort FROM user_login 将连续登录的记录分到一组 分析数据规律，因为日期排序是连续的，我们统计的间隔天数都是一个起始日期，所以，如果登录日期是连续的，那么，排序-间隔天数的差值应该也是一样的。按照date-sort分组，如果连续，那么日期之间的差值也是相等的。 如果连续的话，当sort=3时，日期应该是2017-1-3，而不是2017-1-4。 那么到2017-01-04的时候，相当于连续中断了，需要开启一个新的分组。所以根据差值来分组，然后统计个数，就是区间的连续登录天数了。 123456789101112131415SELECT UID,date_sub(date1,sort) as login_group,min(date1) as start_date1,max(date1) as end_date1,count(1) as continuous_daysFROM ( -- 第一段首先根据用户分组，登陆时间排序，结果按照登陆时间升序排列 SELECT UID, date1, row_number() OVER(PARTITION BY UID order by date1) as sort FROM user_login) aGROUP BY UID,date_sub(date1,sort) 再以uid分组，去取max(continuous_days) 123456789101112131415161718192021222324252627SELECT UID,max(continuous_days) as maxdayFROM (SELECT UID,date_sub(date1,sort) as login_group,min(date1) as start_date1,max(date1) as end_date1,count(1) as continuous_daysFROM ( -- 第一段首先根据用户分组，登陆时间排序，结果按照登陆时间升序排列 SELECT UID, date1, row_number() OVER(PARTITION BY UID order by date1) as sort FROM user_login) aGROUP BY UID,date_sub(date1,sort))bGROUP BY UID]]></content>
      <categories>
        <category>DB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Apache Zookeeper]]></title>
    <url>%2F2020%2F01%2F10%2FApache-Zookeeper%2F</url>
    <content type="text"><![CDATA[本文将全局介绍 Zookeeper 的整体架构。 Zookeeper分布式锁参考：七张图彻底讲清楚Zookeeper分布式锁的实现原理 先整理到本博客中，方便查看，后面再结合 Zookeeper 的代码一起看下。 多客户端获取及释放zk分布式锁的整个流程及背后的原理。zk的锁就是zk上的一个节点。 假设客户端A抢先一步，对zk发起了加分布式锁的请求，这个加锁请求是用到了zk中的一个特殊的概念，叫做“临时顺序节点”。简单来说，就是直接在“my_lock”这个锁节点下，创建一个顺序节点，这个顺序节点有zk内部自行维护的一个节点序号。第一个客户端来搞一个顺序节点，zk内部会给起个名字叫做：xxx-000001，第二个客户端来搞一个顺序节点，zk可能会起个名字叫做：xxx-000002。这个序号是依次递增的，从1开始逐次递增，zk会维护这个顺序。 客户端A发起一个加锁请求，先会在需要加锁的node下搞一下临时顺序节点： 客户端A创建完顺序节点，会查一下”my_lock”这个锁节点下的所有子节点，并且这些子节点是按照序号排序的，拿出一个集合： 接着，客户端A会走一个关键性的判断，判断自己创建的顺序节点是否排在队列的第一个，如果是的话，就可以加锁成功了。 客户端A加完锁了，客户端B过来想要加锁，会做一样的操作，先是在”my_lock”这个锁节点下创建一个临时顺序节点，此时名字会变成类似于： 接着客户端B会走加锁判断逻辑： 查询“my_lock”锁节点下的所有子节点，检查自己创建的顺序节点是不是集合中的第一个，显然不是，所以加锁失败。 加锁失败了以后，客户端B会通过 zk 的 API 对它的顺序节点的上一个顺序节点加一个监听器，监听这个节点是否被删除等变化。zk天然就可以实现对某个节点的监听。 接着客户端A加锁之后，可能处理了一些代码逻辑，然后就会释放锁，也就是把自己在zk里创建的那个顺序节点删除： 此时，就会通知客户端B重新尝试去获取锁，此时集合里就只有客户端B创建的唯一一个顺序节点了。客户端B判断出自己是集合中的第一个顺序节点，直接完成加锁，运行后续的业务代码即可，运行完了再次释放锁： 总结： 大家上来都是创建一个锁节点下的一个接一个的临时顺序节点 如果自己不是第一个节点，就对上一个节点加监听器 只要上一个节点释放锁，自己就排到前面去了，相当于是一个排队机制 zk感知到哪个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者是自动取消排队 应用： 关于实时平台的思考与总结 中的事件管理。 Zookeeper的Leader选举服务器启动时期的Leader选举三个核心选举原则：Zookeeper集群中只有超过半数以上的服务器启动，集群才能正常工作；在集群正常工作之前，myid小的服务器给myid大的服务器投票，直到集群正常工作，选出Leader；选出Leader之后，之前的服务器状态由Looking改变为Following，以后的服务器都是Follower。 下面以一个简单的例子来说明整个选举的过程： 假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。 每个Server都发出投票，然后广播给集群中的其他机器。 假设这些服务器从id1-5，依序启动：因为一共5台服务器，只有超过半数以上，即最少启动3台服务器，集群才能正常工作。（1）服务器1启动，发起一次选举。 服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成； 服务器1状态保持为LOOKING；（2）服务器2启动，再发起一次选举。 服务器1和2分别投自己一票，此时服务器1发现服务器2的id比自己大，更改选票投给服务器2； 此时服务器1票数0票，服务器2票数2票，不够半数以上（3票），选举无法完成； 服务器1，2状态保持LOOKING；（3）服务器3启动，发起一次选举。 与上面过程一样，服务器1和2先投自己一票，然后因为服务器3id最大，两者更改选票投给为服务器3； 此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数（3票），服务器3当选Leader。 服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；（4）服务器4启动，发起一次选举。 此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。 此时服务器4少数服从多数，更改选票信息为服务器3； 服务器4并更改状态为FOLLOWING；（5）服务器5启动，同4一样投票给3，此时服务器3一共5票，服务器5为0票； 服务器5并更改状态为FOLLOWING；（6）选举结果 最终Leader是服务器3，状态为LEADING； 其余服务器是Follower，状态为FOLLOWING。 服务器运行时期的Leader选举（1）变更状态 Leader挂了之后，所有的非Observer服务器都会将自己的状态变更为LOOKING，然后开始进入Leader选举流程。（2）每个Server发出投票 运行期间，服务期间的zxid可能不同，假定Server1的zxid为123，Server3的zxid为122。第一次投票中，Server1和Server3都会投自己， 即分别产生投票（1，123）和（3，122），然后各自将这个投票发给集群中所有机器。（3）接收各个服务器的投票（4）处理投票 优先检查zxid，zxid比较大的服务器作为Leader。如果zxid相同，那么就比较myid，如果myid比较大的服务器作为Leader。 由于Server1的zxid为123，Server3的zxid为122，所以Server1会成为Leader。（5）统计投票（6）改变服务器状态 Zookeeper队列队列的特点：FIFO先进先出 BlockingQueue offer() 向队列尾增加数据poll() 读取队列头的数据 在传统的单进程编程中，我们使用队列来存储一些数据结构，用来在多线程之间共享或传递数据。分布式环境下，同样需要一个类似单进程队列的组件，用来实现跨进程、跨主机、跨网络的数据共享和数据传递，这就是我们的分布式队列。zookeeper 通过持久顺序节点实现分布式队列。 架构图图中左侧代表zookeeper集群，右侧代表生产者和消费者。生产者通过在queue节点下创建顺序节点来存放数据，消费者通过读取顺序节点来消费数据。 offer流程图 poll流程图 代码实现 简单分布式队列 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/** * 简单分布式队列 */public class DistributedSimpleQueue&lt;T&gt; &#123; protected final ZkClient zkClient; // queue节点 protected final String root; // 顺序节点前缀 protected static final String Node_NAME = "n_"; public DistributedSimpleQueue(ZkClient zkClient, String root) &#123; this.zkClient = zkClient; this.root = root; &#125; // 判断队列大小 public int size() &#123; return zkClient.getChildren(root).size(); &#125; // 判断队列是否为空 public boolean isEmpty() &#123; return zkClient.getChildren(root).size() == 0; &#125; // 向队列提供数据 public boolean offer(T element) throws Exception&#123; // 创建顺序节点 String nodeFullPath = root .concat( "/" ).concat( Node_NAME ); try &#123; zkClient.createPersistentSequential(nodeFullPath , element); &#125;catch (ZkNoNodeException e) &#123; zkClient.createPersistent(root); offer(element); &#125; catch (Exception e) &#123; throw ExceptionUtil.convertToRuntimeException(e); &#125; return true; &#125; // 从队列取数据 public T poll() throws Exception &#123; try &#123; // 获取所有顺序节点 List&lt;String&gt; list = zkClient.getChildren(root); if (list.size() == 0) &#123; return null; &#125; // 排序 Collections.sort(list, new Comparator&lt;String&gt;() &#123; public int compare(String lhs, String rhs) &#123; return getNodeNumber(lhs, Node_NAME).compareTo(getNodeNumber(rhs, Node_NAME)); &#125; &#125;); // 循环每个顺序节点名 for ( String nodeName : list )&#123; // 构造出顺序节点的完整路径 String nodeFullPath = root.concat("/").concat(nodeName); try &#123; // 读取顺序节点的内容 T node = (T) zkClient.readData(nodeFullPath); // 删除顺序节点 zkClient.delete(nodeFullPath); return node; &#125; catch (ZkNoNodeException e) &#123; // ignore 由其他客户端把这个顺序节点消费掉了 &#125; &#125; return null; &#125; catch (Exception e) &#123; throw ExceptionUtil.convertToRuntimeException(e); &#125; &#125; private String getNodeNumber(String str, String nodeName) &#123; int index = str.lastIndexOf(nodeName); if (index &gt;= 0) &#123; index += Node_NAME.length(); return index &lt;= str.length() ? str.substring(index) : ""; &#125; return str; &#125;&#125; 阻塞分布式队列 1234567891011121314151617181920212223242526272829303132333435363738394041/** * 阻塞分布式队列 * 扩展自简单分布式队列，在拿不到队列数据时，进行阻塞直到拿到数据 */public class DistributedBlockingQueue&lt;T&gt; extends DistributedSimpleQueue&lt;T&gt;&#123; public DistributedBlockingQueue(ZkClient zkClient, String root) &#123; super(zkClient, root); &#125; @Override public T poll() throws Exception &#123; while (true)&#123; // 结束在latch上的等待后，再来一次 final CountDownLatch latch = new CountDownLatch(1); final IZkChildListener childListener = new IZkChildListener() &#123; public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123; latch.countDown(); // 队列有变化，结束latch上的等待 &#125; &#125;; zkClient.subscribeChildChanges(root, childListener); try&#123; T node = super.poll(); // 获取队列数据 if ( node != null )&#123; return node; &#125; else &#123; latch.await(); // 拿不到队列数据，则在latch上await &#125; &#125; finally &#123; zkClient.unsubscribeChildChanges(root, childListener); &#125; &#125; &#125;&#125; 其他问题Zookeeper 是什么？zookeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 Zookeeper 集群如何保证数据的一致性？zookeeper中 有一个leader 该主机是用于写 写入的更新会自动同步到其他的 follower服务器中 如果leader挂掉后 会重新选举一个leader 所以如果是集群环境 最少2n+1台机器组成。 Zookeeper 如何存储数据？zookeeper使用类似于window注册表的树状结构存储数据 只有一个根节点 / 子节点可以存储数据 Zookeeper 的节点类型有哪些？永久节点：调用写入 自动持久化到硬盘临时节点：在zk会话存在期间才存在的znode，如果会话停止了，那么znode也被删除了。临时性的znode不允许有子节点序列节点：创建znode的时候，可以要求zk为路径后面附上一个单调递增的计数器，这个计数器的格式这样的 %010d，这个计数器是由父znode维护的一个signed int，最大值2147483647 Zookeeper 应用场景作为分布式配置中心用于微服务注册中心注册微服务的信息实现分布式锁，分布式主键 秒杀 Zookeeper 通知机制client端会对某个znode建立一个watcher事件，当该znode发生变化时，这些client会收到zk的通知，然后client可以根据znode变化来做出业务上的改变等。 Zookeeper 做了什么 命名服务命名服务是指通过指定的名字来获取资源或者服务的地址，利用zk创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。 配置管理程序分布式的部署在不同的机器上，将程序的配置信息放在zk的znode下，当有配置发生改变时，也就是znode发生变化时，可以通过改变zk中某个目录节点的内容，利用watcher通知给各个客户端，从而更改配置。 集群管理所谓集群管理主要在乎于两点：是否有机器退出和加入、选举master。对于第一点，所有机器约定在父目录下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了，对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。 分布式锁有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的distribute_lock 节点就释放出锁。对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。具体过程如下：七张图彻底讲清楚Zookeeper分布式锁的实现原理 队列管理两种类型的队列：1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。2、队列按照 FIFO 方式进行入队和出队操作。第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建PERSISTENT_SEQUENTIAL节点，创建成功时Watcher通知等待的队列，队列删除序列号最小的节点用以消费。 Zookeeper 工作原理 Zookeeper 的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 Zookeeper 是如何保证事务的顺序一致性 Zookeeper采用了递增的事务Id来标识，所有的proposal（提议）都在被提出的时候加上了zxid，zxid实际上是一个64位的数字，高32位是epoch（时期; 纪元; 世; 新时代）用来标识leader是否发生改变，如果有新的leader产生出来，epoch会自增，低32位用来递增计数。当新产生proposal的时候，会依据数据库的两阶段过程，首先会向其他的server发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。 Zookeeper 下Server工作状态 每个Server在工作过程中有三种状态： LOOKING：当前Server不知道leader是谁，正在搜寻 LEADING：当前Server即为选举出来的leader FOLLOWING：leader已经选举出来，当前Server与之同步 Zookeeper 是如何选取主leader的 当leader崩溃或者leader失去大多数的follower，这时zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。 zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。 机器中为什么会有leader？ 在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行leader选举。 Zookeeper负载均衡和nginx负载均衡区别 zk的负载均衡是可以调控，nginx只是能调权重，其他需要可控的都需要自己写插件；但是nginx的吞吐量比zk大很多，应该说按业务选择用哪种方式。 Zab协议参考：Zab协议 为了保证写操作的一致性与可用性，Zookeeper专门设计了一种名为原子广播（Zab）的支持崩溃恢复的一致性协议。基于该协议，Zookeeper实现了一种主从模式的系统架构来保持集群中各个副本之间的数据一致性。 根据Zab协议，所有的写操作都必须通过 Leader 完成，Leader 写入本地日志后再复制到所有的 Follower 节点。一旦 Leader 节点无法工作，Zab 协议能够自动从 Follower 节点中重新选出一个合适的替代者，即新的 Leader，该过程即为 Leader 选举。 写 Leader 写 Follower/Observer 读操作 支持的Leader选举算法到 3.4.10 版本为止，可选项有：0 基于UDP的LeaderElection （弃用）1 基于UDP的FastLeaderElection （弃用）2 基于UDP和认证的FastLeaderElection （弃用）3 基于TCP的FastLeaderElection （默认）]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多线程]]></title>
    <url>%2F2020%2F01%2F10%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[XXX 参考：多线程讲的比较好的博客之一 synchronized的实现原理 synchronized 的作用synchronized可以保证在同一个时刻，只有一个线程可以执行某个方法或者某个代码块，其原理是通过当前线程持有当前对象锁，从而拥有访问权限，而其他没有持有当前对象锁的线程无法拥有访问权限，也就保证了线程安全。synchronized也可以保证线程的可见性。synchronized属于隐式锁，锁的持有与释放都是隐式的，我们无需干预。 synchronized最主要的三种应用方式：修饰实例方法：作用于当前实例加锁，进入同步代码前要获得当前实例的锁。修饰静态方法：作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁修饰代码块，指定加锁对象。对给定对象加锁，进入同步代码库前要获得给定对象的锁。 Java 虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现， 无论是显式同步(有明确的 monitorenter 和 monitorexit 指令,即同步代码块)还是隐式同步都是如此。在 Java 语言中，同步用的最多的地方可能是被 synchronized 修饰的同步方法。同步方法 并不是由 monitorenter 和 monitorexit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的 ACC_SYNCHRONIZED 标志来隐式实现的，关于这点，稍后详细分析。下面先来了解一个概念Java对象头，这对深入理解synchronized实现原理非常关键。 在JVM中，对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充。如下： 实例变量：存放类的属性数据信息，包括父类的属性信息，如果是数组的实例部分还包括数组的长度，这部分内存按4字节对齐。 填充数据：由于虚拟机要求对象起始地址必须是8字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐，这点了解即可。 而对于顶部，则是Java头对象，它实现synchronized的锁对象的基础，这点我们重点分析它，一般而言，synchronized使用的锁对象是存储在Java对象头里的，jvm中采用2个字来存储对象头(如果对象是数组则会分配3个字，多出来的1个字记录的是数组长度)，其主要结构是由Mark Word 和 Class Metadata Address 组成，其结构说明如下表： 虚拟机位数 头对象结构 说明 32/64bit Mark Word 存储对象的hashCode、锁信息或分代年龄或GC标志等信息 32/64bit Class Metadata Address 类型指针指向对象的类元数据，JVM通过这个指针确定该对象是哪个类的实例 Mark Word默认存储结构：| 锁状态 | 25bit | 4bit | 1bit是否是偏向锁 | 2bit 锁标志位 || —- | —- | —- | —- | —- || 无锁状态 | 对象HashCode| 对象分代年龄 | 0 | 01 | 由于对象头的信息是与对象自身定义的数据没有关系的额外存储成本，因此考虑到JVM的空间效率，Mark Word 被设计成为一个非固定的数据结构，以便存储更多有效的数据，它会根据对象本身的状态复用自己的存储空间，如32位JVM下，除了上述列出的Mark Word默认存储结构外，还有如下可能变化的结构： Java虚拟机对synchronized的优化锁的状态总共有四种，无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级，关于重量级锁，前面我们已详细分析过，下面我们将介绍偏向锁和轻量级锁以及JVM的其他优化手段，这里并不打算深入到每个锁的实现和转换过程更多地是阐述Java虚拟机所提供的每个锁的核心优化思想，毕竟涉及到具体过程比较繁琐，如需了解详细过程可以查阅《深入理解Java虚拟机原理》。 偏向锁偏向锁是Java 6之后加入的新锁，它是一种针对加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁(会涉及到一些CAS操作,耗时)的代价而引入偏向锁。偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。下面我们接着了解轻量级锁。 轻量级锁倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)，此时Mark Word 的结构也变为轻量级锁的结构。轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步周期内都不存在竞争”，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，就会导致轻量级锁膨胀为重量级锁。 自旋锁轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，因此自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环(这也是称为自旋的原因)，一般不会太久，可能是50个循环或100循环，在经过若干次循环后，如果得到锁，就顺利进入临界区。如果还不能获得锁，那就会将线程在操作系统层面挂起，这就是自旋锁的优化方式，这种方式确实也是可以提升效率的。最后没办法也就只能升级为重量级锁了。 锁消除消除锁是虚拟机另外一种锁的优化，这种优化更彻底，Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间，如下StringBuffer的append是一个同步方法，但是在add方法中的StringBuffer属于一个局部变量，并且不会被其他线程所使用，因此StringBuffer不可能存在共享资源竞争的情景，JVM会自动将其锁消除。 1234567891011121314151617public class StringBufferRemoveSync &#123; public void add(String str1, String str2) &#123; //StringBuffer是线程安全,由于sb只会在append方法中使用,不可能被其他线程引用 //因此sb属于不可能共享的资源,JVM会自动消除内部的锁 StringBuffer sb = new StringBuffer(); sb.append(str1).append(str2); &#125; public static void main(String[] args) &#123; StringBufferRemoveSync rmsync = new StringBufferRemoveSync(); for (int i = 0; i &lt; 10000000; i++) &#123; rmsync.add("abc", "123"); &#125; &#125;&#125; AQS原理抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的 ReentrantLock/Semaphore/CountDownLatch。 它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。这里volatile是核心关键词，具体volatile的语义，在此不述。state的访问方式有三种: getState()setState()compareAndSetState() AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法： isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 acquire(int) // 未完待续 问题SemaphoreCallable与Future的组合用法ReentrantLock与Condition如果是两个线程，通常有一个lock，分别控制着两个线程的Condition。以便争用同一个锁的多线程之间可以通过Condition的状态来获取通知。condition.signal();condition.await(); CountDownLatchCyclicBarrierTreeMap红黑树，二叉排序树，平衡二叉树 1、每个结点要么是红的要么是黑的。2、根结点是黑的。3、每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。4、如果一个结点是红的，那么它的两个儿子都是黑的。5、对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。 BlockingQueueArrayBlockingQueueDelayQueueLinkedBlockingDequeLinkedBlockingQueuePriorityBlockingQueueSynchronousQueue 互斥锁同一时刻只能有一个线程得到互斥锁。如果线程无法获取锁，立刻放弃CPU时间片并阻塞挂起。 自旋锁如果线程无法获取自旋锁，不会立刻放弃CPU时间片，而是一直申请CPU时间片轮询自旋锁，通过死循环不断检测是否达到释放锁的条件，如果达到就释放锁，否则就继续循环。 条件锁需要一个条件才能唤醒线程，比如多线程共用一个变量，一个线程在对变量修改时达到另一个已经挂起的线程的条件，当前线程就会把另一个线程唤醒。 读写锁把读锁和写锁分离。写锁被占用时，所有申请读锁和写锁的线程都会被阻塞。读锁被占用时，申请写锁的线程被阻塞，其他申请读锁的线程不会。 Synchronized和Lock的区别synchronized关键字可以将对象或者方法标记为同步，以实现对对象和方法的互斥访问，可以用synchronized(对象) { … }定义同步代码块，或者在声明方法时将synchronized作为方法的修饰符。Lock 能完成synchronized所实现的所有功能。主要不同点：Lock有比synchronized更精确的线程语义和更好的性能，而且不强制性的要求一定要获得锁。synchronized会自动释放锁，而Lock一定要求程序员手工释放，并且最好在finally 块中释放（这是释放外部资源的最好的地方）。 当一个线程进入一个对象的synchronized方法A之后，其它线程是否可进入此对象的synchronized方法B？不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的synchronized修饰符要求执行方法时要获得对象的锁，如果已经进入A方法说明对象锁已经被取走，那么试图进入B方法的线程就只能在等锁池（注意不是等待池哦）中等待对象的锁。 Synchronized和ReentrantLock的区别synchronized是和if、else、for、while一样的关键字，ReentrantLock是类，这是二者的本质区别。既然ReentrantLock是类，那么它就提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量，ReentrantLock比synchronized的扩展性体现在几点上： ReentrantLock可以对获取锁的等待时间进行设置，这样就避免了死锁ReentrantLock可以获取各种锁的信息ReentrantLock可以灵活地实现多路通知另外，二者的锁机制其实也是不一样的:ReentrantLock底层调用的是Unsafe的park方法加锁，synchronized操作的应该是对象头中mark word. 线程池 newSingleThreadExecutor：创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newFixedThreadPool：创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newCachedThreadPool：创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 newScheduledThreadPool：创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 newSingleThreadExecutor：创建一个单线程的线程池。此线程池支持定时以及周期性执行任务的需求。 线程基本状态及状态之间的关系 其中Running表示运行状态；Runnable表示就绪状态（万事俱备，只欠CPU）；Blocked表示阻塞状态；阻塞状态又有多种情况，可能是因为调用wait()方法进入等待池，也可能是执行同步方法或同步代码块进入等锁池，或者是调用了sleep()方法或join()方法等待休眠或其他线程结束，或是因为发生了I/O中断。 产生死锁的条件 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 线程同步以及线程调度相关的方法 wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁； sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理InterruptedException异常； notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由JVM确定唤醒哪个线程，而且与优先级无关； notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； sleep()和yeild()方法的区别 sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会； 线程执行sleep()方法后转入阻塞（blocked）状态，而执行yield()方法后转入就绪（ready）状态； sleep()方法声明抛出InterruptedException，而yield()方法没有声明任何异常； sleep()方法比yield()方法（跟操作系统CPU调度相关）具有更好的可移植性。 ThreadLocal 原理分析及其内存泄漏的原因 原理分析 ThreadLocal为解决多线程程序的并发问题提供了一种新的思路。ThreadLocal，顾名思义是线程的一个本地化对象，当工作于多线程中的对象使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程分配一个独立的变量副本，所以每一个线程都可以独立的改变自己的副本，而不影响其他线程所对应的副本。从线程的角度看，这个变量就像是线程的本地变量。 ThreadLocal类非常简单好用，只有四个方法，能用上的也就是下面三个方法： void set(T value)：设置当前线程的线程局部变量的值。T get()：获得当前线程所对应的线程局部变量的值。void remove()：删除当前线程中线程局部变量的值。ThreadLocal是如何做到为每一个线程维护一份独立的变量副本的呢？在ThreadLocal类中有一个Map，键为线程对象，值是其线程对应的变量的副本，自己要模拟实现一个ThreadLocal类其实并不困难，代码如下所示： 123456789101112131415import java.util.Collections;import java.util.HashMap;import java.util.Map;public class MyThreadLocal&lt;T&gt; &#123; private Map&lt;Thread, T&gt; map = Collections.synchronizedMap(new HashMap&lt;Thread, T&gt;()); public void set(T newValue) &#123; map.put(Thread.currentThread(), newValue); &#125; public T get() &#123; return map.get(Thread.currentThread()); &#125; public void remove() &#123; map.remove(Thread.currentThread()); &#125;&#125; ThreadLocal的实现是这样的：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 是 ThreadLocal实例本身，value 是真正需要存储的 Object。 也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，弱引用的对象在 GC 时会被回收。 ThreadLocal泄漏的原因以及避免方案 原因ThreadLocalMap使用ThreadLocal的弱引用作为key，如果没有外部强引用来引用它，那么系统GC的时候，这个ThreadLocal势必会被回收，这样 ThreadLocalMap 中就会出现key为null的Entry，就没办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。 其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。 但是这些被动的预防措施并不能保证不会内存泄漏： 使用static的ThreadLocal，延长了ThreadLocal的生命周期，可能导致的内存泄漏。分配使用了ThreadLocal又不再调用get(),set(),remove()方法，那么就会导致内存泄漏。 为什么使用弱引用key 使用强引用：引用的ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。key 使用弱引用：引用的ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的弱引用，即使没有手动删除，ThreadLocal也会被回收。value在下一次ThreadLocalMap调用set,get，remove的时候会被清除。比较两种情况，我们可以发现：由于ThreadLocalMap的生命周期跟Thread一样长，如果都没有手动删除对应key，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用ThreadLocal不会内存泄漏，对应的value在下一次ThreadLocalMap调用set,get,remove的时候会被清除。 因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 - ThreadLocal最佳实践 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。 在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。 对于已经不再被使用且已被回收的 ThreadLocal 对象，它在每个线程内对应的实例由于被线程的 ThreadLocalMap 的 Entry 强引用，无法被回收，可能会造成内存泄漏。 针对该问题，ThreadLocalMap 的 set 方法中，通过 replaceStaleEntry 方法将所有键为 null 的 Entry 的值设置为 null，从而使得该值可被回收。另外，会在 rehash 方法中通过 expungeStaleEntry 方法将键和值为 null 的 Entry 设置为 null 从而使得该 Entry 可被回收。通过这种方式，ThreadLocal 可防止内存泄漏。123456789101112131415161718192021private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 什么是CAS CAS，全称为Compare and Swap，即比较-替换。假设有三个操作数：内存值V、旧的预期值A、要修改的值B，当且仅当预期值A和内存值V相同时，才会将内存值修改为B并返回true，否则什么都不做并返回false。当然CAS一定要volatile变量配合，这样才能保证每次拿到的变量是主内存中最新的那个值，否则旧的预期值A对某条线程来说，永远是一个不会变的值A，只要某次CAS操作失败，永远都不可能成功。 AtomicInteger AtomicStampedRefrenceInteger 解决ABA问题 乐观锁和悲观锁 乐观锁：乐观锁认为竞争不总是会发生，因此它不需要持有锁，将比较-替换这两个动作作为一个原子操作尝试去修改内存中的变量，如果失败则表示发生冲突，那么就应该有相应的重试逻辑。 悲观锁：悲观锁认为竞争总是会发生，因此每次对某资源进行操作时，都会持有一个独占的锁，就像synchronized，不管三七二十一，直接上了锁就操作资源了。 ConcurrentHashMap的工作原理 ConcurrentHashMap在jdk 1.6和jdk 1.8实现原理是不同的. jdk 1.6: ConcurrentHashMap是线程安全的，但是与Hashtablea相比，实现线程安全的方式不同。Hashtable是通过对hash表结构进行锁定，是阻塞式的，当一个线程占有这个锁时，其他线程必须阻塞等待其释放锁。ConcurrentHashMap是采用分离锁的方式，它并没有对整个hash表进行锁定，而是局部锁定，也就是说当一个线程占有这个局部锁时，不影响其他线程对hash表其他地方的访问。 具体实现:ConcurrentHashMap内部有一个Segment jdk 1.8 在jdk 8中，ConcurrentHashMap不再使用Segment分离锁，而是采用一种乐观锁CAS算法来实现同步问题，但其底层还是“数组+链表-&gt;红黑树”的实现。 CyclicBarrier和CountDownLatch区别这两个类非常类似，都在java.util.concurrent下，都可以用来表示代码运行到某个点上，二者的区别在于： CyclicBarrier的某个线程运行到某个点上之后，该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行；CountDownLatch则不是，某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行 CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务 CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了 java中的++操作线程安全吗？不是线程安全的操作。它涉及到多个指令，如读取变量值，增加，然后存储回内存，这个过程可能会出现多个线程交差 有三个线程T1，T2，T3，怎么确保它们按顺序执行？在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。 你有哪些多线程开发良好的实践？ 给线程命名 最小化同步范围 优先使用volatile 尽可能使用更高层次的并发工具而非wait和notify()来实现线程通信,如BlockingQueue,Semeaphore 优先使用并发容器而非同步容器. 考虑使用线程池 线程池的处理流程 首先第一步会判断核心线程数有没有达到上限，如果没有则创建线程(会获取全局锁)，满了则会将任务丢进阻塞队列。 如果队列也满了则需要判断最大线程数是否达到上限，如果没有则创建线程(获取全局锁)，如果最大线程数也满了则会根据饱和策略处理。 常用的饱和策略有:直接丢弃任务。调用者线程处理。丢弃队列中的最近任务，执行当前任务。所以当线程池完成预热之后都是将任务放入队列，接着由工作线程一个个从队列里取出执行。 合理配置线程池线程池并不是配置越大越好，而是要根据任务的熟悉来进行划分： 如果是 CPU 密集型任务应当分配较少的线程，比如 CPU 个数相当的大小。如果是 IO 密集型任务，由于线程并不是一直在运行，所以可以尽可能的多配置线程，比如 CPU 个数 * 2 。当是一个混合型任务，可以将其拆分为 CPU 密集型任务以及 IO 密集型任务，这样来分别配置。 Synchronized实现原理众所周知 Synchronize 关键字是解决并发问题常用解决方案，有以下三种使用方式: 同步普通方法，锁的是当前对象。同步静态方法，锁的是当前 Class 对象。同步块，锁的是 {} 中的对象。实现原理： JVM 是通过进入、退出对象监视器( Monitor )来实现对方法、同步块的同步的。具体实现是在编译之后在同步方法调用前加入一个 monitor.enter 指令，在退出方法和异常处插入 monitor.exit 的指令。 其本质就是对一个对象监视器( Monitor )进行获取，而这个获取过程具有排他性从而达到了同一时刻只能一个线程访问的目的。 而对于没有获取到锁的线程将会阻塞到方法入口处，直到获取锁的线程 monitor.exit 之后才能尝试继续获取锁。 流程图如下: 产生死锁的条件、现象、解决办法等1、三个人 三根筷子：每个人需要拿到身边的两根筷子才能开始吃饭 2、银行转账问题：线程 A 从 X 账户向 Y 账户转账，线程 B 从账户 Y 向账户 X 转账，那么就会发生死锁。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[End-to-End Exactly-Once Processing in Apache Flink]]></title>
    <url>%2F2019%2F12%2F16%2FEnd-to-End-Exactly-Once-Processing-in-Apache-Flink%2F</url>
    <content type="text"><![CDATA[2017年发版的Flink 1.4.0，引进了一个里程碑式的新特性：TwoPhaseCommitSinkFunction ，抽象了两阶段提交协议的通用逻辑，相关联的jira单，并在Kafka Producer的connector中实现了它，支持了对外部Kafka sink的exactly-once语义。在此基础上，结合 Flink、sources、sinks 和 Kafka 0.11以上的版本，可以实现端到端一致性语义的应用，需要用户实现少数方法来实现一致性语义。在这篇博客中，介绍以下几个方面： 描述 Flink checkpoints 在保证一致性语义中的角色作用 展示Flink如何通过两阶段提交协议与sources和sinks交互，以提供端到端的一致性语义 通过一个简单的示例，通过使用 TwoPhaseCommitSinkFunction ，实现落地到文件目的端的一致性语义 Flink中的exactly-once语义Exactly-once语义简称EOS，指的是每条输入消息只会影响最终结果一次，注意这里是影响一次，而非处理一次。保证即使在机器或应用发生故障的情况下，也不会有重复数据或者漏处理的数据。Flink应用内部的 exactly-once 是通过 Flink 的 checkpointing算法实现的，Flink 做 checkpoint 的一致性快照中的内容有： 应用的当前state 输入流的位置 Flink以可配置的间隔生成 checkpoints ，并持久化到存储系统，如 S3、HDFS 等。这个持久化过程是异步的，也就是说，Flink 在 checkpointing 的过程中会继续处理数据。当机器或应用发生故障后重启，Flink 会从最近一个成功完成的 checkpoint 恢复，恢复应用的state，回滚到输入流的消费位置，就像故障从来没发生过一样。 在 Flink 1.4.0 之前，exactly-once 语义仅限于 Flink 应用内部，不涉及到外部系统。为了实现端到端的 exactly-once 语义，外部系统必须与Flink checkpoint 协调处理提供一种能提交与回滚的写入方式。常见的协调提交与回滚操作的途径就是两阶段提交协议。 Flink中的端到端的exactly-once应用Flink一直宣称自己支持EOS，实际上主要是针对Flink应用内部来说的，对于外部系统（端到端）则有比较强的限制： 外部系统写入支持幂等性 外部系统支持以事务的方式写入 Kafka的幂等性和事务Kafka 在0.11版本之前只能保证 at-least-once 和 at-most-once 语义，从0.11版本开始，引入了幂等发送和事务，从而实现 exactly-once 语义。 幂等性未引入幂等性之前，Kafka正常发送和重试发送消息的流程图如下： 未引入幂等性之前，Kafka重试发送消息的流程图如下，可能会造成数据重复： 为了实现Producer的幂等语义，Kafka引入了Producer ID（即PID）和Sequence Number。每个新的Producer在初始化的时候会被分配一个唯一的PID，该PID对用户完全透明而不会暴露给用户。Producer发送每条消息&lt;Topic, Partition&gt;对于Sequence Number会从0开始单调递增，broker端会为每个&lt;PID, Topic, Partition&gt;维护一个序号，每次commit一条消息此序号加一，对于接收的每条消息，如果其序号比Broker维护的序号（即最后一次Commit的消息的序号）大1以上，则Broker会接受它，否则将其丢弃： 序号比Broker维护的序号大1以上，说明存在乱序。序号比Broker维护的序号小，说明此消息以及被保存，为重复数据。 有了幂等性，Kafka正常发送和重试发送消息流程图如下： 幂等性机制仅解决了单分区上的数据重复和乱序问题，对于跨session和所有分区的重复和乱序问题不能得到解决。于是需要引入事务。 事务// 待补充 端到端的exactly-once实现过程Flink 支持的端到端的 exactly-once 不仅限于 Kafka，可以应用到其他的source/sink，只要它们提供了必需的协调机制。以下示例中： 从Kafka读取数据的opertor(KafkaConsumer) 窗口聚合操作 写入到Kafka目的源的operator(KafkaProducer)，两个checkpoints之间的待写入数据集合，必须是事务操作，当发生故障时要能回滚。 预提交阶段一次 checkpoint 的开始代表两阶段提交协议中的预提交阶段。Flink JobManager 向数据流中注入一个 checkpoint barrier ，分隔两次 checkpoints 之间的数据集合。barrier 从一个 operator 流向下一个 operator，每个 operator 接收到 barrier 之后就会触发 state backend 操作，做 state 快照。 Data Source存储它消费到Kafka位置，完成之后向下一个 operator 传递 barrier 。内部状态是指所有能通过 Flink state 存储和管理的状态。例如，窗口求和的状态。处理过程中如果只有内部状态，那么在与提交阶段就只需要持久化state，不需要额外的操作，因为Flink 会接管state的持久化操作，state 成功写入就进行 commit 操作，发生故障时会丢弃这些 state。 然而，如果处理过程中涉及到外部状态，外部状态来自于写入外部系统（如Kafka）时。为了保证 exactly-once ，外部系统必须支持整合二阶段提交协议的事务操作。在预提交阶段，data sink除了存储预提交的外部状态，还要进行事务的预提交操作。 当checkpoint barrier传递到所有的operators之后，代表预提交阶段完成。所有触发的state快照被认为是checkpoint的一部分，checkpoint 是整个应用的state快照，包括预提交的外部状态。发生故障时，就可以从上次的完整快照恢复。下一步就进入提交阶段了，JobManager通知所有的operator checkpoint已成功完成，处理每个 operator 的checkpoint完成的回调。data source 和 window operator 没有外部状态，因此在提交阶段不需要做任何操作，data sink有外部状态，外部系统的事务操作需要进行真正的提交操作了。 让我们总结一下上面的过程： 一旦所有的 operators 完成了预提交，就进行提交操作 如果其中一个operator的预提交操作失败，其他的预提交操作就会终止，并回滚到上一次成功执行的checkpoint中记录的状态 预提交成功之后，提交操作必须保证成功，所有的 operators 和 外部系统都要参与其中。如果提交失败（例如由于偶发性的网络问题），整个Flink应用就会失败，应用会根据用户设置的重启策略进行重启，重试提交操作。这个过程是很关键的，因为如果提交操作最终没有成功，将会导致数据丢失。 因此，我们可以确保所有的operators的对最终的checkpoint 结果都是一致的：数据不是被提交了，就是提交被终止并回滚了。 TwoPhaseCommitSinkFunction使用idea自带的Diagrams插件，查看TwoPhaseCommitSinkFunction类的继承关系： 几个重要的方法initializeState(FunctionSnapshotContext context)初始化状态：直接开启一个新事务；从已有的状态中恢复：读取 state 中记录的待提交事务 pendingCommitTransactions 列表，执行状态恢复和事务commit操作，最后清空 pendingCommitTransactions 列表；读取 state 中记录的 pending 状态的事务列表，执行事务abort操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243@Overridepublic void initializeState(FunctionInitializationContext context) throws Exception &#123; state = context.getOperatorStateStore().getListState(stateDescriptor); boolean recoveredUserContext = false; if (context.isRestored()) &#123; // 开始读取snapshot中的state，进行状态恢复操作 LOG.info("&#123;&#125; - restoring state", name()); for (State&lt;TXN, CONTEXT&gt; operatorState : state.get()) &#123; userContext = operatorState.getContext(); // 获取从state中读取的待提交的事务列表 List&lt;TransactionHolder&lt;TXN&gt;&gt; recoveredTransactions = operatorState.getPendingCommitTransactions(); for (TransactionHolder&lt;TXN&gt; recoveredTransaction : recoveredTransactions) &#123; // If this fails to succeed eventually, there is actually data loss // 恢复状态的同时会执行提交操作 recoverAndCommitInternal(recoveredTransaction); LOG.info("&#123;&#125; committed recovered transaction &#123;&#125;", name(), recoveredTransaction); &#125; recoverAndAbort(operatorState.getPendingTransaction().handle); LOG.info("&#123;&#125; aborted recovered transaction &#123;&#125;", name(), operatorState.getPendingTransaction()); if (userContext.isPresent()) &#123; finishRecoveringContext(); recoveredUserContext = true; &#125; &#125; &#125; // if in restore we didn't get any userContext or we are initializing from scratch if (!recoveredUserContext) &#123; LOG.info("&#123;&#125; - no state to restore", name()); userContext = initializeUserContext(); &#125; // 清除 pendingCommitTransactions 集合 this.pendingCommitTransactions.clear(); // 开启事务 currentTransactionHolder = beginTransactionInternal(); LOG.debug("&#123;&#125; - started new transaction '&#123;&#125;'", name(), currentTransactionHolder);&#125; snapshotState(FunctionSnapshotContext context)两阶段提交 Function 做 state 快照，从 context 中取出当前的 checkpointId；接着执行预提交操作，并把待提交的事务加到 pendingCommitTransactions 列表中；开启一个新的事务；存储新的state。 12345678910111213141516171819202122232425262728@Overridepublic void snapshotState(FunctionSnapshotContext context) throws Exception &#123; // this is like the pre-commit of a 2-phase-commit transaction // we are ready to commit and remember the transaction checkState(currentTransactionHolder != null, "bug: no transaction object when performing state snapshot"); // 从上下文中取得当前的checkpointId，是从0开始递增 long checkpointId = context.getCheckpointId(); LOG.debug("&#123;&#125; - checkpoint &#123;&#125; triggered, flushing transaction '&#123;&#125;'", name(), context.getCheckpointId(), currentTransactionHolder); // 执行预提交操作，传入事务操作类，进行事务的预提交操作 preCommit(currentTransactionHolder.handle); // 执行过预提交操作的事务放入pendingCommitTransactions集合， pendingCommitTransactions.put(checkpointId, currentTransactionHolder); LOG.debug("&#123;&#125; - stored pending transactions &#123;&#125;", name(), pendingCommitTransactions); currentTransactionHolder = beginTransactionInternal(); LOG.debug("&#123;&#125; - started new transaction '&#123;&#125;'", name(), currentTransactionHolder); // 清除上一次的state state.clear(); // 存储新的状态 state.add(new State&lt;&gt;( this.currentTransactionHolder, new ArrayList&lt;&gt;(pendingCommitTransactions.values()), userContext));&#125; notifyCheckpointComplete(long checkpointId)两阶段提交Function执行notify操作，遍历待提交事务列表，执行commit操作；执行完 commit 操作的事务，从 pendingCommitTransactions 中移除。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172@Overridepublic final void notifyCheckpointComplete(long checkpointId) throws Exception &#123; // the following scenarios are possible here // // (1) there is exactly one transaction from the latest checkpoint that // was triggered and completed. That should be the common case. // Simply commit that transaction in that case. // // (2) there are multiple pending transactions because one previous // checkpoint was skipped. That is a rare case, but can happen // for example when: // // - the master cannot persist the metadata of the last // checkpoint (temporary outage in the storage system) but // could persist a successive checkpoint (the one notified here) // // - other tasks could not persist their status during // the previous checkpoint, but did not trigger a failure because they // could hold onto their state and could successfully persist it in // a successive checkpoint (the one notified here) // // In both cases, the prior checkpoint never reach a committed state, but // this checkpoint is always expected to subsume the prior one and cover all // changes since the last successful one. As a consequence, we need to commit // all pending transactions. // // (3) Multiple transactions are pending, but the checkpoint complete notification // relates not to the latest. That is possible, because notification messages // can be delayed (in an extreme case till arrive after a succeeding checkpoint // was triggered) and because there can be concurrent overlapping checkpoints // (a new one is started before the previous fully finished). // // ==&gt; There should never be a case where we have no pending transaction here // Iterator&lt;Map.Entry&lt;Long, TransactionHolder&lt;TXN&gt;&gt;&gt; pendingTransactionIterator = pendingCommitTransactions.entrySet().iterator(); checkState(pendingTransactionIterator.hasNext(), "checkpoint completed, but no transaction pending"); Throwable firstError = null; while (pendingTransactionIterator.hasNext()) &#123; Map.Entry&lt;Long, TransactionHolder&lt;TXN&gt;&gt; entry = pendingTransactionIterator.next(); Long pendingTransactionCheckpointId = entry.getKey(); TransactionHolder&lt;TXN&gt; pendingTransaction = entry.getValue(); // 如果待提交的事务的 checkpointId 大于当前通知的已完成的checkpointId，则循环继续下一个 if (pendingTransactionCheckpointId &gt; checkpointId) &#123; continue; &#125; LOG.info("&#123;&#125; - checkpoint &#123;&#125; complete, committing transaction &#123;&#125; from checkpoint &#123;&#125;", name(), checkpointId, pendingTransaction, pendingTransactionCheckpointId); // 如果事务执行时间相对较长的，可以打印出警告日志 logWarningIfTimeoutAlmostReached(pendingTransaction); try &#123; // 执行commit操作 commit(pendingTransaction.handle); &#125; catch (Throwable t) &#123; if (firstError == null) &#123; firstError = t; &#125; &#125; LOG.debug("&#123;&#125; - committed checkpoint transaction &#123;&#125;", name(), pendingTransaction); pendingTransactionIterator.remove(); &#125; if (firstError != null) &#123; throw new FlinkRuntimeException("Committing one of transactions failed, logging first encountered failure", firstError); &#125;&#125; 需要子类实现的方法 invoke：接收到数据，处理数据 beginTransaction：开启事物，做一些准备工作 preCommit：预提交阶段，进行预提交操作 commit：提交阶段，进行真正的提交操作 abort：回滚操作，撤回预提交阶段的操作 实现file sink exactly-once的示例实现逻辑要实现上面的过程会有点复杂，因此Flink抽象出一个TwoPhaseCommitSinkFunction类。假设写入的目的端是磁盘文件系统 file sink，我们只需要实现以下4个方法就可以实现file sink 的 exactly-once 语义： invoke：接收到数据，将数据写入临时文件 beginTransaction：开启事物，在目标文件系统的临时目录创建临时文件，创建写文件的writer，后续可以向临时文件中写入数据 preCommit：预提交阶段，刷写并关闭文件，永远不会再向这个文件写入了。并为下一次checkpoint启用新的事务 commit：提交阶段，将临时文件从临时目录移动到正式目录，显然这会让目的端数据延时可见 abort：删除临时文件 当Flink任务发生故障时，所有的 operators 会恢复成最近一次成功生成的 checkpoint 中的状态。对于预提交的事务，我们必须存储足够的状态信息，以便在重启的时候决定提交还是回滚事务。在本例子中，存储的应该是临时文件和真正的存储目录。TwoPhaseCommitSinkFunction 考虑到了这个场景，从checkpoint中恢复状态时总是进行提交操作。我们应该确保写入目的端的提交操作是幂等的，通常情况下，这也不是一个必须要考虑的问题。在本例子中，临时文件不在临时目录中，就是已经被移动到真正的存储目录中了。 可以查看 Flink源码中自带的两阶段提交测试示例自定义实现两阶段提交的function ContentDumpSinkFunction： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 实现两阶段提交的function */private class ContentDumpSinkFunction extends TwoPhaseCommitSinkFunction&lt;String, ContentTransaction, Void&gt; &#123; public ContentDumpSinkFunction() &#123; super( new KryoSerializer&lt;&gt;(ContentTransaction.class, new ExecutionConfig()), VoidSerializer.INSTANCE, clock); &#125; /** * 接收到事件，invoke处理事件 */ @Override protected void invoke(ContentTransaction transaction, String value, Context context) throws Exception &#123; transaction.tmpContentWriter.write(value); &#125; /** * 开启事务 */ @Override protected ContentTransaction beginTransaction() throws Exception &#123; // 开启事务，创建写文件的writer，文件名为随机生成的UUID return new ContentTransaction(tmpDirectory.createWriter(UUID.randomUUID().toString())); &#125; /** * 预提交阶段 */ @Override protected void preCommit(ContentTransaction transaction) throws Exception &#123; // 刷写到临时文件并关闭文件，因为再也不会向这个临时文件中写入数据 transaction.tmpContentWriter.flush(); transaction.tmpContentWriter.close(); &#125; /** * 提交阶段 */ @Override protected void commit(ContentTransaction transaction) &#123; if (throwException.get()) &#123; throw new RuntimeException("Expected exception"); &#125; // 转移tmp目录下的文件到target目录下 ContentDump.move( transaction.tmpContentWriter.getName(), tmpDirectory, targetDirectory); &#125; /** * abort，进行回滚操作，把预提交阶段刷写的临时文件删除 */ @Override protected void abort(ContentTransaction transaction) &#123; transaction.tmpContentWriter.close(); tmpDirectory.delete(transaction.tmpContentWriter.getName()); &#125;&#125; 定义事务操作类 ContentTransaction： 123456789101112131415/** * 事务操作类 */private static class ContentTransaction &#123; private ContentDump.ContentWriter tmpContentWriter; public ContentTransaction(ContentDump.ContentWriter tmpContentWriter) &#123; this.tmpContentWriter = tmpContentWriter; &#125; @Override public String toString() &#123; return String.format("ContentTransaction[%s]", tmpContentWriter.getName()); &#125;&#125; 模拟内存文件读写操作的应用类 ContentDump： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104/** * Utility class to simulate in memory file like writes, flushes and closing. */public class ContentDump &#123; private boolean writable = true; /** * filename,List&lt;String&gt; */ private Map&lt;String, List&lt;String&gt;&gt; filesContent = new HashMap&lt;&gt;(); public Set&lt;String&gt; listFiles() &#123; return new HashSet&lt;&gt;(filesContent.keySet()); &#125; public void setWritable(boolean writable) &#123; this.writable = writable; &#125; /** * Creates an empty file. */ public ContentWriter createWriter(String name) &#123; checkArgument(!filesContent.containsKey(name), "File [%s] already exists", name); filesContent.put(name, new ArrayList&lt;&gt;()); return new ContentWriter(name, this); &#125; public static void move(String name, ContentDump source, ContentDump target) &#123; Collection&lt;String&gt; content = source.read(name); try (ContentWriter contentWriter = target.createWriter(name)) &#123; contentWriter.write(content).flush(); &#125; source.delete(name); &#125; public void delete(String name) &#123; filesContent.remove(name); &#125; public Collection&lt;String&gt; read(String name) &#123; List&lt;String&gt; content = filesContent.get(name); checkState(content != null, "Unknown file [%s]", name); List&lt;String&gt; result = new ArrayList&lt;&gt;(content); return result; &#125; private void putContent(String name, List&lt;String&gt; values) &#123; List&lt;String&gt; content = filesContent.get(name); checkState(content != null, "Unknown file [%s]", name); if (!writable) &#123; throw new NotWritableException(name); &#125; content.addAll(values); &#125; /** * &#123;@link ContentWriter&#125; represents an abstraction that allows to putContent to the &#123;@link ContentDump&#125;. */ public static class ContentWriter implements AutoCloseable &#123; private final ContentDump contentDump; private final String name; private final List&lt;String&gt; buffer = new ArrayList&lt;&gt;(); private boolean closed = false; private ContentWriter(String name, ContentDump contentDump) &#123; this.name = checkNotNull(name); this.contentDump = checkNotNull(contentDump); &#125; public String getName() &#123; return name; &#125; public ContentWriter write(String value) &#123; checkState(!closed); buffer.add(value); return this; &#125; public ContentWriter write(Collection&lt;String&gt; values) &#123; values.forEach(this::write); return this; &#125; public ContentWriter flush() &#123; contentDump.putContent(name, buffer); return this; &#125; public void close() &#123; buffer.clear(); closed = true; &#125; &#125; /** * Exception thrown for an attempt to write into read-only &#123;@link ContentDump&#125;. */ public class NotWritableException extends RuntimeException &#123; public NotWritableException(String name) &#123; super(String.format("File [%s] is not writable", name)); &#125; &#125;&#125; 测试场景1:测试完整的两阶段提交流程1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 发送element "42" - 0 * 发送snapshot 0 - 1 * 发送element "43" - 2 * 发送snapshot 1 - 3 * 发送element "44" - 4 * 发送snapshot 2 - 5 * &lt;p&gt; * 测试场景： * 测试完整的两阶段提交流程，通知checkpointId-1完成，那写入到target目录的数据只有"42","43" * 此时，在tmp目录下状态数据应该有一个checkpointId-2和数据"44" */@Testpublic void testNotifyOfCompletedCheckpoint() throws Exception &#123; // 准备工作：initializeEmptyState、initializeState()方法中会进行开启事务操作、打开userFunction、初始化SimpleContext // 开启事务操作，会创建一个临时文件1 harness.open(); // 向临时文件1中写入数据42 harness.processElement("42", 0); // 给checkpointId-0做snapshot，执行预提交，刷写并关闭临时文件1；开启事务操作，创建一个临时文件2 harness.snapshot(0, 1); // 向临时文件2中写入数据43 harness.processElement("43", 2); // 给checkpointId-1做snapshot，执行预提交，刷写并关闭临时文件2；开启事务操作，创建一个临时文件3 harness.snapshot(1, 3); // 向临时文件3中写入数据44 harness.processElement("44", 4); // 给checkpointId-2做snapshot，执行预提交，刷写并关闭临时文件3；开启事务操作，创建一个临时文件4 harness.snapshot(2, 5); // notify checkpointId-1 完成，才会执行提交操作，把checkpointId-1之前的临时文件都移动正式目录 harness.notifyOfCompletedCheckpoint(1); for (String name : targetDirectory.listFiles()) &#123; System.out.println("File " + name + " in target directory. content " + targetDirectory.read(name)); &#125; for (String name : tmpDirectory.listFiles()) &#123; System.out.println("File " + name + " in tmp directory. content " + tmpDirectory.read(name)); &#125; assertExactlyOnce(Arrays.asList("42", "43")); // one for checkpointId 2 and second for the currentTransaction assertEquals(2, tmpDirectory.listFiles().size());&#125; 运行结果：通知checkpointId-1完成，此时会执行提交操作。在checkpointId-1 之前处理的数据只有42、43，前两次做snapshot产生了两个临时文件，提交阶段会将这两个临时文件移动到目的目录。initializeState() 方法中会执行 beginTransactionInternal()，创建一个临时文件。接着在做snapshot的过程中 snapshotState() 方法中会先执行 preCommit() 预提交操作，关闭之前的临时文件；再执行 beginTransactionInternal()，创建一个临时文件。 12345File 5562e15f-7c27-447c-80e9-b1d773141bec in target directory. content [42]File e7df0b52-e27a-45c1-b804-26d970d87912 in target directory. content [43]File 12583208-4bef-412d-95d8-76f8bf0a0a75 in tmp directory. content []File 6bfd6970-d58e-4aea-979d-dd7dd37d77ca in tmp directory. content [44] 场景2:测试通知checkpoint完成之前出现故障1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 测试场景： * 没有执行notifyOfCompletedCheckpoint之前程序挂了，恢复时从checkpointId-1恢复 * 应该保证target目录中的数据只有"42","43" * 临时文件在close时被清除，无状态数据 */@Testpublic void testFailBeforeNotify() throws Exception &#123; harness.open(); harness.processElement("42", 0); harness.snapshot(0, 1); harness.processElement("43", 2); OperatorSubtaskState snapshot = harness.snapshot(1, 3); // 模拟在执行notifyOfCompletedCheckpoint之前出故障的场景 // 设置临时目录不可写入，为了制造不可写入的异常，以方便捕获它 tmpDirectory.setWritable(false); try &#123; // 执行到这一步因不可写入直接走到catch harness.processElement("44", 4); harness.snapshot(2, 5); fail("something should fail"); &#125; catch (Exception ex) &#123; if (!(ex.getCause() instanceof ContentDump.NotWritableException)) &#123; throw ex; &#125; // ignore &#125; // 模拟发生异常之后退出流处理流程 closeTestHarness(); // 设置临时目录可写 tmpDirectory.setWritable(true); // 从快照checkpointId-1恢复state，恢复的同时会进行提交操作，会把checkpointId-1之前的临时文件移动到目的目录中 setUpTestHarness(); harness.initializeState(snapshot); // close操作最终会调用TwoPhaseCommitSinkFunction中的close方法，会调用事务的abort()方法，abort()方法中会清除临时文件 closeTestHarness(); for (String name : targetDirectory.listFiles()) &#123; System.out.println("File " + name + " in target directory. content " + targetDirectory.read(name)); &#125; if (tmpDirectory.listFiles().isEmpty()) &#123; System.out.println("There is no file in tmp directory."); &#125; for (String name : tmpDirectory.listFiles()) &#123; System.out.println("File " + name + " in tmp directory. content " + tmpDirectory.read(name)); &#125; assertExactlyOnce(Arrays.asList("42", "43")); assertEquals(0, tmpDirectory.listFiles().size());&#125; 运行结果：重启之后，从checkpointId-1记录的snapshot恢复，initializeSize会读取状态并进行commit操作，42、43被成功提交了。关闭会执行到AbstractUdfStreamOperator.close()，进而执行到 TwoPhaseCommitSinkFunction.close() -&gt; ContentDumpSinkFunction.abort()，abort()方法中会清除临时文件。 1234File e3154038-939d-4b07-bde2-a8a399a3a648 in target directory. content [43]File bbfca9d4-ea4a-46d8-9c91-79c0938875a3 in target directory. content [42]There is no file in tmp directory. 场景3:忽略commit阶段超时提交的异常12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 测试场景： * 当执行commit操作时，如果设置了忽略提交超时异常，并且也确定是超时了，则不会向外抛出异常了 */@Testpublic void testIgnoreCommitExceptionDuringRecovery() throws Exception &#123; clock.setEpochMilli(0); harness.open(); harness.processElement("42", 0); final OperatorSubtaskState snapshot = harness.snapshot(0, 1); // 此时，42数据已经被写入目标目录，checkpointId-1 大于当前的0，可以执行到提交操作 harness.notifyOfCompletedCheckpoint(1); for (String name : targetDirectory.listFiles()) &#123; System.out.println("File " + name + " in target directory. content " + targetDirectory.read(name)); &#125; // 人为设置提交时抛出异常 throwException.set(true); closeTestHarness(); setUpTestHarness(); final long transactionTimeout = 1000; // 设置事务超时时长 sinkFunction.setTransactionTimeout(transactionTimeout); // 事务一直提交失败，已经超时了，设置跳过 sinkFunction.ignoreFailuresAfterTransactionTimeout(); try &#123; // 从snapshot恢复state，从checkpointId-0处恢复，会执行commit操作，此时commit操作中就会抛出异常 harness.initializeState(snapshot); fail("Expected exception not thrown"); &#125; catch (RuntimeException e) &#123; assertEquals("Expected exception", e.getMessage()); &#125; clock.setEpochMilli(transactionTimeout + 1); // 从snapshot恢复state，从checkpointId-0处恢复，会执行commit操作，此时commit操作中就会抛出异常 // 如果设置了忽略事务提交超时异常，并且确实发生了超时异常，则忽略此异常，外层就不用捕获了 harness.initializeState(snapshot); assertExactlyOnce(Collections.singletonList("42"));&#125; 运行结果： 1File cd349e67-b6aa-4d04-8150-79e4f80f7058 in target directory. content [42] 这个结果在前面的通知阶段就已经生成了。重启服务进行状态初始化，都会进行commit操作，这个操作可能一直失败直到超时，有时我们想放程序执行而不抛出异常，就可以通过设置以下两个属性：sinkFunction.setTransactionTimeout(transactionTimeout);sinkFunction.ignoreFailuresAfterTransactionTimeout();]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[About Python]]></title>
    <url>%2F2019%2F11%2F11%2FAbout-Python%2F</url>
    <content type="text"><![CDATA[本文将简单介绍Python 3.x。资料来自于菜鸟教程。 基础语法标识符 第一个字符必须是字母或者下划线 标识符的其他的部分由字母、数字和下划线组成 标识符对大小写敏感 python保留字python的标准库提供了一个keyword模块，可以输出当前版本的所有关键字： 123&gt;&gt;&gt; import keyword&gt;&gt;&gt; keyword.kwlist['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'] 注释1234567891011121314#!/usr/bin/python3# 第一个注释print ("Hello, Python!") # 第二个注释'''第三注释第四注释''' """第五注释第六注释""" 行与缩进python最具特色的就是使用缩进来表示代码块，不需要使用大括号 {} 。缩进的空格数是可变的，但是同一个代码块的语句必须包含相同的缩进空格数。实例如下： 123456if True: print ("Answer") print ("True")else: print ("Answer") print ("False") 多行语句python通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠()来实现多行语句，例如： 123total = item_one + \ item_two + \ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠()，例如： 12total = ['item_one', 'item_two', 'item_three', 'item_four', 'item_five'] 数字类型python中数字有四种类型：整数、布尔型、浮点数和复数。 int (整数), 如 1, 只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 bool (布尔), 如 True。 float (浮点数), 如 1.23、3E-2 complex (复数), 如 1 + 2j、 1.1 + 2.2j 字符串 python中单引号和双引号使用完全相同。 使用三引号(‘’’或”””)可以指定一个多行字符串。 转义符 ‘&#39; 反斜杠可以用来转义，使用r可以让反斜杠不发生转义。。 如 r”this is a line with \n” 则\n会显示，并不是换行。 按字面意义级联字符串，如”this “ “is “ “string”会被自动转换为this is string。 字符串可以用 + 运算符连接在一起，用 * 运算符重复。 Python 中的字符串有两种索引方式，从左往右以 0 开始，从右往左以 -1 开始。 Python中的字符串不能改变。 Python 没有单独的字符类型，一个字符就是长度为 1 的字符串。 字符串的截取的语法格式如下：变量[头下标:尾下标:步长] 12345678910111213141516171819word = '字符串'sentence = "这是一个句子。"paragraph = """这是一个段落，可以由多行组成"""str='Runoob' print(str) # 输出字符串print(str[0:-1]) # 输出第一个到倒数第二个的所有字符print(str[0]) # 输出字符串第一个字符print(str[2:5]) # 输出从第三个开始到第五个的字符print(str[2:]) # 输出从第三个开始后的所有字符print(str * 2) # 输出字符串两次print(str + '你好') # 连接字符串 print('------------------------------') print('hello\nrunoob') # 使用反斜杠(\)+n转义特殊字符print(r'hello\nrunoob') # 在字符串前面添加一个 r，表示原始字符串，不会发生转义 空行函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。 空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。 记住：空行也是程序代码的一部分。 等待用户输入执行下面的程序在按回车键后就会等待用户输入： 123#!/usr/bin/python3 input("\n\n按下 enter 键后退出。") 同一行显示多条语句Python可以在同一行中使用多条语句，语句之间使用分号(;)分割，以下是一个简单的实例： 123#!/usr/bin/python3 import sys; x = 'runoob'; sys.stdout.write(x + '\n') 多个语句构成代码组缩进相同的一组语句构成一个代码块，我们称之代码组。 像if、while、def和class这样的复合语句，首行以关键字开始，以冒号( : )结束，该行之后的一行或多行代码构成代码组。 我们将首行及后面的代码组称为一个子句(clause)。 123456if expression : suiteelif expression : suite else : suite Print输出print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=””： 12345678910111213#!/usr/bin/python3 x="a"y="b"# 换行输出print( x )print( y ) print('---------')# 不换行输出print( x, end=" " )print( y, end=" " )print() import 与 from…import在 python 用 import 或者 from…import 来导入相应的模块。将整个模块(somemodule)导入，格式为： import somemodule从某个模块中导入某个函数,格式为： from somemodule import somefunction从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc将某个模块中的全部函数导入，格式为： from somemodule import * 导入sys模块: 123456import sysprint('================Python import mode==========================')print ('命令行参数为:')for i in sys.argv: print (i)print ('\n python 路径为',sys.path) 导入sys模块的argv,path成员: 1234from sys import argv,path # 导入特定的成员 print('================python from import===================================')print('path:',path) # 因为已经导入path成员，所以此处引用时不需要加sys.path 命令行参数Python可以使用-h参数查看各参数帮助信息： 1234567891011121314151617181920python3.8 -husage: /usr/local/bin/python3.8 [option] ... [-c cmd | -m mod | file | -] [arg] ...Options and arguments (and corresponding environment variables):-b : issue warnings about str(bytes_instance), str(bytearray_instance) and comparing bytes/bytearray with str. (-bb: issue errors)-B : don't write .pyc files on import; also PYTHONDONTWRITEBYTECODE=x-c cmd : program passed in as string (terminates option list)-d : debug output from parser; also PYTHONDEBUG=x-E : ignore PYTHON* environment variables (such as PYTHONPATH)-h : print this help message and exit (also --help)-i : inspect interactively after running script; forces a prompt even if stdin does not appear to be a terminal; also PYTHONINSPECT=x-I : isolate Python from the user's environment (implies -E and -s)-m mod : run library module as a script (terminates option list)-O : remove assert and __debug__-dependent statements; add .opt-1 before .pyc extension; also PYTHONOPTIMIZE=x-OO : do -O changes and also discard docstrings; add .opt-2 before .pyc extension...... 基本数据类型Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。等号（=）用来给变量赋值。等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值。 123456789101112131415#!/usr/bin/python3 counter = 100 # 整型变量miles = 1000.0 # 浮点型变量name = "runoob" # 字符串 print (counter)print (miles)print (name)# 创建一个整型对象，值为 1，从后向前赋值，三个变量被赋予相同的数值a = b = c = 1# 两个整型对象 1 和 2 的分配给变量 a 和 b，字符串对象 "runoob" 分配给变量 ca, b, c = 1, 2, "runoob" Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 NumberPython3 支持 int、float、bool、complex（复数）。在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。像大多数语言一样，数值类型的赋值和计算都是很直观的。内置的 type() 函数可以用来查询变量所指的对象类型。 123&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class 'int'&gt; &lt;class 'float'&gt; &lt;class 'bool'&gt; &lt;class 'complex'&gt; 此外还可以用isinstance来判断: 123&gt;&gt;&gt; a = 111&gt;&gt;&gt; isinstance(a, int)True type不会认为子类是一种父类类型，isinstance会认为子类是一种父类类型： 1234567891011121314&gt;&gt;&gt; class A:... pass... &gt;&gt;&gt; class B(A):... pass... &gt;&gt;&gt; isinstance(A(), A)True&gt;&gt;&gt; type(A()) == A True&gt;&gt;&gt; isinstance(B(), A)True&gt;&gt;&gt; type(B()) == AFalse 当指定一个值时，Number对象就会被创建，可以使用del语句删除单个或多个对象的引用： 123var1=1var2=10del var1,var2 数值运算： 1234567891011121314&gt;&gt;&gt; 5+4 # 加法9&gt;&gt;&gt; 4.3-2 # 减法2.3&gt;&gt;&gt; 3*7 # 乘法21&gt;&gt;&gt; 2/4 # 除法，得到一个浮点数0.5&gt;&gt;&gt; 2//4 # 除法，得到一个整数0&gt;&gt;&gt; 17%3 # 取余2&gt;&gt;&gt; 2**5 # 乘方32 StringPython中的字符串用单引号 ‘ 或双引号 “ 括起来。使用反斜杠 \ 转义特殊字符，如果你不想让反斜杠发生转义，可以在字符串前面添加一个 r，表示原始字符串。索引值以 0 为开始值，-1 为从末尾的开始位置。加号 + 是字符串的连接符， 星号 * 表示复制当前字符串，紧跟的数字为复制的次数。 1234567891011#!/usr/bin/python3 str = 'Runoob' print (str) # 输出字符串 Runoobprint (str[0:-1]) # 输出第一个到倒数第二个的所有字符 Runooprint (str[0]) # 输出字符串第一个字符 Rprint (str[2:5]) # 输出从第三个开始到第五个的字符 nooprint (str[2:]) # 输出从第三个开始的后的所有字符 noobprint (str * 2) # 输出字符串两次 RunoobRunoobprint (str + "TEST") # 连接字符串 RunoobTEST ListList（列表） 是 Python 中使用最频繁的数据类型。列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。列表是写在方括号 [] 之间、用逗号分隔开的元素列表。加号 + 是列表连接运算符，星号 * 是重复操作。如下实例： 1234567891011#!/usr/bin/python3 list = [ 'abcd', 786 , 2.23, 'runoob', 70.2 ]tinylist = [123, 'runoob'] print (list) # 输出完整列表 ['abcd', 786, 2.23, 'runoob', 70.2]print (list[0]) # 输出列表第一个元素 abcdprint (list[1:3]) # 从第二个开始输出到第三个元素 [786, 2.23]print (list[2:]) # 输出从第三个元素开始的所有元素 [2.23, 'runoob', 70.2]print (tinylist * 2) # 输出两次列表 [123, 'runoob', 123, 'runoob']print (list + tinylist) # 连接列表 ['abcd', 786, 2.23, 'runoob', 70.2, 123, 'runoob'] 与Python字符串不一样的是，列表中的元素是可以改变的: 12345678&gt;&gt;&gt; a=[1,2,3,4,5,6]&gt;&gt;&gt; a[0]=9&gt;&gt;&gt; a[2:5]=[13,14,15]&gt;&gt;&gt; a[9, 2, 13, 14, 15, 6]&gt;&gt;&gt; a[2:5]=[] # 将对应的元素值设置为[]&gt;&gt;&gt; a[9, 2, 6] List内置了很多方法，如append()、pop()等 Tuple元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。元组中的元素类型也可以不相同 1234567891011#!/usr/bin/python3 tuple = ( 'abcd', 786 , 2.23, 'runoob', 70.2 )tinytuple = (123, 'runoob') print (tuple) # 输出完整元组 ('abcd', 786, 2.23, 'runoob', 70.2)print (tuple[0]) # 输出元组的第一个元素 abcdprint (tuple[1:3]) # 输出从第二个元素开始到第三个元素 (786, 2.23)print (tuple[2:]) # 输出从第三个元素开始的所有元素 (2.23, 'runoob', 70.2)print (tinytuple * 2) # 输出两次元组 (123, 'runoob', 123, 'runoob')print (tuple + tinytuple) # 连接元组 ('abcd', 786, 2.23, 'runoob', 70.2, 123, 'runoob') string、list 和 tuple 都属于 sequence（序列）。 Set集合（set）是由一个或数个形态各异的大小整体组成的，构成集合的事物或对象称作元素或是成员。基本功能是进行成员关系测试和删除重复元素。可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 12345678910111213141516171819202122#!/usr/bin/python3 student = &#123;'Tom', 'Jim', 'Mary', 'Tom', 'Jack', 'Rose'&#125; print(student) # 输出集合，重复的元素被自动去掉 &#123;'Mary', 'Jim', 'Rose', 'Jack', 'Tom'&#125; # 成员测试if 'Rose' in student : print('Rose 在集合中') # Rose 在集合中else : print('Rose 不在集合中') # set可以进行集合运算a = set('abracadabra')b = set('alacazam') print(a) # &#123;'b', 'a', 'c', 'r', 'd'&#125;print(a - b) # a 和 b 的差集 &#123;'b', 'd', 'r'&#125; print(a | b) # a 和 b 的并集 &#123;'l', 'r', 'a', 'c', 'z', 'm', 'b', 'd'&#125; print(a &amp; b) # a 和 b 的交集 &#123;'a', 'c'&#125;print(a ^ b) # a 和 b 中不同时存在的元素 &#123;'l', 'r', 'z', 'm', 'b', 'd'&#125; Dictionary字典（dictionary）是Python中另一个非常有用的内置数据类型。列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。键(key)必须使用不可变类型。在同一个字典中，键(key)必须是唯一的。 1234567891011121314#!/usr/bin/python3 dict = &#123;&#125;dict['one'] = "1 - 菜鸟教程"dict[2] = "2 - 菜鸟工具" tinydict = &#123;'name': 'runoob','code':1, 'site': 'www.runoob.com'&#125; print (dict['one']) # 输出键为 'one' 的值 1 - 菜鸟教程print (dict[2]) # 输出键为 2 的值 2 - 菜鸟工具print (tinydict) # 输出完整的字典 &#123;'name': 'runoob', 'code': 1, 'site': 'www.runoob.com'&#125;print (tinydict.keys()) # 输出所有键 dict_keys(['name', 'code', 'site'])print (tinydict.values()) # 输出所有值 dict_values(['runoob', 1, 'www.runoob.com']) 数据类型转换 函数 描述 int(x[,base]) 将x转换成整数 float(x) 将x转换成浮点数 complex(real[,imag]) 创建复数 str(x) 将x转换成字符串 repr(x) 将x转换成表达式字符串 eval(str) 用来计算在字符串中的有效Python表达式，并返回一个对象 tuple(s) 将序列s转换为元组 list(s) 将序列s转换为列表 set(s) 转换成可变集合 dict(d) 创建一个字典，d必须是一个(key,value)元组序列 frozenset(s) 转换成不可变集合 chr(x) 将整数转换成字符 ord(x) 将字符转换成它的整数值 hex(x) 将整数转换成十六进制字符串 oct(x) 将整数转换成八进制字符串 解释器一般默认的python版本为2.x，可以安装python3.x版本，将命令如 python3.8 加入到 PATH 环境变量中。 交互式编程12345&gt;&gt;&gt; flag=True&gt;&gt;&gt; if flag:... print("flag条件为True")... flag条件为True 脚本式编程hello.py在脚本顶部添加#! /usr/bin/env python3.8，可以让Python脚本像Shell脚本一样直接执行： 12#! /usr/bin/env python3.8print("Hello,Python!"); 需要修改脚本执行权限: 1chmod +x hello.py python执行: 12$ python3.8 hello.pyHello,Python! shell执行: 12$ ./hello.pyHello,Python! 运算符算术运算符1+、-、*、/、%、**、// 12345678910111213141516171819202122232425262728293031#!/usr/bin/python3 a = 21b = 10c = 0 c = a + bprint ("1 - c 的值为：", c) # 31 c = a - bprint ("2 - c 的值为：", c) # 11 c = a * bprint ("3 - c 的值为：", c) # 210 c = a / bprint ("4 - c 的值为：", c) # 2 c = a % bprint ("5 - c 的值为：", c) # 1 # 修改变量 a 、b 、ca = 2b = 3c = a**b print ("6 - c 的值为：", c) # 8 a = 10b = 5c = a//b print ("7 - c 的值为：", c) # 2 比较运算符1==、!=、&gt;、&lt;、&gt;=、&lt;= 1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/python3 a = 21b = 10c = 0 if ( a == b ): print ("1 - a 等于 b")else: print ("1 - a 不等于 b") if ( a != b ): print ("2 - a 不等于 b")else: print ("2 - a 等于 b") if ( a &lt; b ): print ("3 - a 小于 b")else: print ("3 - a 大于等于 b") if ( a &gt; b ): print ("4 - a 大于 b")else: print ("4 - a 小于等于 b") # 修改变量 a 和 b 的值a = 5;b = 20;if ( a &lt;= b ): print ("5 - a 小于等于 b")else: print ("5 - a 大于 b") if ( b &gt;= a ): print ("6 - b 大于等于 a")else: print ("6 - b 小于 a") 赋值运算符1=、+=、-=、*=、/=、%=、**=、//=、:= 123# 海象运算符，可在表达式内部为变量赋值。if(n := len(a)) &gt; 10: print(f"List is too long (&#123;n&#125; elements, expected &lt;= 10)") 逻辑运算符1and、or、not 位运算符1&amp;、|、^、～ 成员运算符判断当前对象是否在字符串、列表或元组中 12345678910111213141516171819202122#!/usr/bin/python3 a = 10b = 20list = [1, 2, 3, 4, 5 ]; if ( a in list ): print ("1 - 变量 a 在给定的列表中 list 中")else: print ("1 - 变量 a 不在给定的列表中 list 中") if ( b not in list ): print ("2 - 变量 b 不在给定的列表中 list 中")else: print ("2 - 变量 b 在给定的列表中 list 中") # 修改变量 a 的值a = 2if ( a in list ): print ("3 - 变量 a 在给定的列表中 list 中")else: print ("3 - 变量 a 不在给定的列表中 list 中") 身份运算符比较两个对象的存储单元 1234567891011121314151617181920212223242526#!/usr/bin/python3 a = 20b = 20 if ( a is b ): print ("1 - a 和 b 有相同的标识")else: print ("1 - a 和 b 没有相同的标识") if ( id(a) == id(b) ): print ("2 - a 和 b 有相同的标识")else: print ("2 - a 和 b 没有相同的标识") # 修改变量 b 的值b = 30if ( a is b ): print ("3 - a 和 b 有相同的标识")else: print ("3 - a 和 b 没有相同的标识") if ( a is not b ): print ("4 - a 和 b 没有相同的标识")else: print ("4 - a 和 b 有相同的标识") 条件控制12345678910111213141516#!/usr/bin/python3 age = int(input("请输入你家狗狗的年龄: "))print("")if age &lt;= 0: print("你是在逗我吧!")elif age == 1: print("相当于 14 岁的人。")elif age == 2: print("相当于 22 岁的人。")elif age &gt; 2: human = 22 + (age -2)*5 print("对应人类年龄: ", human) ### 退出提示input("点击 enter 键退出") 循环语句while语句无限循环： 12345678#!/usr/bin/python3 var = 1while var == 1 : # 表达式永远为 true num = int(input("输入一个数字 :")) print ("你输入的数字是: ", num) print ("Good bye!") while循环中使用else语句： 12345678#!/usr/bin/python3 count = 0while count &lt; 5: print (count, " 小于 5") count = count + 1else: print (count, " 大于或等于 5") for语句for循环： 123456789&gt;&gt;&gt;languages = ["C", "C++", "Perl", "Python"] &gt;&gt;&gt; for x in languages:... print (x)... CC++PerlPython&gt;&gt;&gt; pass语句pass语句不做任何事情，一般用做占位语句 12&gt;&gt;&gt;while True:... pass # 等待键盘中断 (Ctrl+C) 迭代器与生成器迭代是Python最强大的功能之一，是访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。迭代器有两个基本的方法：iter() 和 next()。字符串，列表或元组对象都可用于创建迭代器： 使用常规for语句进行遍历： 123456#!/usr/bin/python3 list=[1,2,3,4]it = iter(list) # 创建迭代器对象for x in it: print (x, end=" ") 使用next()函数： 123456789101112#!/usr/bin/python3 import sys # 引入 sys 模块 list=[1,2,3,4]it = iter(list) # 创建迭代器对象 while True: try: print (next(it)) except StopIteration: sys.exit() 函数你可以定义一个由自己想要功能的函数，以下是简单的规则：函数代码块以 def 关键词开头，后接函数标识符名称和圆括号 ()。任何传入参数和自变量必须放在圆括号中间，圆括号之间可以用于定义参数。函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。函数内容以冒号起始，并且缩进。return [表达式] 结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回 None。 12345678910111213#!/usr/bin/python3 # 计算面积函数def area(width, height): return width * height def print_welcome(name): print("Welcome", name) print_welcome("Runoob")w = 4h = 5print("width =", w, " height =", h, " area =", area(w, h)) 数据结构列表当作堆栈使用123456789101112131415&gt;&gt;&gt; stack = [3, 4, 5]&gt;&gt;&gt; stack.append(6)&gt;&gt;&gt; stack.append(7)&gt;&gt;&gt; stack[3, 4, 5, 6, 7]&gt;&gt;&gt; stack.pop()7&gt;&gt;&gt; stack[3, 4, 5, 6]&gt;&gt;&gt; stack.pop()6&gt;&gt;&gt; stack.pop()5&gt;&gt;&gt; stack[3, 4] 将列表当作队列使用12345678910&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; queue = deque(["Eric", "John", "Michael"])&gt;&gt;&gt; queue.append("Terry") # Terry arrives&gt;&gt;&gt; queue.append("Graham") # Graham arrives&gt;&gt;&gt; queue.popleft() # The first to arrive now leaves'Eric'&gt;&gt;&gt; queue.popleft() # The second to arrive now leaves'John'&gt;&gt;&gt; queue # Remaining queue in order of arrivaldeque(['Michael', 'Terry', 'Graham']) 列表推导式123456789&gt;&gt;&gt; vec = [2, 4, 6]&gt;&gt;&gt; [3*x for x in vec][6, 12, 18]&gt;&gt;&gt; [[x, x**2] for x in vec][[2, 4], [4, 16], [6, 36]]&gt;&gt;&gt; [3*x for x in vec if x &gt; 3][12, 18]&gt;&gt;&gt; [3*x for x in vec if x &lt; 2][] 12345678&gt;&gt;&gt; vec1=[2,4,6]&gt;&gt;&gt; vec2=[4,3,-9]&gt;&gt;&gt; [x*y for x in vec1 for y in vec2][8, 6, -18, 16, 12, -36, 24, 18, -54]&gt;&gt;&gt; [vec1[i]*vec2[i] for i in range(len(vec1))][8, 12, -54]&gt;&gt;&gt; [str(round(355/113, i)) for i in range(1, 6)]['3.1', '3.14', '3.142', '3.1416', '3.14159'] 嵌套列表解析将3X4的矩阵列表转换为4X3列表: 1234567&gt;&gt;&gt; matrix=[... [1,2,3,4],... [5,6,7,8],... [9,10,11,12],... ]&gt;&gt;&gt; [[row[i] for row in matrix] for i in range(4)][[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] del语句使用del语句可以从一个列表中依索引来删除一个元素 1234567891011&gt;&gt;&gt; a = [-1, 1, 66.25, 333, 333, 1234.5]&gt;&gt;&gt; del a[0]&gt;&gt;&gt; a[1, 66.25, 333, 333, 1234.5]&gt;&gt;&gt; del a[2:4]&gt;&gt;&gt; a[1, 66.25, 1234.5]&gt;&gt;&gt; del a[:] # 清空整个列表&gt;&gt;&gt; a[]&gt;&gt;&gt; del a # 删除实体变量 元组和序列元组在输出时总是有括号的，以便于正确表达嵌套结构。输入时可以没有括号。 12345678&gt;&gt;&gt; t = 12345, 54321, 'hello!'&gt;&gt;&gt; t[0]12345&gt;&gt;&gt; t(12345, 54321, 'hello!')&gt;&gt;&gt; u=t,(1,2,3,4,5)&gt;&gt;&gt; u((12345, 54321, 'hello!'), (1, 2, 3, 4, 5)) 集合12345678910111213141516171819202122232425&gt;&gt;&gt; basket = &#123;'apple', 'orange', 'apple', 'pear', 'orange', 'banana'&#125;&gt;&gt;&gt; basket&#123;'banana', 'orange', 'apple', 'pear'&#125;&gt;&gt;&gt; 'orange' in basketTrue&gt;&gt;&gt; 'crabgrass' in basketFalse&gt;&gt;&gt; # 以下演示了两个集合的操作...&gt;&gt;&gt; a = set('abracadabra')&gt;&gt;&gt; b = set('alacazam')&gt;&gt;&gt; a&#123;'d', 'a', 'r', 'b', 'c'&#125;&gt;&gt;&gt; a-b&#123;'d', 'r', 'b'&#125;&gt;&gt;&gt; a|b&#123;'d', 'a', 'r', 'm', 'z', 'l', 'b', 'c'&#125;&gt;&gt;&gt; a&amp;b&#123;'a', 'c'&#125;&gt;&gt;&gt; a^b&#123;'d', 'r', 'm', 'z', 'l', 'b'&#125;&gt;&gt;&gt; a = &#123;x for x in 'abracadabra' if x not in 'abc'&#125;&gt;&gt;&gt; a&#123;'d', 'r'&#125; 字典12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; tel = &#123;'jack': 4098, 'sape': 4139&#125;&gt;&gt;&gt; tel['guido'] = 4127&gt;&gt;&gt; tel&#123;'sape': 4139, 'guido': 4127, 'jack': 4098&#125;&gt;&gt;&gt; tel['jack']4098&gt;&gt;&gt; del tel['sape']&gt;&gt;&gt; tel['irv'] = 4127&gt;&gt;&gt; tel&#123;'guido': 4127, 'irv': 4127, 'jack': 4098&#125;&gt;&gt;&gt; list(tel.keys())['irv', 'guido', 'jack']&gt;&gt;&gt; sorted(tel.keys())['guido', 'irv', 'jack']&gt;&gt;&gt; 'guido' in telTrue&gt;&gt;&gt; 'jack' not in telFalse&gt;&gt;&gt; # 直接从键值对元组列表中构建字典...&gt;&gt;&gt; dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])&#123;'sape': 4139, 'jack': 4098, 'guido': 4127&#125;&gt;&gt;&gt; dict(sape=4139, guido=4127, jack=4098)&#123;'sape': 4139, 'jack': 4098, 'guido': 4127&#125;&gt;&gt;&gt; # 字典推导可以用来创建任意键和值的表达式词典...&gt;&gt;&gt; &#123;x: x**2 for x in (2, 4, 6)&#125;&#123;2: 4, 4: 16, 6: 36&#125; 遍历遍历字典： 123456&gt;&gt;&gt; knights = &#123;'gallahad': 'the pure', 'robin': 'the brave'&#125;&gt;&gt;&gt; for k, v in knights.items():... print(k, v)...gallahad the purerobin the brave 遍历序列： 123456&gt;&gt;&gt; for i, v in enumerate(['tic', 'tac', 'toe']):... print(i, v)...0 tic1 tac2 toe 同时遍历两个或更多的序列： 12345678&gt;&gt;&gt; questions = ['name', 'quest', 'favorite color']&gt;&gt;&gt; answers = ['lancelot', 'the holy grail', 'blue']&gt;&gt;&gt; for q, a in zip(questions, answers):... print('What is your &#123;0&#125;? It is &#123;1&#125;.'.format(q, a))...What is your name? It is lancelot.What is your quest? It is the holy grail.What is your favorite color? It is blue. 反向遍历序列： 12345678&gt;&gt;&gt; for i in reversed(range(1, 10, 2)):... print(i)...97531 按序遍历序列： 12345678&gt;&gt;&gt; basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']&gt;&gt;&gt; for f in sorted(set(basket)):... print(f)...applebananaorangepear 模块import语句123456#!/usr/bin/python3# Filename: support.py def print_func( par ): print ("Hello : ", par) return 12345678#!/usr/bin/python3# Filename: test.py # 导入模块import support # 现在可以调用模块里包含的函数了support.print_func("Runoob") 一个模块只会被导入一次，不管执行了多少次import，这样可以防止模块被重复执行。当执行import语句时，Python解释器是如何找到对应的文件呢？搜索路径被存在sys模块的path变量。 123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path['', '/Library/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages'] from…import语句从模块中导入指定部分到当前命名空间中，引入部分函数 12345678910111213141516171819#!/usr/bin/python3# Filename: fibo.py# 斐波那契(fibonacci)数列模块 def fib(n): # 定义到 n 的斐波那契数列 a, b = 0, 1 while b &lt; n: print(b, end=' ') a, b = b, a+b print() def fib2(n): # 返回到 n 的斐波那契数列 result = [] a, b = 0, 1 while b &lt; n: result.append(b) a, b = b, a+b return result 12345678910&gt;&gt;&gt; import fibo# code object from /private/tmp/fibo.py# created '/private/tmp/__pycache__/fibo.cpython-38.pyc'import 'fibo' # &lt;_frozen_importlib_external.SourceFileLoader object at 0x10b173910&gt;&gt;&gt;&gt; fibo.fib(1000)1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 &gt;&gt;&gt; fibo.fib2(100)[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]&gt;&gt;&gt; fibo.__name__'fibo' 12345&gt;&gt;&gt; from fibo import fib,fib2&gt;&gt;&gt; fib(1000)1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 &gt;&gt;&gt; fib2(100)[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89] 可以一次性把模块中的所有函数、变量都导入过来。大多数情况下，Python程序员不会使用这种方法，因为引入的其它来源命名，很可能覆盖了已有的定义。 1&gt;&gt;&gt; from fibo import * name属性一个模块第一次被一个程序引用时，其主程序将被运行。如果想在模块被引用时，模块中的某一程序块不被执行，可以用name属性来使该程序块仅在该模块自身运行时执行。 1234567#!/usr/bin/python3# Filename: using_name.pyif __name__ == '__main__': print('程序自身在运行')else: print('我来自另一模块') 运行输出如下： 12$ python using_name.py程序自身在运行 123&gt;&gt;&gt; import using_name我来自另一模块&gt;&gt;&gt; dir()函数内置的dir()函数可以找到模块内定义的所有属性，以一个字符串列表的形式返回 12345678&gt;&gt;&gt; import fibo,sys# /private/tmp/__pycache__/fibo.cpython-38.pyc matches /private/tmp/fibo.py# code object from '/private/tmp/__pycache__/fibo.cpython-38.pyc'import 'fibo' # &lt;_frozen_importlib_external.SourceFileLoader object at 0x104370a60&gt;&gt;&gt;&gt; dir(fibo)['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'fib', 'fib2']&gt;&gt;&gt; dir(sys)['__breakpointhook__', '__displayhook__', '__doc__', '__excepthook__', '__interactivehook__', '__loader__', '__name__', '__package__', '__spec__', '__stderr__', '__stdin__', '__stdout__', '__unraisablehook__', '_base_executable', '_clear_type_cache', '_current_frames', '_debugmallocstats', '_framework', '_getframe', '_git', '_home', '_xoptions', 'abiflags', 'addaudithook', 'api_version', 'argv', 'audit', 'base_exec_prefix', 'base_prefix', 'breakpointhook', 'builtin_module_names', 'byteorder', 'call_tracing', 'callstats', 'copyright', 'displayhook', 'dont_write_bytecode', 'exc_info', 'excepthook', 'exec_prefix', 'executable', 'exit', 'flags', 'float_info', 'float_repr_style', 'get_asyncgen_hooks', 'get_coroutine_origin_tracking_depth', 'getallocatedblocks', 'getcheckinterval', 'getdefaultencoding', 'getdlopenflags', 'getfilesystemencodeerrors', 'getfilesystemencoding', 'getprofile', 'getrecursionlimit', 'getrefcount', 'getsizeof', 'getswitchinterval', 'gettrace', 'hash_info', 'hexversion', 'implementation', 'int_info', 'intern', 'is_finalizing', 'maxsize', 'maxunicode', 'meta_path', 'modules', 'path', 'path_hooks', 'path_importer_cache', 'platform', 'prefix', 'ps1', 'ps2', 'pycache_prefix', 'set_asyncgen_hooks', 'set_coroutine_origin_tracking_depth', 'setcheckinterval', 'setdlopenflags', 'setprofile', 'setrecursionlimit', 'setswitchinterval', 'settrace', 'stderr', 'stdin', 'stdout', 'thread_info', 'unraisablehook', 'version', 'version_info', 'warnoptions'] 没有给定参数，那么dir()函数会罗列出当前定义的所有属性 12345678&gt;&gt;&gt; a=[1,2,3,4,5]&gt;&gt;&gt; import fibo&gt;&gt;&gt; fib = fibo.fib&gt;&gt;&gt; dir() # 得到一个当前模块中定义的属性列表['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'a', 'fib', 'fibo', 'support', 'sys']&gt;&gt;&gt; del a&gt;&gt;&gt; dir()['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'fib', 'fibo', 'support', 'sys'] 标准模块Python 本身自带一些标准的模块库，有些模块直接被构建在解析器里。 123456789&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.ps1'&gt;&gt;&gt; '&gt;&gt;&gt; sys.ps2'... '&gt;&gt;&gt; sys.ps1='C&gt; 'C&gt; print('Runoob!')Runoob!C&gt; 包目录只有包含一个叫做init.py的文件才会被认为是一个包，可以是一个空文件或包含一些初始化代码。 1234567891011121314151617181920212223sound/ 顶层包 __init__.py 初始化 sound 包 formats/ 文件格式转换子包 __init__.py wavread.py wavwrite.py aiffread.py aiffwrite.py auread.py auwrite.py ... effects/ 声音效果子包 __init__.py echo.py surround.py reverse.py ... filters/ filters 子包 __init__.py equalizer.py vocoder.py karaoke.py ... 用户可以每次只导入一个包里面的特定模块： 123456# 导入子模块方法1import sound.effects.echo# 导入子模块方法2from sound.effects import echo# 直接导入函数from sound.effects.echo import echofilter 输入和输出输出格式美化Python两种输出值的方式：表达式语句和print()函数 str()：函数返回一个用户易读的表达形式 repr()：产生一个解释器易读的表达形式 可以用format等格式美化后输出 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; for x in range(1, 11):... # rjust方法可以将字符串靠右，并在左边填充空格。ljust()、center()... print(repr(x).rjust(2), repr(x*x).rjust(3), end=' ')... # 注意前一行 'end' 的使用... print(repr(x*x*x).rjust(4))... 1 1 1 2 4 8 3 9 27 4 16 64 5 25 125 6 36 216 7 49 343 8 64 512 9 81 72910 100 1000&gt;&gt;&gt; for x in range(1, 11):... print('&#123;0:2d&#125; &#123;1:3d&#125; &#123;2:4d&#125;'.format(x, x*x, x*x*x))... 1 1 1 2 4 8 3 9 27 4 16 64 5 25 125 6 36 216 7 49 343 8 64 512 9 81 72910 100 1000 zfill会在左边填充0： 123456&gt;&gt;&gt; '12'.zfill(5)'00012'&gt;&gt;&gt; '-3.14'.zfill(7)'-003.14'&gt;&gt;&gt; '3.14159265359'.zfill(5)'3.14159265359' str.format： 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; print('&#123;&#125;网址： "&#123;&#125;!"'.format('菜鸟教程', 'www.runoob.com'))菜鸟教程网址： "www.runoob.com!"&gt;&gt;&gt; print('&#123;0&#125; 和 &#123;1&#125;'.format('Google', 'Runoob'))Google 和 Runoob&gt;&gt;&gt; print('&#123;1&#125; 和 &#123;0&#125;'.format('Google', 'Runoob'))Runoob 和 Google&gt;&gt;&gt; print('&#123;name&#125;网址： &#123;site&#125;'.format(name='菜鸟教程', site='www.runoob.com'))菜鸟教程网址： www.runoob.com&gt;&gt;&gt; print('站点列表 &#123;0&#125;, &#123;1&#125;, 和 &#123;other&#125;。'.format('Google', 'Runoob', other='Taobao'))站点列表 Google, Runoob, 和 Taobao。&gt;&gt;&gt; import math&gt;&gt;&gt; print('常量 PI 的值近似为 &#123;0:.3f&#125;。'.format(math.pi))常量 PI 的值近似为 3.142。&gt;&gt;&gt; table = &#123;'Google': 1, 'Runoob': 2, 'Taobao': 3&#125;&gt;&gt;&gt; for name, number in table.items():... print('&#123;0:10&#125; ==&gt; &#123;1:10d&#125;'.format(name, number))...Google ==&gt; 1Runoob ==&gt; 2Taobao ==&gt; 3&gt;&gt;&gt; table = &#123;'Google': 1, 'Runoob': 2, 'Taobao': 3&#125;&gt;&gt;&gt; print('Runoob: &#123;0[Runoob]:d&#125;; Google: &#123;0[Google]:d&#125;; Taobao: &#123;0[Taobao]:d&#125;'.format(table))Runoob: 2; Google: 1; Taobao: 3&gt;&gt;&gt; table = &#123;'Google': 1, 'Runoob': 2, 'Taobao': 3&#125;&gt;&gt;&gt; print('Runoob: &#123;Runoob:d&#125;; Google: &#123;Google:d&#125;; Taobao: &#123;Taobao:d&#125;'.format(**table))Runoob: 2; Google: 1; Taobao: 3 读取键盘输入1234#!/usr/bin/python3str = input("请输入：");print ("你输入的内容是: ", str) 读和写文件mode决定了打开文件的模式： 1r,rb,r+,rb+,w,wb,w+,wb+,a,ab,a+,ab+ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/usr/bin/python3f = open("/tmp/foo.txt", "w")f.write( "Python 是一个非常好的语言。\n是的，的确非常好!!\n" )f.close()f = open("/tmp/foo.txt", "r")# 没有指定size或为负，那么该文件的所有内容都将被读取并且返回str = f.read()print(str)f.close()f = open("/tmp/foo.txt", "r")# 从文件中读取单独的一行str = f.readline()print(str)f.close()f = open("/tmp/foo.txt", "r")# 将返回该文件中包含的所有行str = f.readlines()print(str)f.close()f = open("/tmp/foo.txt", "r")# 迭代一个文件对象然后读取每行for line in f: print(line, end='')f.close()f = open("/tmp/foo.txt", "w")# 返回写入的字节数num = f.write( "Python 是一个非常好的语言。\n是的，的确非常好!!\n" )print(num)f.close()f = open("/tmp/foo1.txt", "w")value = ('www.runoob.com', 14)# 如果要写入一些不是字符串的东西，那么将需要先进行转换s = str(value)f.write(s)f.close()# 返回文件对象当前所处的位置，是从文件开头开始算起的字节数f.tell() 错误和异常try/except一个try语句可能包含多个except子句： 12345678910111213141516import sys try: f = open('myfile.txt') s = f.readline() i = int(s.strip())except OSError as err: print("OS error: &#123;0&#125;".format(err))except ValueError: print("Could not convert data to an integer.")# 最后一个except子句可以忽略异常的名称，它将被当作通配符使用except: # 打印错误信息 print("Unexpected error:", sys.exc_info()[0]) # 再次把异常抛出 raise try/except…else12345678for arg in sys.argv[1:]: try: f = open(arg, 'r') except IOError: print('cannot open', arg) else: print(arg, 'has', len(f.readlines()), 'lines') f.close() else子句将在try子句没有发生任何异常的时候执行。使用else子句比把所有的语句都放在try子句里面要好，这样可以避免一些意想不到，而except又无法捕获的异常。 try-finallytry-finally语句无论是否发生异常都将执行最后的代码： 123456789101112try: runoob()except AssertionError as error: print(error)else: try: with open('file.log') as file: read_data = file.read() except FileNotFoundError as fnf_error: print(fnf_error)finally: print('这句话，无论异常是否发生都会执行。') 如果一个异常在try子句里(或者在except和else子句)被抛出，而又没有任何的except把它截住，那么这个异常会在finally子句执行后被抛出： 12345678910111213141516171819202122&gt;&gt;&gt;def divide(x, y): try: result = x / y except ZeroDivisionError: print("division by zero!") else: print("result is", result) finally: print("executing finally clause") &gt;&gt;&gt; divide(2, 1)result is 2.0executing finally clause&gt;&gt;&gt; divide(2, 0)division by zero!executing finally clause&gt;&gt;&gt; divide("2", "1")executing finally clauseTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in ? File "&lt;stdin&gt;", line 3, in divideTypeError: unsupported operand type(s) for /: 'str' and 'str' raisePython使用raise抛出一个指定的异常。必须是一个异常的实例或者是异常的类（也就是Exception的子类） 123x = 10if x &gt; 5: raise Exception('x 不能大于 5。x 的值为: &#123;&#125;'.format(x)) 一个简单的raise语句就可以将异常再次抛出 12345&gt;&gt;&gt;try: raise NameError('HiThere') except NameError: print('An exception flew by!') raise 用户自定义异常当创建一个模块有可能抛出多种不同的异常时，一种通常的做法是为这个包建立一个基础异常类，然后基于这个基础类为不同的错误情况创建不同的子类： 123456789101112131415161718192021222324252627282930class Error(Exception): """Base class for exceptions in this module.""" pass class InputError(Error): """Exception raised for errors in the input. Attributes: expression -- input expression in which the error occurred message -- explanation of the error """ def __init__(self, expression, message): self.expression = expression self.message = message class TransitionError(Error): """Raised when an operation attempts a state transition that's not allowed. Attributes: previous -- state at beginning of transition next -- attempted new state message -- explanation of why the specific transition is not allowed """ def __init__(self, previous, next, message): self.previous = previous self.next = next self.message = message 大多数异常的名字都以”Error”结尾，就跟标准的异常命名一样。 预定义的清理行为一些对象定义了标准的清理行为，无论系统是否成功的使用了它，一旦不需要它了，那么这个标准的清理行为就会执行。 12for line in open("myfile.txt"): print(line, end="") 上面这段代码的问题是，当执行完毕后，文件会保持打开状态，并没有关闭。关键词with就可以保证诸如文件之类的对象在使用完之后一定会执行它的清理方法： 123with open("myfile.txt") as f: for line in f: print(line, end="") 面向对象类对象12345678910111213141516171819202122232425262728293031323334353637383940414243#!/usr/bin/python3 class MyClass: """一个简单的类实例""" i = 12345 def f(self): return 'hello world' # 实例化类x = MyClass() # 访问类的属性和方法print("MyClass 类的属性 i 为：", x.i)print("MyClass 类的方法 f 输出为：", x.f())class Complex: # 类有一个名为__init__()的特殊方法（构造方法），该方法在类实例化时会自动调用 # __init__()方法可以有参数，传递到类的实例化操作上 def __init__(self, realpart, imagpart): self.r = realpart self.i = imagpartx = Complex(3.0, -4.5)print(x.r, x.i) # 输出结果：3.0 -4.5class Test: # 类的方法和普通的函数只有一个特别的区别，必须有一个额外的第一个参数名称，按照惯例它的名称是self def prt(self): print(self) # &lt;__main__.Test instance at 0x100771878&gt; self代表的是类的实例，代表当前对象的地址 print(self.__class__) # __main__.Test 指向类 t = Test()t.prt()class Test: def prt(runoob): # self不是Python的关键字，换成runoob也是一样的 print(runoob) # &lt;__main__.Test instance at 0x100771878&gt; print(runoob.__class__) # __main__.Test t = Test()t.prt() 类的方法在类的内部，使用 def 关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数 self, 且为第一个参数，self 代表的是类的实例。 1234567891011121314151617181920#!/usr/bin/python3 #类定义class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说: 我 %d 岁。" %(self.name,self.age)) # 实例化类p = people('runoob',10,30)p.speak() 类的继承支持类的单继承和多重继承。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/usr/bin/python3 #类定义class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说: 我 %d 岁。" %(self.name,self.age)) #单继承示例class student(people): grade = '' def __init__(self,n,a,w,g): #调用父类的构函 people.__init__(self,n,a,w) self.grade = g #覆写父类的方法 def speak(self): print("%s 说: 我 %d 岁了，我在读 %d 年级"%(self.name,self.age,self.grade)) #另一个类，多重继承之前的准备class speaker(): topic = '' name = '' def __init__(self,n,t): self.name = n self.topic = t def speak(self): print("我叫 %s，我是一个演说家，我演讲的主题是 %s"%(self.name,self.topic)) #多重继承class sample(speaker,student): a ='' def __init__(self,n,a,w,g,t): student.__init__(self,n,a,w,g) speaker.__init__(self,n,t) test = sample("Tim",25,80,4,"Python")test.speak() #方法名同，默认调用的是在括号中排前地父类的方法 方法重写12345678910111213#!/usr/bin/python3 class Parent: # 定义父类 def myMethod(self): print ('调用父类方法') class Child(Parent): # 定义子类 def myMethod(self): print ('调用子类方法') c = Child() # 子类实例c.myMethod() # 子类调用重写方法super(Child,c).myMethod() #用子类对象调用父类已被覆盖的方法 类属性与方法 类的私有属性：__private_attrs 两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。 12345678910111213141516#!/usr/bin/python3 class JustCounter: __secretCount = 0 # 私有变量 publicCount = 0 # 公开变量 def count(self): self.__secretCount += 1 self.publicCount += 1 print (self.__secretCount) counter = JustCounter()counter.count()counter.count()print (counter.publicCount)print (counter.__secretCount) # 报错，实例不能访问私有变量 类的方法：def 关键字来定义一个方法，与一般的函数不同，类方法必须包含参数 self，且为第一个参数，self代表的是类的实例。self的名字并不是规定死的，也可以使用this，但是最好还是按照规定使用self。 类的私有方法：__private_method 两个下划线开头，声明该方法为私有，只能在类的内部使用。 12345678910111213141516171819202122#!/usr/bin/python3 class Site: def __init__(self, name, url): self.name = name # public self.__url = url # private def who(self): print('name : ', self.name) print('url : ', self.__url) def __foo(self): # 私有方法 print('这是私有方法') def foo(self): # 公共方法 print('这是公共方法') self.__foo() x = Site('菜鸟教程', 'www.runoob.com')x.who() # 正常输出x.foo() # 正常输出x.__foo() # 报错 类的专有方法 init : 构造函数，在生成对象时调用 del : 析构函数，释放对象时使用 repr : 打印，转换 setitem : 按照索引赋值 getitem: 按照索引获取值 len: 获得长度 cmp: 比较运算 call: 函数调用 add: 加运算 sub: 减运算 mul: 乘运算 truediv: 除运算 mod: 求余运算 pow: 乘方 运算符重载Python同样支持运算符重载，我们可以对类的专有方法进行重载。 12345678910111213141516#!/usr/bin/python3 class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print (v1 + v2) 命名空间/作用域命名空间一共有3种命名空间 内置名称：Python内置的名称，如函数名abs、char和异常名称BaseException和Exception等 全局名称：模块中定义的名称，记录了模块的变量，包括函数、类、其他模块导入的模块、模块级的变量和常量 局部名称：函数中定义的名称，记录了函数的变量，包括函数的参数和局部定义的变量 命名空间的查找顺序：局部的命名空间 -&gt; 全局命名空间 -&gt; 内置的命名空间 12345678910# var1 是全局名称var1 = 5def some_func(): # var2 是局部名称 var2 = 6 def some_inner_func(): # var3 是内嵌的局部名称 var3 = 7 全局变量和局部变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python3 total = 0 # 这是一个全局变量# 可写函数说明def sum( arg1, arg2 ): #返回2个参数的和." total = arg1 + arg2 # total在这里是局部变量. print ("函数内是局部变量 : ", total) # 30 return total #调用sum函数sum( 10, 20 )print ("函数外是全局变量 : ", total) # 0 num = 1def fun1(): # 当内部作用域想修改外部作用域的变量时，需要用到global关键字 # 需要使用 global 关键字声明 global num print(num) # 1 num = 123 print(num) # 123fun1()print(num) # 123 def outer(): num = 10 def inner(): # 修改嵌套作用域(enclosing作用域，外层非全局作用域)中的变量则需要nonlocal关键字了 nonlocal num num = 100 print(num) # 100 inner() print(num) # 100outer()a = 10def test(): a = a + 1 print(a)test() # UnboundLocalError: local variable 'a' referenced before assignmenta = 10def test(a): a = a + 1 print(a)test(a) # 11 标准库概览操作系统接口os模块提供了不少与操作系统相关联的函数 12345678910111213&gt;&gt;&gt; import os&gt;&gt;&gt; os.getcwd() # 返回当前的工作目录'C:\\Python34'&gt;&gt;&gt; os.chdir('/server/accesslogs') # 修改当前的工作目录&gt;&gt;&gt; os.system('mkdir today') # 执行系统命令 mkdir 0&gt;&gt;&gt; # 使用os这样的大型模块时内置的dir()和help()函数非常有用&gt;&gt;&gt; ...&gt;&gt;&gt; dir(os)# &lt;returns a list of all module functions&gt;&gt;&gt;&gt; help(os)# &lt;returns an extensive manual page created from the module's docstrings&gt; 针对日常的文件和目录管理任务，shutil模块提供了一个易于使用的高级接口： 123&gt;&gt;&gt; import shutil&gt;&gt;&gt; shutil.copyfile('data.db', 'archive.db')&gt;&gt;&gt; shutil.move('/build/executables', 'installdir') 文件通配符glob模块提供了一个函数用于从目录通配符搜索中生成文件列表： 123&gt;&gt;&gt; import glob&gt;&gt;&gt; glob.glob('*.py')['primes.py', 'random.py', 'quote.py'] 命令行参数通用工具脚本经常调用命令行参数。这些命令行参数以链表形式存储于 sys 模块的 argv 变量。例如在命令行中执行 “python demo.py one two three” 后可以得到以下输出结果: 123&gt;&gt;&gt; import sys&gt;&gt;&gt; print(sys.argv)['demo.py', 'one', 'two', 'three'] 错误输出重定向和程序终止sys 还有 stdin，stdout 和 stderr 属性，即使在 stdout 被重定向时，后者也可以用于显示警告和错误信息。 12&gt;&gt;&gt; sys.stderr.write('Warning, log file not found starting a new one\n')Warning, log file not found starting a new one 大多数脚本的定向终止都使用sys.exit()。 字符串正则匹配re模块提供了正则表达式工具 12345&gt;&gt;&gt; import re&gt;&gt;&gt; re.findall(r'\bf[a-z]*', 'which foot or hand fell fastest')['foot', 'fell', 'fastest']&gt;&gt;&gt; re.sub(r'(\b[a-z]+) \1', r'\1', 'cat in the the hat')'cat in the hat' 数学math模块为浮点运算提供了对底层C函数库的访问： 12345&gt;&gt;&gt; import math&gt;&gt;&gt; math.cos(math.pi / 4)0.70710678118654757&gt;&gt;&gt; math.log(1024, 2)10.0 random模块提供了生成随机数的工具： 123456789&gt;&gt;&gt; import random&gt;&gt;&gt; random.choice(['apple', 'pear', 'banana']) # 随机选择一个样本'apple'&gt;&gt;&gt; random.sample(range(100), 10) # 随机生成100以内的10个样本[30, 83, 16, 4, 8, 81, 41, 50, 18, 33]&gt;&gt;&gt; random.random() # 随机生成浮点数0.17970987693706186&gt;&gt;&gt; random.randrange(6) # 随机生成6以内的数字4 日期和时间datetime模块为日期和时间处理同时提供了简单和复杂的方法。支持日期和时间算法的同时，重点放在更有效的处理和格式化输出。还支持时区处理。 1234567891011&gt;&gt;&gt; from datetime import date&gt;&gt;&gt; now=date.today()&gt;&gt;&gt; nowdatetime.date(2019, 12, 12)&gt;&gt;&gt; now.strftime("%m-%d-%y. %d %b %Y is a %A on the %d day of %B.")'12-12-19. 12 Dec 2019 is a Thursday on the 12 day of December.'&gt;&gt;&gt; birthday = date(1964, 7, 31)&gt;&gt;&gt; age = now - birthday&gt;&gt;&gt; age.days20222 数据压缩支持通用的数据打包和压缩格式：zlib，gzip，bz2，zipfile，以及 tarfile。 1234567891011&gt;&gt;&gt; import zlib&gt;&gt;&gt; s = b'witch which has which witches wrist watch'&gt;&gt;&gt; len(s)41&gt;&gt;&gt; t=zlib.compress(s)&gt;&gt;&gt; len(t)37&gt;&gt;&gt; zlib.decompress(t)b'witch which has which witches wrist watch'&gt;&gt;&gt; zlib.crc32(s)226805979 性能度量同一问题的不同解决方法之间是有性能差异的。Python提供了timeit模块作为一个度量工具。例如，使用元组封装和拆封来交换元素看起来比传统的方法更快一些： 12345&gt;&gt;&gt; import timeit&gt;&gt;&gt; Timer('t=a; a=b; b=t', 'a=1; b=2').timeit()0.02384132400038652&gt;&gt;&gt; Timer('a,b = b,a', 'a=1; b=2').timeit()0.022208834998309612 相对于timeit的细粒度，profile和pstats模块提供了针对更大代码块的时间度量工具。 测试模块doctest模块提供了一个工具，扫描模块并根据程序中内嵌的文档字符串执行测试。它强化了文档，允许doctest模块确认代码的结果是否与文档一致。 12345678910def average(values): """Computes the arithmetic mean of a list of numbers. &gt;&gt;&gt; print(average([20, 30, 70])) 40.0 """ return sum(values) / len(values)import doctestdoctest.testmod() # 自动验证嵌入测试 1TestResults(failed=0, attempted=1) unittest模块不像doctest模块那么容易使用，它可以在一个独立的文件里提供一个更全面的测试集： 12345678910111213141516171819import unittestdef average(values): """Computes the arithmetic mean of a list of numbers. &gt;&gt;&gt; print(average([20, 30, 70])) 40.0 """ return sum(values) / len(values)class TestStatisticalFunctions(unittest.TestCase): def test_average(self): self.assertEqual(average([20, 30, 70]), 40.0) self.assertEqual(round(average([1, 5, 7]), 1), 4.3) self.assertRaises(ZeroDivisionError, average, []) self.assertRaises(TypeError, average, 20, 30, 70)unittest.main() # Calling from the command line invokes all tests 12345.----------------------------------------------------------------------Ran 1 test in 0.000sOK 正则表达式re.match 与 re.searchre.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None。re.search匹配整个字符串，直到找到一个匹配的。 1234567891011121314151617#!/usr/bin/python3 import re line = "Cats are smarter than dogs"; matchObj = re.match( r'dogs', line, re.M|re.I)if matchObj: print ("match --&gt; matchObj.group() : ", matchObj.group())else: print ("No match!!") # No match!! matchObj = re.search( r'dogs', line, re.M|re.I)if matchObj: print ("search --&gt; matchObj.group() : ", matchObj.group()) # search --&gt; matchObj.group() : dogselse: print ("No match!!") 检索和替换1234567re.sub(pattern, repl, string, count=0, flags=0)- pattern： 正则中的模式字符串- repl：替换的字符串，也可为一个函数- string：要被替换查找的原始字符串- count：模式匹配后替换的最大次数，默认0表示替换所有- flags：编译时用的匹配模式，数字形式 123456789101112#!/usr/bin/python3import re phone = "2004-959-559 # 这是一个电话号码" # 删除注释num = re.sub(r'#.*$', "", phone)print ("电话号码 : ", num) # 2004-959-559 # 移除非数字的内容num = re.sub(r'\D', "", phone)print ("电话号码 : ", num) # 2004959559 1234567# 将匹配的数字乘于 2def double(matched): value = int(matched.group('value')) return str(value * 2) s = 'A23G4HFD567'print(re.sub('(?P&lt;value&gt;\d+)', double, s)) # A46G8HFD1134 compilecompile用于编译正则表达式，生成一个正则表达式（Pattern）对象，供match()和search()这两个函数使用 123456789re.compile(pattern[, flags])- pattern：一个字符串形式的正则表达式- flags：表示匹配模式，比如忽略大小写，多行模式等 * re.I：忽略大小写 * re.L：表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境 * re.M：多行模式 * re.S：即为&apos; . &apos;并且包括换行符在内的任意字符（&apos; . &apos;不包括换行符） * re.U：表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于 Unicode 字符属性数据库 * re.X：为了增加可读性，忽略空格和&apos; # &apos;后面的注释 1234567891011121314151617&gt;&gt;&gt; import re&gt;&gt;&gt; pattern = re.compile(r'\d+') # 用于匹配至少一个数字 &gt;&gt;&gt; m = pattern.match('one12twothree34four') # 查找头部，没有匹配&gt;&gt;&gt; m&gt;&gt;&gt; m = pattern.match('one12twothree34four', 2, 10) # 从'e'的位置开始匹配，没有匹配&gt;&gt;&gt; m&gt;&gt;&gt; m = pattern.match('one12twothree34four', 3, 10) # 从'1'的位置开始匹配，正好匹配&gt;&gt;&gt; m&lt;re.Match object; span=(3, 5), match='12'&gt;&gt;&gt;&gt; m.group(0) # 可省略 0 , 方法用于获得一个或多个分组匹配的字符串'12'&gt;&gt;&gt; m.start(0) # 可省略 0 , 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引）3&gt;&gt;&gt; m.end(0) # 可省略 0 , 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1)5&gt;&gt;&gt; m.span(0) # 可省略 0 , 方法返回 (start(group), end(group))(3, 5) 1234567891011121314151617&gt;&gt;&gt; import re&gt;&gt;&gt; pattern = re.compile(r'([a-z]+) ([a-z]+)', re.I) # re.I 表示忽略大小写&gt;&gt;&gt; m = pattern.match('Hello World Wide Web')&gt;&gt;&gt; m&lt;re.Match object; span=(0, 11), match='Hello World'&gt; # 匹配成功，返回一个 Match 对象&gt;&gt;&gt; m.group(0) # 返回匹配成功的整个子串'Hello World' &gt;&gt;&gt; m.groups() # 等价于 (m.group(1), m.group(2), ...)('Hello', 'World')&gt;&gt;&gt; m.group(1) # 返回第一个分组匹配成功的子串'Hello'&gt;&gt;&gt; m.span(1) # 返回第一个分组匹配成功的子串的索引(0, 5)&gt;&gt;&gt; m.group(2) # 返回第二个分组匹配成功的子串'World'&gt;&gt;&gt; m.span(2) # 返回第二个分组匹配成功的子串的索引(6, 11) findall在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。match 和 search 是匹配一次，findall 匹配所有。 1234re.findall(string[, pos[, endpos]])- string 待匹配的字符串- pos 可选参数，指定字符串的起始位置，默认为 0- endpos 可选参数，指定字符串的结束位置，默认为字符串的长度 12345678&gt;&gt;&gt; import re&gt;&gt;&gt; pattern = re.compile(r'\d+')&gt;&gt;&gt; result1 = pattern.findall('runoob 123 google 456')&gt;&gt;&gt; result2 = pattern.findall('run88oob123google456', 0, 10)&gt;&gt;&gt; result1['123', '456']&gt;&gt;&gt; result2['88', '12'] finditer和findall类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回 1234re.finditer(pattern, string, flags=0)- pattern 匹配的正则表达式- string 要匹配的字符串- flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 123456789&gt;&gt;&gt; import re&gt;&gt;&gt; it = re.finditer(r"\d+","12a32bc43jf3")&gt;&gt;&gt; for match in it: ... print (match.group())... 1232433 split12345re.split(pattern, string[, maxsplit=0, flags=0])- pattern：匹配的正则表达式- string：要匹配的字符串- maxsplit：分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数- flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 12345678&gt;&gt;&gt; re.split('\W+', 'runoob, runoob, runoob.')['runoob', 'runoob', 'runoob', '']&gt;&gt;&gt; re.split('(\W+)', ' runoob, runoob, runoob.') ['', ' ', 'runoob', ', ', 'runoob', ', ', 'runoob', '.', '']&gt;&gt;&gt; re.split('\W+', ' runoob, runoob, runoob.', 1)['', 'runoob, runoob, runoob.']&gt;&gt;&gt; re.split('\W+', ' runoob, runoob, runoob.', 1)['', 'runoob, runoob, runoob.'] MySQL(mysql-connector)使用pip命令来安装mysql-connector： 1python3 -m pip install mysql-connector 1234567891011121314151617181920212223242526272829303132333435# 批量插入数据sql = "INSERT INTO sites (name, url) VALUES (%s, %s)"val = [ ('Google', 'https://www.google.com'), ('Github', 'https://www.github.com'), ('Taobao', 'https://www.taobao.com'), ('stackoverflow', 'https://www.stackoverflow.com/')]mycursor.executemany(sql, val)mydb.commit() # 数据表内容有更新，必须使用到该语句print(mycursor.rowcount, "记录插入成功。")# 查询数据 mycursor.execute("SELECT * FROM sites")myresult = mycursor.fetchall() # fetchall() 获取所有记录for x in myresult: print(x)myresult = mycursor.fetchone() # fetchone() 只取一条记录print(myresult)# 删除数据sql = "DELETE FROM sites WHERE name = 'stackoverflow'"mycursor.execute(sql)mydb.commit()print(mycursor.rowcount, " 条记录删除") MySQL(PyMySQL)PyMySQL 是在 Python3.x 版本中用于连接 MySQL 服务器的一个库。使用pip3命令安装最新版的 PyMySQL: 1pip3 install PyMySQL 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import pymysql # 打开数据库连接db = pymysql.connect("localhost","root","123456","TESTDB" )# 使用 cursor() 方法创建一个游标对象 cursorcursor = db.cursor()# 使用 execute() 方法执行 SQL 查询 cursor.execute("SELECT VERSION()")# 使用 fetchone() 方法获取单条数据.data = cursor.fetchone()print ("Database version : %s " % data)cursor.execute("USE TESTDB")# 使用 execute() 方法执行 SQL，如果表存在则删除cursor.execute("DROP TABLE IF EXISTS EMPLOYEE")# 使用预处理语句创建表sql = """CREATE TABLE EMPLOYEE ( FIRST_NAME CHAR(20) NOT NULL, LAST_NAME CHAR(20), AGE INT, SEX CHAR(1), INCOME FLOAT )"""cursor.execute(sql)# SQL 插入语句sql = "INSERT INTO EMPLOYEE(FIRST_NAME, \ LAST_NAME, AGE, SEX, INCOME) \ VALUES ('%s', '%s', %s, '%s', %s)" % \ ('Mac', 'Mohan', 20, 'M', 2000)try: # 执行sql语句 cursor.execute(sql) # 提交到数据库执行 db.commit()except: # 如果发生错误则回滚 db.rollback()# SQL 查询语句sql = "SELECT * FROM EMPLOYEE \ WHERE INCOME &gt; %s" % (1000)try: # 执行SQL语句 cursor.execute(sql) # 获取所有记录列表 results = cursor.fetchall() for row in results: fname = row[0] lname = row[1] age = row[2] sex = row[3] income = row[4] # 打印结果 print ("fname=%s,lname=%s,age=%s,sex=%s,income=%s" % \ (fname, lname, age, sex, income ))except: print ("Error: unable to fetch data")# 关闭数据库连接db.close() 网络编程Socket又称”套接字”，应用程序通常通过”套接字”向网络发出请求或者应答网络请求，使主机间或者一台计算机上的进程间可以通讯。 server.py： 12345678910111213141516171819202122232425262728293031#!/usr/bin/python3# 文件名：server.py# 导入 socket、sys 模块import socketimport sys# 创建 socket 对象serversocket = socket.socket( socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()port = 9999# 绑定端口号serversocket.bind((host, port))# 设置最大连接数，超过后排队serversocket.listen(5)while True: # 建立客户端连接 clientsocket,addr = serversocket.accept() print("连接地址: %s" % str(addr)) msg='欢迎访问菜鸟教程！'+ "\r\n" clientsocket.send(msg.encode('utf-8')) clientsocket.close() client.py： 12345678910111213141516171819202122232425#!/usr/bin/python3# 文件名：client.py# 导入 socket、sys 模块import socketimport sys# 创建 socket 对象s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()# 设置端口号port = 9999# 连接服务，指定主机和端口s.connect((host, port))# 接收小于 1024 字节的数据msg = s.recv(1024)s.close()print (msg.decode('utf-8')) 多线程Python3 使用线程有两种方式： 12_threadthreading（推荐使用） threading模块包含_thread模块中的所有方法，还提供了其他方法： 123threading.currentThread(): 返回当前的线程变量。threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。 线程模块同样提供了Thread类来处理线程： 123456run(): 用以表示线程活动的方法start(): 启动线程活动join([time]): 等待至线程终止。阻塞调用线程直至线程的join()方法被调用终止-正常退出或者抛出未处理的异常-或者是有超时发生isAlive(): 返回线程是否是活动的getName(): 返回线程名setName(): 设置线程名 使用threading模块创建线程可以直接从 threading.Thread 继承创建一个新的子类，并实例化调用start()方法启动新线程： 12345678910111213141516171819202122232425262728293031#!/usr/bin/python3import threadingimport timeclass myThread(threading.Thread): def __init__(self,threadID,name,delay): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.delay = delay def run(self): print("开始线程：" + self.name) print_time(self.name,self.delay,5) print("退出线程：" + self.name)def print_time(threadName,delay,counter): while counter: time.sleep(delay) print("%s: %s" % (threadName, time.ctime(time.time()))) counter -= 1# 创建新线程thread1 = myThread(1, "Thread-1", 1)thread2 = myThread(2, "Thread-2", 2)# 启动新线程thread1.start()thread2.start()thread1.join()thread2.join()print("退出主线程") 运行结果： 123456789101112131415开始线程：Thread-1开始线程：Thread-2Thread-1: Fri Dec 13 14:33:11 2019Thread-2: Fri Dec 13 14:33:12 2019Thread-1: Fri Dec 13 14:33:12 2019Thread-1: Fri Dec 13 14:33:13 2019Thread-2: Fri Dec 13 14:33:14 2019Thread-1: Fri Dec 13 14:33:14 2019Thread-1: Fri Dec 13 14:33:15 2019退出线程：Thread-1Thread-2: Fri Dec 13 14:33:16 2019Thread-2: Fri Dec 13 14:33:18 2019Thread-2: Fri Dec 13 14:33:20 2019退出线程：Thread-2退出主线程 线程同步对于那些需要每次只允许一个线程操作的数据，可以放到acquire和release方法之间： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/python3import threadingimport timeclass myThread(threading.Thread): def __init__(self,threadID,name,delay): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.delay = delay def run(self): print("开始线程：" + self.name) # 获取锁，用于线程同步 threadLock.acquire() print("线程：" + self.name + " 获取锁") print_time(self.name,self.delay,3) # 释放锁，开启下一个线程 threadLock.release() print("线程：" + self.name + " 释放锁")def print_time(threadName,delay,counter): while counter: time.sleep(delay) print("%s: %s" % (threadName, time.ctime(time.time()))) counter -= 1threadLock = threading.Lock()threads = []# 创建新线程thread1 = myThread(1, "Thread-1", 1)thread2 = myThread(2, "Thread-2", 2)# 启动新线程thread1.start()thread2.start()# 添加线程到线程列表threads.append(thread1)threads.append(thread2)# 等待所有线程完成for t in threads: t.join()print("退出主线程") 运行结果： 12345678910111213开始线程：Thread-1线程：Thread-1 获取锁开始线程：Thread-2Thread-1: Fri Dec 13 16:49:32 2019Thread-1: Fri Dec 13 16:49:33 2019Thread-1: Fri Dec 13 16:49:34 2019线程：Thread-1 释放锁线程：Thread-2 获取锁Thread-2: Fri Dec 13 16:49:36 2019Thread-2: Fri Dec 13 16:49:38 2019Thread-2: Fri Dec 13 16:49:40 2019线程：Thread-2 释放锁退出主线程 线程优先级队列Python的queue模块中提供了同步的、线程安全的队列类，包括FIFO（先入先出）队列Queue，LIFO（后入先出）队列LifoQueue 和优先级队列PriorityQueue。queue模块中的常用方法： 123456789Queue.qsize()： 返回队列的大小Queue.empty()： 如果队列为空，返回True,反之FalseQueue.full()： 如果队列满了，返回True,反之False，Queue.full 与 maxsize 大小对应Queue.get([block[, timeout]])： 获取队列，timeout等待时间Queue.get_nowait()： 相当Queue.get(False)Queue.put(item)： 写入队列，timeout等待时间Queue.put_nowait(item)： 相当Queue.put(item, False)Queue.task_done()： 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号Queue.join()： 实际上意味着等到队列为空，再执行别的操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/usr/bin/python3 import queueimport threadingimport timeexitFlag = 0class myThread (threading.Thread): def __init__(self, threadID, name, q): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.q = q def run(self): print ("开启线程：" + self.name) process_data(self.name, self.q) print ("退出线程：" + self.name)def process_data(threadName, q): while not exitFlag: queueLock.acquire() if not workQueue.empty(): data = q.get() queueLock.release() print ("%s processing %s" % (threadName, data)) else: queueLock.release() time.sleep(1)threadList = ["Thread-1", "Thread-2", "Thread-3"]nameList = ["One", "Two", "Three", "Four", "Five"]queueLock = threading.Lock()workQueue = queue.Queue(10)threads = []threadID = 1# 创建新线程for tName in threadList: thread = myThread(threadID, tName, workQueue) thread.start() threads.append(thread) threadID += 1# 填充队列queueLock.acquire()for word in nameList: workQueue.put(word)queueLock.release()# 等待队列清空while not workQueue.empty(): pass# 通知线程是时候退出exitFlag = 1# 等待所有线程完成for t in threads: t.join()print ("退出主线程") 运行结果，线程退出的顺序是不一定的： 123456789101112开启线程：Thread-1开启线程：Thread-2开启线程：Thread-3Thread-1 processing OneThread-2 processing TwoThread-3 processing ThreeThread-1 processing FourThread-2 processing Five退出线程：Thread-3退出线程：Thread-2退出线程：Thread-1退出主线程 JSONjson.dumps对数据进行编码。Python编码为JSON类型转换对应表：| Python | JSON || :———-: | :—-: || dict | object || list,tuple | array || str | string || int,float… | number || True | true || False | false || None | null | 1234567891011121314#!/usr/bin/python3 import json # Python 字典类型转换为 JSON 对象data = &#123; 'no' : 1, 'name' : 'Runoob', 'url' : 'http://www.runoob.com'&#125; json_str = json.dumps(data)print ("Python 原始数据：", repr(data)) # Python 原始数据： &#123;'no': 1, 'name': 'Runoob', 'url': 'http://www.runoob.com'&#125;print ("JSON 对象：", json_str) # JSON 对象： &#123;"no": 1, "name": "Runoob", "url": "http://www.runoob.com"&#125; json.loads对数据进行解码。JSON解码为Python类型转换对应表：| JSON | Python || :———–: | :——: || object | dict || array | list || string | str || number(int) | int || number(real) | float || true | True || false | False || null | None | 12345678910111213141516171819#!/usr/bin/python3 import json # Python 字典类型转换为 JSON 对象data1 = &#123; 'no' : 1, 'name' : 'Runoob', 'url' : 'http://www.runoob.com'&#125; json_str = json.dumps(data1)print ("Python 原始数据：", repr(data1)) # Python 原始数据： &#123;'no': 1, 'name': 'Runoob', 'url': 'http://www.runoob.com'&#125;print ("JSON 对象：", json_str) # JSON 对象： &#123;"no": 1, "name": "Runoob", "url": "http://www.runoob.com"&#125; # 将 JSON 对象转换为 Python 字典data2 = json.loads(json_str)print ("data2['name']: ", data2['name']) # data2['name']: Runoobprint ("data2['url']: ", data2['url']) # data2['url']: http://www.runoob.com]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink SQL with Calcite]]></title>
    <url>%2F2019%2F11%2F10%2FFlink-SQL-with-Calcite%2F</url>
    <content type="text"><![CDATA[Apache Calcite 是一个动态数据的管理框架，可以用来构建数据库系统的语法解析模块，不包含数据存储、数据处理等功能。Calcite只是对各种数据库（不同的数据源）的查询进行了封装，并对外提供了统一的查询入口。可以将Calcite理解成一个不包含存储层的数据库，它不需要关心任何文件格式。Flink SQL 使用并对其扩展以支持 SQL 语句的解析与验证。目前 Calcite 流处理语句已实现对 SELECT, WHERE, GROUP BY, HAVING, UNION ALL, ORDER BY 以及 FLOOR, CEIL 函数的支持。本文将以代码的形式说明Calcite是如何解析DDL语句，javacc与java语言之间的转换关系。 Apache Calcitemvn install -DskipTests -Dcheckstyle.skip=true 快速入门编译Calcite源码123$ git clone https://github.com/apache/calcite.git$ cd calcite$ mvn install -DskipTests -Dcheckstyle.skip=true csv1$ cd example/csv model.json使用sqlline来连接Calcite，这个工程里面包含了SQL shell脚本: 12./sqllinesqlline&gt; !connect jdbc:calcite:model=target/test-classes/model.json admin admin target/test-classes/model.json的内容如下： 1234567891011121314&#123; "version": "1.0", "defaultSchema": "SALES", "schemas": [ &#123; "name": "SALES", "type": "custom", "factory": "org.apache.calcite.adapter.csv.CsvSchemaFactory", "operand": &#123; "directory": "sales" &#125; &#125; ]&#125; target/test-classes/sales目录下包含的文件将会被转换成3张表: 123DEPTS.csvEMPS.csv.gzSDEPTS.csv 执行元数据查询: 123456789100: jdbc:calcite:model=target/test-classes/mod&gt; !tables+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| | SALES | DEPTS | TABLE | | | | | | || | SALES | EMPS | TABLE | | | | | | || | SALES | SDEPTS | TABLE | | | | | | || | metadata | COLUMNS | SYSTEM TABLE | | | | | | || | metadata | TABLES | SYSTEM TABLE | | | | | | |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+ 查询emps表的数据: 12345678910110: jdbc:calcite:model=target/test-classes/mod&gt; SELECT * FROM emps;+-------+-------+--------+--------+---------------+-------+------+---------+---------+------------+| EMPNO | NAME | DEPTNO | GENDER | CITY | EMPID | AGE | SLACKER | MANAGER | JOINEDAT |+-------+-------+--------+--------+---------------+-------+------+---------+---------+------------+| 100 | Fred | 10 | | | 30 | 25 | true | false | 1996-08-03 || 110 | Eric | 20 | M | San Francisco | 3 | 80 | | false | 2001-01-01 || 110 | John | 40 | M | Vancouver | 2 | null | false | true | 2002-05-03 || 120 | Wilma | 20 | F | | 1 | 5 | | true | 2005-09-07 || 130 | Alice | 40 | F | Vancouver | 2 | null | false | true | 2007-01-01 |+-------+-------+--------+--------+---------------+-------+------+---------+---------+------------+5 rows selected (1.377 seconds) 加入JOIN和GROUP BY操作: 12345678910110: jdbc:calcite:model=target/test-classes/mod&gt; SELECT d.name, COUNT(*). . . . . . . . . . . . . . . . . . semicolon&gt; FROM emps AS e . . . . . . . . . . . . . . . . . . semicolon&gt; JOIN depts AS d ON e.deptno = d.deptno. . . . . . . . . . . . . . . . . . semicolon&gt; GROUP BY d.name;+-----------+--------+| NAME | EXPR$1 |+-----------+--------+| Sales | 1 || Marketing | 2 |+-----------+--------+2 rows selected (0.353 seconds) VALUES操作能够生成单独的一行，可以方便的用来测试表达式和内置的SQL函数: 12345670: jdbc:calcite:model=target/test-classes/mod&gt; VALUES CHAR_LENGTH(&apos;Hello, &apos; || &apos;world!&apos;);+--------+| EXPR$0 |+--------+| 13 |+--------+1 row selected (0.09 seconds) model-with-view.json12./sqllinesqlline&gt; !connect jdbc:calcite:model=target/test-classes/model-with-view.json admin admin target/test-classes/model-with-view.json的内容如下： 123456789101112131415161718192021&#123; "version": "1.0", "defaultSchema": "SALES", "schemas": [ &#123; "name": "SALES", "type": "custom", "factory": "org.apache.calcite.adapter.csv.CsvSchemaFactory", "operand": &#123; "directory": "sales" &#125;, "tables": [ &#123; "name": "FEMALE_EMPS", "type": "view", "sql": "SELECT * FROM emps WHERE gender = 'F'" &#125; ] &#125; ]&#125; 执行元数据查询,多了一个FEMALE_EMPS视图: 12345678910110: jdbc:calcite:model=target/test-classes/mod&gt; !tables+-----------+-------------+-------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION |+-----------+-------------+-------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| | SALES | DEPTS | TABLE | | | | | | || | SALES | EMPS | TABLE | | | | | | || | SALES | SDEPTS | TABLE | | | | | | || | SALES | FEMALE_EMPS | VIEW | | | | | | || | metadata | COLUMNS | SYSTEM TABLE | | | | | | || | metadata | TABLES | SYSTEM TABLE | | | | | | |+-----------+-------------+-------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+ 查询FEMALE_EMPS视图,就像查询表一样查询它: 12345670: jdbc:calcite:model=target/test-classes/mod&gt; SELECT e.name, d.name FROM female_emps AS e JOIN depts AS d on e.deptno = d.deptno;+-------+-----------+| NAME | NAME |+-------+-----------+| Wilma | Marketing |+-------+-----------+1 row selected (0.945 seconds) model-with-custom-table.json自定义表由用户自定义的代码来实现。 12./sqllinesqlline&gt; !connect jdbc:calcite:model=target/test-classes/model-with-custom-table.json admin admin target/test-classes/model-with-custom-table.json的内容如下: 1234567891011121314151617181920&#123; "version": "1.0", "defaultSchema": "CUSTOM_TABLE", "schemas": [ &#123; "name": "CUSTOM_TABLE", "tables": [ &#123; "name": "EMPS", "type": "custom", "factory": "org.apache.calcite.adapter.csv.CsvTableFactory", "operand": &#123; "file": "sales/EMPS.csv.gz", "flavor": "scannable" &#125; &#125; ] &#125; ]&#125; 执行元数据查询: 123456781: jdbc:calcite:model=target/test-classes/mod&gt; !tables+-----------+--------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION |+-----------+--------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| | CUSTOM_TABLE | EMPS | TABLE | | | | | | || | metadata | COLUMNS | SYSTEM TABLE | | | | | | || | metadata | TABLES | SYSTEM TABLE | | | | | | |+-----------+--------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+ 查询自定义表emps的数据: 12345678910111: jdbc:calcite:model=target/test-classes/mod&gt; SELECT empno, name FROM custom_table.emps;+-------+-------+| EMPNO | NAME |+-------+-------+| 100 | Fred || 110 | Eric || 110 | John || 120 | Wilma || 130 | Alice |+-------+-------+5 rows selected (0.049 seconds) 执行计划优化查询// TODO 未完待续 1234567891011121314151617181920sqlline&gt; !connect jdbc:calcite:model=target/test-classes/model.json admin admin0: jdbc:calcite:model=target/test-classes/mod&gt; explain plan for select name from emps;+------------------------------------------------------------------------------------------------------------------------+| PLAN |+------------------------------------------------------------------------------------------------------------------------+| EnumerableCalc(expr#0..9=[&#123;inputs&#125;], NAME=[$t1]) EnumerableInterpreter BindableTableScan(table=[[SALES, EMPS]]) |+------------------------------------------------------------------------------------------------------------------------+1 row selected (0.759 seconds)0: jdbc:calcite:model=target/test-classes/mod&gt; !connect jdbc:calcite:model=target/test-classes/smart.json admin admin1: jdbc:calcite:model=target/test-classes/sma&gt; explain plan for select name from emps;+----------------------------------------------------+| PLAN |+----------------------------------------------------+| CsvTableScan(table=[[SALES, EMPS]], fields=[[1]]) |+----------------------------------------------------+1 row selected (0.059 seconds) target/test-classes/smart.json的内容如下: 12345678910111213141516&#123; "version": "1.0", "defaultSchema": "SALES", "schemas": [ &#123; "name": "SALES", "type": "custom", "factory": "org.apache.calcite.adapter.csv.CsvSchemaFactory", "operand": &#123; "directory": "sales", // 这一行会创建一个CsvSchema，并且它的createTable方法创建的是CsvTranslatableTable，而不是CsvScannableTable "flavor": "TRANSLATABLE" &#125; &#125; ]&#125; 代数// TODO 未完待续 适配器// TODO 未完待续 流Calcite扩展了SQL和关系代数，以支持流式查询。流是连续的永远的流记录的集合。与表不同，它们通常不存储在磁盘上，而是通过网络流动，在内存中保存较短时间。流补充了表，流表示当前和未来的情况，表则表示过去。与表一样，通常希望使用基于关系代数的高级语言查询流，根据模式进行验证，并优化以利用可用的资源和算法。 流表JOIN如果表的内容没有发生变化，那么流到表的连接很简单，这个查询用每个产品的单价丰富了订单流，也就是通常所说的生成宽表。 12345678910111213141516SELECT STREAM o.rowtime, o.productId, o.orderId, o.units, p.name, p.unitPriceFROM Orders AS oJOIN Products AS p ON o.productId = p.productId; rowtime | productId | orderId | units | name | unitPrice----------+-----------+---------+-------+ -------+----------- 10:17:00 | 30 | 5 | 4 | Cheese | 17 10:17:05 | 10 | 6 | 1 | Beer | 0.25 10:18:05 | 20 | 7 | 2 | Wine | 6 10:18:07 | 30 | 8 | 20 | Cheese | 17 11:02:00 | 10 | 9 | 6 | Beer | 0.25 11:04:00 | 10 | 10 | 1 | Beer | 0.25 11:09:30 | 40 | 11 | 12 | Bread | 100 11:24:11 | 10 | 12 | 4 | Beer | 0.25 但是如果表发生变化，例如，假设产品10的单价在11点增加到0.35。11点之前的订单应该是旧价格，11点之后的订单应该是新价格。实现这一功能的一种方法是创建一个表，其中包含每个版本的开始和结束生效日期。即对维表进行版本控制。 12345678910111213141516SELECT STREAM *FROM Orders AS oJOIN ProductVersions AS p ON o.productId = p.productId AND o.rowtime BETWEEN p.startDate AND p.endDate rowtime | productId | orderId | units | productId1 | name | unitPrice----------+-----------+---------+-------+ -----------+--------+----------- 10:17:00 | 30 | 5 | 4 | 30 | Cheese | 17 10:17:05 | 10 | 6 | 1 | 10 | Beer | 0.25 10:18:05 | 20 | 7 | 2 | 20 | Wine | 6 10:18:07 | 30 | 8 | 20 | 30 | Cheese | 17 11:02:00 | 10 | 9 | 6 | 10 | Beer | 0.35 11:04:00 | 10 | 10 | 1 | 10 | Beer | 0.35 11:09:30 | 40 | 11 | 12 | 40 | Bread | 100 11:24:11 | 10 | 12 | 4 | 10 | Beer | 0.35 流流JOIN如果连接条件以某种方式强制两个流之间保持有限距离，那么连接两个流是有意义的。在以下查询中，发货日期为订单日期后1小时内: 123456789101112SELECT STREAM o.rowtime, o.productId, o.orderId, s.rowtime AS shipTimeFROM Orders AS oJOIN Shipments AS s ON o.orderId = s.orderId AND s.rowtime BETWEEN o.rowtime AND o.rowtime + INTERVAL &apos;1&apos; HOUR; rowtime | productId | orderId | shipTime----------+-----------+---------+---------- 10:17:00 | 30 | 5 | 10:55:00 10:17:05 | 10 | 6 | 10:20:00 11:02:00 | 10 | 9 | 11:58:00 11:24:11 | 10 | 12 | 11:44:00 相当多的订单没有出现，因为它们在一个小时之内没有发货。当系统接收到时间戳为11:24:11的订单10时，它已经从散列表中删除了包括时间戳为10:18:07的订单8在内的订单。 流式SQLsql-window流SQL中的窗口概念： 滚动窗口 Tumbling Window将元素分配给每个固定长度的窗口，滚动窗口具有固定的尺寸，不重叠元素 滑动窗口 Sliding Window滑动窗口将元素分配给固定长度的窗口，并且附加每次窗口的滑动频率，可以存在窗口重叠的情况 会话窗口 Session Window按照会话元素进行分组，会话窗口不重叠，没有固定的开始时间和结束时间，当一定时间没有接收到新的元素的话，则会话窗口关闭 watermarkWatermark 和 Calcite 基本无关，和流式SQL有关。数据流中经常出现事件时间（Event Time）乱序的情况。 Flink Watermark设计： 周期Watermark（Periodic Watermark） Periodic Watermark 按照固定时间间隔生成新的 watermark，不管是否有新的消息抵达。在两次 watermark 生成时间间隔内会有一部分消息流入，用户可以根据这部分数据来计算出新的watermark。 12345678910111213class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxOutOfOrderness = 3500L; // 3.5 seconds var currentMaxTimestamp: Long; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; val timestamp = element.getCreationTime() currentMaxTimestamp = max(timestamp, currentMaxTimestamp) timestamp; &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound new Watermark(currentMaxTimestamp - maxOutOfOrderness); &#125;&#125; 其中extractTimestamp用于从消息中提取事件时间，而getCurrentWatermark用于生成新的水位线，新的水位线只有大于当前水位线才是有效的。每个窗口都会有该类的一个实例，因此可以利用实例的成员变量保存状态，比如上例中的当前最大时间戳。 标点Watermark（Punctuated Watermark） 标点水位线（Punctuated Watermark）通过数据流中某些特殊标记事件来触发新水位线的生成。这种方式下窗口的触发与时间无关，而是决定于何时收到标记事件。 12345678class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123; if (element.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null &#125;&#125; 其中extractTimestamp用于从消息中提取事件时间，checkAndGetNextWatermark用于检查事件是否标点事件，若是则生成新的水位线。不同于定期水位线定时调用getCurrentWatermark，标点水位线是每接受一个事件就需要调用checkAndGetNextWatermark，若返回值非 null 且新水位线大于当前水位线，则触发窗口计算。 解析器ParserfmppFMPP是以 freemarker 为模板的模板生成器 添加 Maven 依赖123456789101112131415161718192021222324252627282930313233343536373839&lt;plugin&gt; &lt;configuration&gt; &lt;!--配置文件地址--&gt; &lt;cfgFile&gt;src/main/codegen/config.fmpp&lt;/cfgFile&gt; &lt;!--文件输出目录--&gt; &lt;outputDirectory&gt;target/generated-sources/fmpp/&lt;/outputDirectory&gt; &lt;!--文件模板存放目录--&gt; &lt;templateDirectory&gt;src/main/codegen/templates&lt;/templateDirectory&gt; &lt;/configuration&gt; &lt;groupId&gt;com.googlecode.fmpp-maven-plugin&lt;/groupId&gt; &lt;artifactId&gt;fmpp-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.28&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.fmpp&lt;/groupId&gt; &lt;artifactId&gt;fmpp&lt;/artifactId&gt; &lt;version&gt;0.9.16&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/plugin&gt; fmpp配置文件config.fmpp 123456789101112131415161718# 用data标示为变量# 一般变量替换为 $&#123;one&#125; or $&#123;two.three&#125; ,具体语法请参考freemarker语法# 也可以引用一个外部的tdd文件，# include 指令插入另外的freemarker模板data: &#123; one:1, two: &#123; three: 3 &#125; implementationFiles: [ &quot;parserImpls.ftl&quot; ]&#125;#freemarkerLinks: &#123; includes: includes/&#125; freemarker模板模板1: 1234567891011public class Main &#123; public static void main(String[] args)&#123; System.out.println($&#123;one&#125; + $&#123;two.three&#125;); &#125; /** * 额外附加代码 */ &lt;#list implementationFiles as file&gt; &lt;#include &quot;/@includes/&quot;+file /&gt; &lt;/#list&gt;&#125; 模板2: 1234static &#123; System.out.println($&#123;one&#125;); System.out.println($&#123;two.three&#125;);&#125; 执行maven插件1mvn fmpp:generate 生成文件在target/generated-sources/fmpp目录下会生成如下代码文件: 123456789101112public class Main &#123; public static void main(String[] args)&#123; System.out.println(1 + 3); &#125; /** * 额外附加代码 */static &#123; System.out.println(1); System.out.println(3);&#125;&#125; javacc使用递归下降语法解析，LL(k)其中，第一个L表示从左到右扫描输入；第二个L表示每次都进行最左推导（在推导语法树的过程中每次都替换句型中最左的非终结符为终结符）k表示每次向前探索(lookahead)k个终结符 语法描述文件12345678910111213141516options &#123; javacc的选项&#125;PARSER_BEGIN(解析器类名)package 包名；import 库名；public class 解析器类名 &#123; 任意的java代码&#125;PARSER_END(解析器类名)扫描器的描述解析器的描述 示例Parser.tdd文件中可以定义如下: 1234567891011package: &quot;com.matty.flink.sql.parser.impl&quot;,class: &quot;SqlParserImpl&quot;,imports: [ &quot;com.matty.flink.sql.parser.ddl.SqlCreateTable&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlCreateTable.TableCreationContext&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlTableColumn&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlTableOption&quot;, ... &quot;java.util.List&quot;, &quot;java.util.ArrayList&quot;] 类介绍会在 target/generated-sources 的 package 包下生成如下几个类: XXX解析类入口，如上面指定的SqlParserImpl SimpleCharStreamSimpleCharStream jj_input_stream;词法分析器的输入流 123456789101112131415161718192021222324252627// 构造函数种类 ,可以接受Reader和InputStreampublic class SimpleCharStream &#123; public SimpleCharStream(java.io.Reader dstream, int startline, int startcolumn, int buffersize); public SimpleCharStream(java.io.Reader dstream, int startline, int startcolumn); public SimpleCharStream(java.io.Reader dstream); public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline, int startcolumn, int buffersize) throws java.io.UnsupportedEncodingException; public SimpleCharStream(java.io.InputStream dstream, int startline, int startcolumn, int buffersize); public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline, int startcolumn) throws java.io.UnsupportedEncodingException; public SimpleCharStream(java.io.InputStream dstream, int startline, int startcolumn); public SimpleCharStream(java.io.InputStream dstream, String encoding) throws java.io.UnsupportedEncodingException; public SimpleCharStream(java.io.InputStream dstream);&#125; XXXConstantsToken常量，包括 SKIP TOKEN 和 TOKEN示例Parser.tdd文件中可以定义如下:1234567891011121314151617181920// 忽略的字符SKIP:&#123; &quot; &quot;&#125;// 关键字TOKEN:&#123; &lt;PLUS :&quot;+&quot;&gt;&#125;// 新增的关键字列表。如果关键字不是一个保留关键字，将其添加到&quot;无保留关键字&quot;部分。keywords:[ &quot;WATERMARK&quot;, &quot;offset&quot;]// &quot;keywords&quot;部分中未保留的关键字列表nonReservedKeywords:[ &quot;offset&quot;] 对应生成的java类为: 1234567891011121314151617181920// 和常量申明对应public interface XXXConstants &#123; int EOF = 0; int PLUS = 2; int DEFAULT = 0; int WATERMARK = 669; int offset = 678; String[] tokenImage = &#123; // EOF 文件结尾 "&lt;EOF&gt;", // 忽律字符串 "\" \"", // PLUSA "\"+\"", "\"WATERMARK\"", "\"offset\"" &#125;;&#125; XXXTokenManager词法分析器 123456789// 常见方法说明public class XXXTokenManager implements XXXConstants&#123; // 输入流 protected SimpleCharStream input_stream; // 构造函数 public XXXTokenManager(SimpleCharStream stream); // 获取下一个Token public Token getNextToken();&#125; Token 123456789101112131415161718192021222324public class Token &#123; // Constants.java的种类 public int kind; // 开始行和开始列,结束行和结束列 public int beginLine, beginColumn, endLine, endColumn; // token的字符串 public String image; // 下一个token public Token next; // 特殊令牌 public Token specialToken; // Returns the image. public String toString() &#123; return image; &#125;&#125; ParseException语法解析异常 TokenMgrError语法错误提示 语法介绍 java代码java代码块用{}声明 123456789// 定义java代码块void javaCodeDemo():&#123;&#125;&#123; &#123; int i = 0; System.out.println(i); &#125;&#125; java函数需要使用JAVACODE声明 123JAVACODE void print(Token t)&#123; System.out.println(t);&#125; 条件 if语句 [] 1234567891011121314// if语句void ifExpr():&#123;&#125;&#123; [ &lt;SELECT&gt; &#123; System.out.println(&quot;if select&quot;); &#125; ] // 循环，出现一次 (&lt;SELECT&gt;)?&#125; if else 语句 | 12345678910111213141516// if - elsevoid ifElseExpr():&#123;&#125;&#123; ( &lt;SELECT&gt; &#123;System.out.println(&quot;if else select&quot;);&#125; | &lt;UPDATE&gt; &#123;System.out.println(&quot;if else update&quot;);&#125; | &lt;DELETE&gt; &#123;System.out.println(&quot;if else delete&quot;);&#125; | &#123; System.out.println(&quot;other&quot;); &#125; )&#125; 循环 while 0~n 123456// while 0~nvoid while1Expr():&#123;&#125;&#123; (&lt;SELECT&gt;)*&#125; while 1~n 123456// while 1~nvoid while2Expr():&#123;&#125;&#123; (&lt;SELECT&gt;)+&#125; 正则表达式[]: 内容可选+: 内容出现1次或多次*: 内容出现0次或多次?: 内容出现0次或1次|: 或(): 优先级改变或整体操作 Validatorsql优化优化过程一般为以下过程： 对SQL进行词法分析得到一个语法树 根据关系代数进行SQL的逻辑优化 根据代价估算算法进行物理查询的优化 执行器执行 逻辑优化sql执行根据不同的Node定义了代码的实现方法，从最底层的RelNode依次执行，采用source接收数据，sink发送数据。 AggregateNodeFilterNodeProjectNodeSortNodeWindowNodeFlink SQL解析Flink SQL 解析的代码参考：Flink SQL 解析 config.mpp1234567891011121314151617181920212223242526## Calcite&apos;s parser grammar file (Parser.jj) is written in javacc# (https://javacc.org/) with Freemarker (http://freemarker.org/) variables# to allow clients to:# 1. have custom parser implementation class and package name.# 2. insert new parser method implementations written in javacc to parse# custom:# a) SQL statements.# b) literals.# c) data types.# 3. add new keywords to support custom SQL constructs added as part of (2).# 4. add import statements needed by inserted custom parser implementations.## Parser template file (Parser.jj) along with this file are packaged as# part of the calcite-core-&lt;version&gt;.jar under &quot;codegen&quot; directory.# calcite模板配置data: &#123; # calcite模板配置，引入parser模板文件 parser: tdd(../data/Parser.tdd)&#125;# freemarker模板的位置freemarkerLinks: &#123; includes: includes/&#125; Parser.tdd123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464&#123; # Generated parser implementation package and class name. # 生成解析器实现类包和名称 # 包名 package: &quot;com.matty.flink.sql.parser.impl&quot;, # 实体类名 class: &quot;SqlParserImpl&quot;, # List of additional classes and packages to import. # Example. &quot;org.apache.calcite.sql.*&quot;, &quot;java.util.List&quot;. # 导入处理语句 imports: [ &quot;com.matty.flink.sql.parser.ddl.SqlCreateTable&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlCreateTable.TableCreationContext&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlTableColumn&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlTableOption&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlCreateView&quot;, &quot;com.matty.flink.sql.parser.ddl.SqlCreateFunction&quot;, &quot;com.matty.flink.sql.parser.ddl.FunctionType&quot;, &quot;com.matty.flink.sql.parser.dml.RichSqlInsert&quot;, &quot;com.matty.flink.sql.parser.dml.RichSqlInsertKeyword&quot;, &quot;com.matty.flink.sql.parser.type.SqlArrayType&quot;, &quot;com.matty.flink.sql.parser.type.SqlBytesType&quot;, &quot;com.matty.flink.sql.parser.type.SqlMapType&quot;, &quot;com.matty.flink.sql.parser.type.SqlMultisetType&quot;, &quot;com.matty.flink.sql.parser.type.SqlRowType&quot;, &quot;com.matty.flink.sql.parser.type.SqlStringType&quot;, &quot;com.matty.flink.sql.parser.type.SqlTimestampType&quot;, &quot;com.matty.flink.sql.parser.type.SqlTimeType&quot;, &quot;com.matty.flink.sql.parser.validate.FlinkSqlConformance&quot;, &quot;com.matty.flink.sql.parser.FlinkSqlDataTypeSpec&quot;, &quot;org.apache.calcite.sql.SqlDrop&quot;, &quot;org.apache.calcite.sql.SqlCreate&quot;, &quot;java.util.List&quot;, &quot;java.util.ArrayList&quot; ] # List of new keywords. Example: &quot;DATABASES&quot;, &quot;TABLES&quot;. If the keyword is not a reserved # keyword, please also add it to &apos;nonReservedKeywords&apos; section. # 新增的关键字列表。如果关键字不是一个保留关键字，将其添加到&quot;无保留关键字&quot;部分。 keywords: [ &quot;COMMENT&quot;, &quot;PARTITIONED&quot;, &quot;IF&quot;, &quot;WATERMARK&quot;, &quot;withOffset&quot; &quot;ASCENDING&quot;, &quot;FROM_SOURCE&quot;, &quot;BOUNDED&quot;, &quot;DELAY&quot;, &quot;OVERWRITE&quot;, &quot;STRING&quot;, &quot;BYTES&quot;, &quot;offset&quot;, &quot;reset&quot;, &quot;SCALA&quot; ] # List of keywords from &quot;keywords&quot; section that are not reserved. # &quot;keywords&quot;部分中未保留的关键字列表。 nonReservedKeywords: [ &quot;A&quot; &quot;ABSENT&quot; &quot;ABSOLUTE&quot; &quot;ACTION&quot; &quot;ADA&quot; &quot;ADD&quot; &quot;ADMIN&quot; &quot;AFTER&quot; &quot;ALWAYS&quot; &quot;APPLY&quot; &quot;ASC&quot; &quot;ASSERTION&quot; &quot;ASSIGNMENT&quot; &quot;ATTRIBUTE&quot; &quot;ATTRIBUTES&quot; &quot;BEFORE&quot; &quot;BERNOULLI&quot; &quot;BREADTH&quot; &quot;C&quot; &quot;CASCADE&quot; &quot;CATALOG&quot; &quot;CATALOG_NAME&quot; &quot;CENTURY&quot; &quot;CHAIN&quot; &quot;CHARACTER_SET_CATALOG&quot; &quot;CHARACTER_SET_NAME&quot; &quot;CHARACTER_SET_SCHEMA&quot; &quot;CHARACTERISTICS&quot; &quot;CHARACTERS&quot; &quot;CLASS_ORIGIN&quot; &quot;COBOL&quot; &quot;COLLATION&quot; &quot;COLLATION_CATALOG&quot; &quot;COLLATION_NAME&quot; &quot;COLLATION_SCHEMA&quot; &quot;COLUMN_NAME&quot; &quot;COMMAND_FUNCTION&quot; &quot;COMMAND_FUNCTION_CODE&quot; &quot;COMMITTED&quot; &quot;CONDITION_NUMBER&quot; &quot;CONDITIONAL&quot; &quot;CONNECTION&quot; &quot;CONNECTION_NAME&quot; &quot;CONSTRAINT_CATALOG&quot; &quot;CONSTRAINT_NAME&quot; &quot;CONSTRAINT_SCHEMA&quot; &quot;CONSTRAINTS&quot; &quot;CONSTRUCTOR&quot; &quot;CONTINUE&quot; &quot;CURSOR_NAME&quot; &quot;DATA&quot; &quot;DATABASE&quot; &quot;DATETIME_INTERVAL_CODE&quot; &quot;DATETIME_INTERVAL_PRECISION&quot; &quot;DECADE&quot; &quot;DEFAULTS&quot; &quot;DEFERRABLE&quot; &quot;DEFERRED&quot; &quot;DEFINED&quot; &quot;DEFINER&quot; &quot;DEGREE&quot; &quot;DEPTH&quot; &quot;DERIVED&quot; &quot;DESC&quot; &quot;DESCRIPTION&quot; &quot;DESCRIPTOR&quot; &quot;DIAGNOSTICS&quot; &quot;DISPATCH&quot; &quot;DOMAIN&quot; &quot;DOW&quot; &quot;DOY&quot; &quot;DYNAMIC_FUNCTION&quot; &quot;DYNAMIC_FUNCTION_CODE&quot; &quot;ENCODING&quot; &quot;EPOCH&quot; &quot;ERROR&quot; &quot;EXCEPTION&quot; &quot;EXCLUDE&quot; &quot;EXCLUDING&quot; &quot;FINAL&quot; &quot;FIRST&quot; &quot;FOLLOWING&quot; &quot;FORMAT&quot; &quot;FORTRAN&quot; &quot;FOUND&quot; &quot;FRAC_SECOND&quot; &quot;G&quot; &quot;GENERAL&quot; &quot;GENERATED&quot; &quot;GEOMETRY&quot; &quot;GO&quot; &quot;GOTO&quot; &quot;GRANTED&quot; &quot;HIERARCHY&quot; &quot;IGNORE&quot; &quot;IMMEDIATE&quot; &quot;IMMEDIATELY&quot; &quot;IMPLEMENTATION&quot; &quot;INCLUDING&quot; &quot;INCREMENT&quot; &quot;INITIALLY&quot; &quot;INPUT&quot; &quot;INSTANCE&quot; &quot;INSTANTIABLE&quot; &quot;INVOKER&quot; &quot;ISODOW&quot; &quot;ISOYEAR&quot; &quot;ISOLATION&quot; &quot;JAVA&quot; &quot;JSON&quot; &quot;K&quot; &quot;KEY&quot; &quot;KEY_MEMBER&quot; &quot;KEY_TYPE&quot; &quot;LABEL&quot; &quot;LAST&quot; &quot;LENGTH&quot; &quot;LEVEL&quot; &quot;LIBRARY&quot; &quot;LOCATOR&quot; &quot;M&quot; &quot;MATCHED&quot; &quot;MAXVALUE&quot; &quot;MICROSECOND&quot; &quot;MESSAGE_LENGTH&quot; &quot;MESSAGE_OCTET_LENGTH&quot; &quot;MESSAGE_TEXT&quot; &quot;MILLISECOND&quot; &quot;MILLENNIUM&quot; &quot;MINVALUE&quot; &quot;MORE_&quot; &quot;MUMPS&quot; &quot;NAME&quot; &quot;NAMES&quot; &quot;NANOSECOND&quot; &quot;NESTING&quot; &quot;NORMALIZED&quot; &quot;NULLABLE&quot; &quot;NULLS&quot; &quot;NUMBER&quot; &quot;OBJECT&quot; &quot;OCTETS&quot; &quot;OPTION&quot; &quot;OPTIONS&quot; &quot;ORDERING&quot; &quot;ORDINALITY&quot; &quot;OTHERS&quot; &quot;OUTPUT&quot; &quot;OVERRIDING&quot; &quot;PAD&quot; &quot;PARAMETER_MODE&quot; &quot;PARAMETER_NAME&quot; &quot;PARAMETER_ORDINAL_POSITION&quot; &quot;PARAMETER_SPECIFIC_CATALOG&quot; &quot;PARAMETER_SPECIFIC_NAME&quot; &quot;PARAMETER_SPECIFIC_SCHEMA&quot; &quot;PARTIAL&quot; &quot;PASCAL&quot; &quot;PASSING&quot; &quot;PASSTHROUGH&quot; &quot;PAST&quot; &quot;PATH&quot; &quot;PLACING&quot; &quot;PLAN&quot; &quot;PLI&quot; &quot;PRECEDING&quot; &quot;PRESERVE&quot; &quot;PRIOR&quot; &quot;PRIVILEGES&quot; &quot;PUBLIC&quot; &quot;QUARTER&quot; &quot;READ&quot; &quot;RELATIVE&quot; &quot;REPEATABLE&quot; &quot;REPLACE&quot; &quot;RESPECT&quot; &quot;RESTART&quot; &quot;RESTRICT&quot; &quot;RETURNED_CARDINALITY&quot; &quot;RETURNED_LENGTH&quot; &quot;RETURNED_OCTET_LENGTH&quot; &quot;RETURNED_SQLSTATE&quot; &quot;RETURNING&quot; &quot;ROLE&quot; &quot;ROUTINE&quot; &quot;ROUTINE_CATALOG&quot; &quot;ROUTINE_NAME&quot; &quot;ROUTINE_SCHEMA&quot; &quot;ROW_COUNT&quot; &quot;SCALAR&quot; &quot;SCALE&quot; &quot;SCHEMA&quot; &quot;SCHEMA_NAME&quot; &quot;SCOPE_CATALOGS&quot; &quot;SCOPE_NAME&quot; &quot;SCOPE_SCHEMA&quot; &quot;SECTION&quot; &quot;SECURITY&quot; &quot;SELF&quot; &quot;SEQUENCE&quot; &quot;SERIALIZABLE&quot; &quot;SERVER&quot; &quot;SERVER_NAME&quot; &quot;SESSION&quot; &quot;SETS&quot; &quot;SIMPLE&quot; &quot;SIZE&quot; &quot;SOURCE&quot; &quot;SPACE&quot; &quot;SPECIFIC_NAME&quot; &quot;SQL_BIGINT&quot; &quot;SQL_BINARY&quot; &quot;SQL_BIT&quot; &quot;SQL_BLOB&quot; &quot;SQL_BOOLEAN&quot; &quot;SQL_CHAR&quot; &quot;SQL_CLOB&quot; &quot;SQL_DATE&quot; &quot;SQL_DECIMAL&quot; &quot;SQL_DOUBLE&quot; &quot;SQL_FLOAT&quot; &quot;SQL_INTEGER&quot; &quot;SQL_INTERVAL_DAY&quot; &quot;SQL_INTERVAL_DAY_TO_HOUR&quot; &quot;SQL_INTERVAL_DAY_TO_MINUTE&quot; &quot;SQL_INTERVAL_DAY_TO_SECOND&quot; &quot;SQL_INTERVAL_HOUR&quot; &quot;SQL_INTERVAL_HOUR_TO_MINUTE&quot; &quot;SQL_INTERVAL_HOUR_TO_SECOND&quot; &quot;SQL_INTERVAL_MINUTE&quot; &quot;SQL_INTERVAL_MINUTE_TO_SECOND&quot; &quot;SQL_INTERVAL_MONTH&quot; &quot;SQL_INTERVAL_SECOND&quot; &quot;SQL_INTERVAL_YEAR&quot; &quot;SQL_INTERVAL_YEAR_TO_MONTH&quot; &quot;SQL_LONGVARBINARY&quot; &quot;SQL_LONGVARNCHAR&quot; &quot;SQL_LONGVARCHAR&quot; &quot;SQL_NCHAR&quot; &quot;SQL_NCLOB&quot; &quot;SQL_NUMERIC&quot; &quot;SQL_NVARCHAR&quot; &quot;SQL_REAL&quot; &quot;SQL_SMALLINT&quot; &quot;SQL_TIME&quot; &quot;SQL_TIMESTAMP&quot; &quot;SQL_TINYINT&quot; &quot;SQL_TSI_DAY&quot; &quot;SQL_TSI_FRAC_SECOND&quot; &quot;SQL_TSI_HOUR&quot; &quot;SQL_TSI_MICROSECOND&quot; &quot;SQL_TSI_MINUTE&quot; &quot;SQL_TSI_MONTH&quot; &quot;SQL_TSI_QUARTER&quot; &quot;SQL_TSI_SECOND&quot; &quot;SQL_TSI_WEEK&quot; &quot;SQL_TSI_YEAR&quot; &quot;SQL_VARBINARY&quot; &quot;SQL_VARCHAR&quot; &quot;STATE&quot; &quot;STATEMENT&quot; &quot;STRUCTURE&quot; &quot;STYLE&quot; &quot;SUBCLASS_ORIGIN&quot; &quot;SUBSTITUTE&quot; &quot;TABLE_NAME&quot; &quot;TEMPORARY&quot; &quot;TIES&quot; &quot;TIMESTAMPADD&quot; &quot;TIMESTAMPDIFF&quot; &quot;TOP_LEVEL_COUNT&quot; &quot;TRANSACTION&quot; &quot;TRANSACTIONS_ACTIVE&quot; &quot;TRANSACTIONS_COMMITTED&quot; &quot;TRANSACTIONS_ROLLED_BACK&quot; &quot;TRANSFORM&quot; &quot;TRANSFORMS&quot; &quot;TRIGGER_CATALOG&quot; &quot;TRIGGER_NAME&quot; &quot;TRIGGER_SCHEMA&quot; &quot;TYPE&quot; &quot;UNBOUNDED&quot; &quot;UNCOMMITTED&quot; &quot;UNCONDITIONAL&quot; &quot;UNDER&quot; &quot;UNNAMED&quot; &quot;USAGE&quot; &quot;USER_DEFINED_TYPE_CATALOG&quot; &quot;USER_DEFINED_TYPE_CODE&quot; &quot;USER_DEFINED_TYPE_NAME&quot; &quot;USER_DEFINED_TYPE_SCHEMA&quot; &quot;UTF8&quot; &quot;UTF16&quot; &quot;UTF32&quot; &quot;VERSION&quot; &quot;VIEW&quot; &quot;WEEK&quot; &quot;WRAPPER&quot; &quot;WORK&quot; &quot;WRITE&quot; &quot;XML&quot; &quot;ZONE&quot;, # not in core, added in Flink &quot;PARTITIONED&quot;, &quot;IF&quot;, &quot;ASCENDING&quot;, &quot;FROM_SOURCE&quot;, &quot;BOUNDED&quot;, &quot;DELAY&quot;, &quot;OVERWRITE&quot;, &quot;offset&quot;, &quot;reset&quot; ] # List of methods for parsing custom SQL statements. # Return type of method implementation should be &apos;SqlNode&apos;. # Example: SqlShowDatabases(), SqlShowTables(). # 用于解析自定义SQL语句的方法列表 statementParserMethods: [ &quot;RichSqlInsert()&quot; ] # List of methods for parsing custom literals. # Return type of method implementation should be &quot;SqlNode&quot;. # Example: ParseJsonLiteral(). # 解析自定义文本的方法列表 literalParserMethods: [ ] # List of methods for parsing ddl supported data types. # Return type of method implementation should be &quot;SqlIdentifier&quot;. # Example: SqlParseTimeStampZ(). flinkDataTypeParserMethods: [ &quot;SqlArrayType()&quot;, &quot;SqlMultisetType()&quot;, &quot;SqlMapType()&quot;, &quot;SqlRowType()&quot;, &quot;SqlStringType()&quot;, &quot;SqlBytesType()&quot;, &quot;SqlTimestampType()&quot;, &quot;SqlTimeType()&quot; ] # List of methods for parsing custom data types. # Return type of method implementation should be &quot;SqlIdentifier&quot;. # Example: SqlParseTimeStampZ(). # 解析自定义数据类型的方法列表。 dataTypeParserMethods: [ ] # List of methods for parsing builtin function calls. # Return type of method implementation should be &quot;SqlNode&quot;. # Example: DateFunctionCall(). builtinFunctionCallMethods: [ ] # List of methods for parsing extensions to &quot;ALTER &lt;scope&gt;&quot; calls. # Each must accept arguments &quot;(SqlParserPos pos, String scope)&quot;. # Example: &quot;SqlUploadJarNode&quot; # 解析扩展到&quot;Alter&quot;调用的方法。 # 每个都必须接受参数 &quot;（SqlParserPos pos, String scope）&quot; alterStatementParserMethods: [ ] # List of methods for parsing extensions to &quot;CREATE [OR REPLACE]&quot; calls. # Each must accept arguments &quot;(SqlParserPos pos, boolean replace)&quot;. # 解析扩展以&quot;CREATE [OR REPLACE]&quot;调用的方法列表。 # 每个都必须接受参数 &quot;（SqlParserPos pos, boolean replace）&quot; createStatementParserMethods: [ &quot;SqlCreateTable&quot;, &quot;SqlCreateView&quot;, &quot;SqlCreateFunction&quot; ] # List of methods for parsing extensions to &quot;DROP&quot; calls. # Each must accept arguments &quot;(Span s)&quot;. # 解析扩展到&quot;DROP&quot;调用的方法列表。 # 每个都必须接受参数 &quot;（SqlParserPos pos）&quot; dropStatementParserMethods: [ ] # List of files in @includes directory that have parser method # implementations for parsing custom SQL statements, literals or types # given as part of &quot;statementParserMethods&quot;, &quot;literalParserMethods&quot; or # &quot;dataTypeParserMethods&quot;. # @includes目录中具有解析器方法的文件列表 # 解析自定义SQL语句、文本或类型的实现 # 作为“statementParserMethods”、“literalParserMethods”或“dataTypeParserMethods”的一部分给出。 implementationFiles: [ &quot;parserImpls.ftl&quot; ] # List of additional join types. Each is a method with no arguments. # Example: LeftSemiJoin() # 其他连接类型的列表。每个方法都是没有参数的方法。 joinTypes: [ ] includeCompoundIdentifier: true includeBraces: true includeAdditionalDeclarations: false&#125; 以下结合 Parser.tdd 文件和 calcite-core jar包中打进去的 Parser.jj文件结合说明： packge,class,imports主要是负责导入包，设置编译package目录，编译的类名 1234567PARSER_BEGIN($&#123;parser.class&#125;)package $&#123;parser.package&#125;;&lt;#list parser.imports as importStr&gt;import $&#123;importStr&#125;;&lt;/#list&gt; keywords定义关键字 12345678&lt;DEFAULT, DQID, BTID&gt; TOKEN :&#123;...&lt;#-- additional parser keywords are included here --&gt;&lt;#list parser.keywords as keyword&gt;| &lt; $&#123;keyword&#125;: &quot;$&#123;keyword&#125;&quot; &gt;&lt;/#list&gt;&#125; nonReservedKeywordskeywords定义关键字中,非保留的关键字 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061String NonReservedKeyWord() :&#123;&#125;&#123; ( NonReservedKeyWord0of3() | NonReservedKeyWord1of3() | NonReservedKeyWord2of3() ) &#123; return unquotedIdentifier(); &#125;&#125;/** @see #NonReservedKeyWord */void NonReservedKeyWord0of3() :&#123;&#125;&#123; (&lt;#list parser.nonReservedKeywords as keyword&gt;&lt;#if keyword?index == 0&gt; &lt;$&#123;keyword&#125;&gt;&lt;#elseif keyword?index % 3 == 0&gt; | &lt;$&#123;keyword&#125;&gt;&lt;/#if&gt;&lt;/#list&gt; )&#125;/** @see #NonReservedKeyWord */void NonReservedKeyWord1of3() :&#123;&#125;&#123; (&lt;#list parser.nonReservedKeywords as keyword&gt;&lt;#if keyword?index == 1&gt; &lt;$&#123;keyword&#125;&gt;&lt;#elseif keyword?index % 3 == 1&gt; | &lt;$&#123;keyword&#125;&gt;&lt;/#if&gt;&lt;/#list&gt; )&#125;/** @see #NonReservedKeyWord */void NonReservedKeyWord2of3() :&#123;&#125;&#123; (&lt;#list parser.nonReservedKeywords as keyword&gt;&lt;#if keyword?index == 2&gt; &lt;$&#123;keyword&#125;&gt;&lt;#elseif keyword?index % 3 == 2&gt; | &lt;$&#123;keyword&#125;&gt;&lt;/#if&gt;&lt;/#list&gt; )&#125; joinTypesjoin类型 1234567891011121314151617181920212223242526SqlLiteral JoinType() :&#123; JoinType joinType;&#125;&#123; ( &lt;JOIN&gt; &#123; joinType = JoinType.INNER; &#125; | &lt;INNER&gt; &lt;JOIN&gt; &#123; joinType = JoinType.INNER; &#125; | &lt;LEFT&gt; [ &lt;OUTER&gt; ] &lt;JOIN&gt; &#123; joinType = JoinType.LEFT; &#125; | &lt;RIGHT&gt; [ &lt;OUTER&gt; ] &lt;JOIN&gt; &#123; joinType = JoinType.RIGHT; &#125; | &lt;FULL&gt; [ &lt;OUTER&gt; ] &lt;JOIN&gt; &#123; joinType = JoinType.FULL; &#125; | &lt;CROSS&gt; &lt;JOIN&gt; &#123; joinType = JoinType.CROSS; &#125;&lt;#list parser.joinTypes as method&gt; | joinType = $&#123;method&#125;()&lt;/#list&gt; ) &#123; return joinType.symbol(getPos()); &#125;&#125; statementParserMethods解析自定义SQL语句的方法列表，必须继承SqlNode 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Parses an SQL statement. */SqlNode SqlStmt() :&#123; SqlNode stmt;&#125;&#123; (&lt;#-- Add methods to parse additional statements here --&gt;&lt;#list parser.statementParserMethods as method&gt; LOOKAHEAD(2) stmt = $&#123;method&#125; |&lt;/#list&gt; stmt = SqlSetOption(Span.of(), null) | stmt = SqlAlter() |&lt;#if parser.createStatementParserMethods?size != 0&gt; stmt = SqlCreate() |&lt;/#if&gt;&lt;#if parser.dropStatementParserMethods?size != 0&gt; stmt = SqlDrop() |&lt;/#if&gt; stmt = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY) | stmt = SqlExplain() | stmt = SqlDescribe() | stmt = SqlInsert() | stmt = SqlDelete() | stmt = SqlUpdate() | stmt = SqlMerge() | stmt = SqlProcedureCall() ) &#123; return stmt; &#125;&#125; literalParserMethods解析自定义文字的方法列表，必须实现SqlNode 123456789101112131415161718192021222324252627SqlNode Literal() :&#123; SqlNode e;&#125;&#123; ( e = NumericLiteral() | e = StringLiteral() | e = SpecialLiteral() | e = DateTimeLiteral() | e = IntervalLiteral()&lt;#-- additional literal parser methods are included here --&gt;&lt;#list parser.literalParserMethods as method&gt; | e = $&#123;method&#125;&lt;/#list&gt; ) &#123; return e; &#125;&#125; dataTypeParserMethods解析自定义数据类型的方法列表，必须实现SqlIdentifier 12345678910111213141516171819202122232425262728// Some SQL type names need special handling due to the fact that they have// spaces in them but are not quoted.SqlIdentifier TypeName() :&#123; final SqlTypeName sqlTypeName; final SqlIdentifier typeName; final Span s = Span.of();&#125;&#123; ( LOOKAHEAD(2) sqlTypeName = SqlTypeName(s) &#123; typeName = new SqlIdentifier(sqlTypeName.name(), s.end(this)); &#125;&lt;#-- additional types are included here --&gt;&lt;#list parser.dataTypeParserMethods as method&gt; | typeName = $&#123;method&#125;&lt;/#list&gt; | typeName = CollectionsTypeName() | typeName = CompoundIdentifier() ) &#123; return typeName; &#125;&#125; alterStatementParserMethods解析自定义alter语句，必须有构造方法 (SqlParserPos pos, String scope) 1234567891011121314151617181920212223242526/** * Parses an expression for setting or resetting an option in SQL, such as QUOTED_IDENTIFIERS, * or explain plan level (physical/logical). */SqlAlter SqlAlter() :&#123; final Span s; final String scope; final SqlAlter alterNode;&#125;&#123; &lt;ALTER&gt; &#123; s = span(); &#125; scope = Scope() (&lt;#-- additional literal parser methods are included here --&gt;&lt;#list parser.alterStatementParserMethods as method&gt; alterNode = $&#123;method&#125;(s, scope) |&lt;/#list&gt; alterNode = SqlSetOption(s, scope) ) &#123; return alterNode; &#125;&#125; createStatementParserMethods解析自定义create语句，必须有构造方法 (SqlParserPos pos, String scope) 1234567891011121314151617181920212223242526272829&lt;#if parser.createStatementParserMethods?size != 0&gt;/** * Parses a CREATE statement. */SqlCreate SqlCreate() :&#123; final Span s; boolean replace = false; final SqlCreate create;&#125;&#123; &lt;CREATE&gt; &#123; s = span(); &#125; [ &lt;OR&gt; &lt;REPLACE&gt; &#123; replace = true; &#125; ] (&lt;#-- additional literal parser methods are included here --&gt;&lt;#list parser.createStatementParserMethods as method&gt; create = $&#123;method&#125;(s, replace) &lt;#sep&gt;|&lt;/#sep&gt;&lt;/#list&gt; ) &#123; return create; &#125;&#125;&lt;/#if&gt; dropStatementParserMethods解析自定义drop语句，必须有构造方法(SqlParserPos pos) 123456789101112131415161718192021222324&lt;#if parser.dropStatementParserMethods?size != 0&gt;/** * Parses a DROP statement. */SqlDrop SqlDrop() :&#123; final Span s; boolean replace = false; final SqlDrop drop;&#125;&#123; &lt;DROP&gt; &#123; s = span(); &#125; (&lt;#-- additional literal parser methods are included here --&gt;&lt;#list parser.dropStatementParserMethods as method&gt; drop = $&#123;method&#125;(s, replace) &lt;#sep&gt;|&lt;/#sep&gt;&lt;/#list&gt; ) &#123; return drop; &#125;&#125;&lt;/#if&gt; implementationFiles自定义模板文件 1234&lt;#-- Add implementations of additional parser statement calls here --&gt;&lt;#list parser.implementationFiles as file&gt; &lt;#include &quot;/@includes/&quot;+file /&gt;&lt;/#list&gt; includeCompoundIdentifier是否包含CompoundIdentifier解析 Parser.jj常用方法getPos获取当前token的位置，自定义解析时使用 StringLiteral解析语句中的string类型的字段，带’’ 1select * from table where a=&apos;string&apos;; Identifier解析Identifier字段，返回string类型 1select Identifier from table; 可以有SimpleIdentifier，可以是表名、字段名、别名等或CompoundIdentifier，如要解析kafka.bootstrap.servers这样的复合字符串 以下介绍真正运用在Flink SQL解析的示例: DDLCREATE TABLE自定义SQL语法自定义SQL DDL语句，创建source表、维表、sink表。下面给出几种常见的建表sql示例: 创建kafka source表，带计算列和watermark的定义 123456789101112131415CREATE TABLE table_source( channel varchar, -- 频道 pv int, -- 点击次数 xctime varchar, -- yyyyMMddHHmmss格式时间戳，字符串类型 ts AS TO_TIMESTAMP(xctime,&apos;yyyyMMddHHmmss&apos;), -- rowtime,必须为Timestamp类型或Long类型 WATERMARK FOR ts AS withOffset( ts , &apos;120000&apos; ) --watermark计算方法,允许2分钟的乱序时间,即允许数据迟到2分钟 )WITH( type=&apos;kafka11&apos;, kafka.bootstrap.servers=&apos;mwt:9092&apos;, kafka.auto.offset.reset=&apos;latest&apos;, kafka.kerberos.enabled=&apos;false&apos;, kafka.data.type=&apos;json&apos;, kafka.topic=&apos;table_source&apos;, parallelism=&apos;1&apos; ) 创建mysql维表 1234567891011121314CREATE TABLE table_side( name varchar comment &apos;名称&apos;, -- 允许给字段设置comment info varchar comment &apos;详细信息&apos;, PRIMARY KEY(name), -- 创建维表一定要指定 PRIMARY KEY PERIOD FOR SYSTEM_TIME -- 维表标识 )WITH( type =&apos;mysql&apos;, url =&apos;jdbc:mysql://192.168.1.8:3306/demo?charset=utf8&apos;, userName =&apos;dipper&apos;, password =&apos;ide@123&apos;, tableName =&apos;table_side&apos;, cache =&apos;NONE&apos;, parallelism =&apos;1&apos; ) 创建sink表 12345678910CREATE TABLE table_sink( name varchar, channel varchar, pv int, xctime bigint, info varchar )WITH( type =&apos;console&apos;, parallelism=&apos;1&apos; ); 定义解析结果类 SqlCreateTable.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264package com.matty.flink.sql.parser.ddl;import com.matty.flink.sql.parser.ExtendedSqlNode;import com.matty.flink.sql.parser.exception.SqlParseException;import lombok.Data;import org.apache.calcite.sql.*;import org.apache.calcite.sql.dialect.AnsiSqlDialect;import org.apache.calcite.sql.parser.SqlParserPos;import org.apache.calcite.sql.pretty.SqlPrettyWriter;import org.apache.calcite.util.ImmutableNullableList;import org.apache.calcite.util.NlsString;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;import static java.util.Objects.requireNonNull;/** * Description: * * @author mwt * @version 1.0 * @date 2019-10-08 */@Datapublic class SqlCreateTable extends SqlCreate implements ExtendedSqlNode &#123; public static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator("CREATE TABLE", SqlKind.CREATE_TABLE); private final SqlIdentifier tableName; private final SqlNodeList columnList; private final SqlNodeList propertyList; private final SqlNodeList primaryKeyList; private final SqlCharStringLiteral comment; private final boolean sideFlag; private SqlIdentifier eventTimeField; private SqlNode maxOutOrderless; public SqlCreateTable( SqlParserPos pos, SqlIdentifier tableName, SqlNodeList columnList, SqlNodeList primaryKeyList, SqlNodeList propertyList, SqlCharStringLiteral comment, boolean sideFlag, SqlIdentifier eventTimeField, SqlNode maxOutOrderless) &#123; super(OPERATOR, pos, false, false); this.tableName = requireNonNull(tableName, "Table name is missing"); this.columnList = requireNonNull(columnList, "Column list should not be null"); this.primaryKeyList = primaryKeyList; this.propertyList = propertyList; this.comment = comment; this.sideFlag = sideFlag; this.eventTimeField = eventTimeField; this.maxOutOrderless = maxOutOrderless; &#125; @Override public SqlOperator getOperator() &#123; return OPERATOR; &#125; @Override public List&lt;SqlNode&gt; getOperandList() &#123; return ImmutableNullableList.of(tableName, columnList, primaryKeyList, propertyList, comment); &#125; public Long getMaxOutOrderless() &#123; return Long.parseLong(((NlsString) SqlLiteral.value(maxOutOrderless)).getValue()); &#125; @Override public void validate() throws SqlParseException &#123; Set&lt;String&gt; columnNames = new HashSet&lt;&gt;(); if (columnList != null) &#123; for (SqlNode column : columnList) &#123; String columnName = null; if (column instanceof SqlTableColumn) &#123; SqlTableColumn tableColumn = (SqlTableColumn) column; columnName = tableColumn.getName().getSimple(); if (tableColumn.getAlias() != null) &#123; columnName = tableColumn.getAlias().getSimple(); &#125; String typeName = tableColumn.getType().getTypeName().getSimple(); if (SqlColumnType.getType(typeName).isUnsupported()) &#123; throw new SqlParseException( column.getParserPosition(), "Not support type [" + typeName + "], at " + column.getParserPosition()); &#125; &#125; else if (column instanceof SqlBasicCall) &#123; SqlBasicCall tableColumn = (SqlBasicCall) column; columnName = tableColumn.getOperands()[1].toString(); &#125; if (!columnNames.add(columnName)) &#123; throw new SqlParseException( column.getParserPosition(), "Duplicate column name [" + columnName + "], at " + column.getParserPosition()); &#125; &#125; &#125; if (this.primaryKeyList != null) &#123; for (SqlNode primaryKeyNode : this.primaryKeyList) &#123; String primaryKey = ((SqlIdentifier) primaryKeyNode).getSimple(); if (!columnNames.contains(primaryKey)) &#123; throw new SqlParseException( primaryKeyNode.getParserPosition(), "Primary key [" + primaryKey + "] not defined in columns, at " + primaryKeyNode.getParserPosition()); &#125; &#125; &#125; &#125; public boolean containsComputedColumn() &#123; for (SqlNode column : columnList) &#123; if (column instanceof SqlBasicCall) &#123; return true; &#125; &#125; return false; &#125; /** * Returns the projection format of the DDL columns(including computed columns). * e.g. If we got a DDL: * &lt;pre&gt; * create table tbl1( * col1 int, * col2 varchar, * col3 as to_timestamp(col2) * ) with ( * 'connector' = 'csv' * ) * &lt;/pre&gt; * we would return a query like: * * &lt;p&gt;"col1, col2, to_timestamp(col2) as col3", caution that the "computed column" operands * have been reversed. */ public String getColumnSqlString() &#123; SqlPrettyWriter writer = new SqlPrettyWriter(AnsiSqlDialect.DEFAULT); writer.setAlwaysUseParentheses(true); writer.setSelectListItemsOnSeparateLines(false); writer.setIndentation(0); writer.startList("", ""); for (SqlNode column : columnList) &#123; writer.sep(","); if (column instanceof SqlTableColumn) &#123; SqlTableColumn tableColumn = (SqlTableColumn) column; tableColumn.getName().unparse(writer, 0, 0); &#125; else &#123; column.unparse(writer, 0, 0); &#125; &#125; return writer.toString(); &#125; @Override public void unparse( SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword("CREATE TABLE"); tableName.unparse(writer, leftPrec, rightPrec); SqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.create("sds"), "(", ")"); for (SqlNode column : columnList) &#123; printIndent(writer); if (column instanceof SqlBasicCall) &#123; SqlCall call = (SqlCall) column; SqlCall newCall = call.getOperator().createCall( SqlParserPos.ZERO, call.operand(1), call.operand(0)); newCall.unparse(writer, leftPrec, rightPrec); &#125; else &#123; column.unparse(writer, leftPrec, rightPrec); &#125; &#125; if (primaryKeyList != null &amp;&amp; primaryKeyList.size() &gt; 0) &#123; printIndent(writer); writer.keyword("PRIMARY KEY"); SqlWriter.Frame keyFrame = writer.startList("(", ")"); primaryKeyList.unparse(writer, leftPrec, rightPrec); writer.endList(keyFrame); &#125; if (sideFlag) &#123; printIndent(writer); writer.keyword("PERIOD FOR SYSTEM_TIME"); &#125; if (eventTimeField != null) &#123; printIndent(writer); writer.keyword("WATERMARK FOR "); eventTimeField.unparse(writer, leftPrec, rightPrec); writer.keyword("AS withOffset"); SqlWriter.Frame offsetFrame = writer.startList("(", ")"); eventTimeField.unparse(writer, leftPrec, rightPrec); writer.keyword(","); maxOutOrderless.unparse(writer, leftPrec, rightPrec); writer.endList(offsetFrame); &#125; writer.newlineAndIndent(); writer.endList(frame); if (comment != null) &#123; writer.newlineAndIndent(); writer.keyword("COMMENT"); comment.unparse(writer, leftPrec, rightPrec); &#125; if (propertyList != null) &#123; writer.keyword("WITH"); SqlWriter.Frame withFrame = writer.startList("(", ")"); for (SqlNode property : propertyList) &#123; printIndent(writer); property.unparse(writer, leftPrec, rightPrec); &#125; writer.newlineAndIndent(); writer.endList(withFrame); &#125; &#125; private void printIndent(SqlWriter writer) &#123; writer.sep(",", false); writer.newlineAndIndent(); writer.print(" "); &#125; /** * Table creation context. */ public static class TableCreationContext &#123; public List&lt;SqlNode&gt; columnList = new ArrayList&lt;&gt;(); public SqlNodeList primaryKeyList; public boolean sideFlag; public SqlIdentifier eventTimeField; public SqlNode maxOutOrderless; &#125; public String[] fullTableName() &#123; return tableName.names.toArray(new String[0]); &#125;&#125; SqlTableColumn.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Datapublic class SqlTableColumn extends SqlCall &#123; private static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator("COLUMN_DECL", SqlKind.COLUMN_DECL); private SqlIdentifier name; private SqlDataTypeSpec type; private SqlIdentifier alias; private SqlCharStringLiteral comment; public SqlTableColumn(SqlIdentifier name, SqlDataTypeSpec type, SqlIdentifier alias, SqlCharStringLiteral comment, SqlParserPos pos) &#123; super(pos); this.name = requireNonNull(name, "Column name should not be null"); this.type = requireNonNull(type, "Column type should not be null"); this.alias = alias; this.comment = comment; &#125; @Override public SqlOperator getOperator() &#123; return OPERATOR; &#125; @Override public List&lt;SqlNode&gt; getOperandList() &#123; return ImmutableNullableList.of(name, type, comment); &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; this.name.unparse(writer, leftPrec, rightPrec); writer.print(" "); ExtendedSqlType.unparseType(type, writer, leftPrec, rightPrec); if (this.alias != null) &#123; writer.print(" AS "); this.alias.unparse(writer, leftPrec, rightPrec); &#125; if (this.comment != null) &#123; writer.print(" COMMENT "); this.comment.unparse(writer, leftPrec, rightPrec); &#125; &#125;&#125; javacc语法模板 SqlCreateTable的主体语法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Parse a table creation. */SqlCreate SqlCreateTable(Span s, boolean replace) :&#123; final SqlParserPos startPos = s.pos(); SqlIdentifier tableName; SqlNodeList primaryKeyList = null; SqlNodeList columnList = SqlNodeList.EMPTY; SqlCharStringLiteral comment = null; boolean sideFlag = false; SqlIdentifier eventTimeField = null; SqlNode maxOutOrderless = null; SqlNodeList propertyList = SqlNodeList.EMPTY; SqlParserPos pos = startPos;&#125;&#123; &lt;TABLE&gt; tableName = CompoundIdentifier() [ &lt;LPAREN&gt; &#123; pos = getPos(); TableCreationContext ctx = new TableCreationContext();&#125; TableColumn(ctx) ( &lt;COMMA&gt; TableColumn(ctx) )* &#123; pos = pos.plus(getPos()); columnList = new SqlNodeList(ctx.columnList, pos); primaryKeyList = ctx.primaryKeyList; sideFlag = ctx.sideFlag; eventTimeField = ctx.eventTimeField; maxOutOrderless = ctx.maxOutOrderless; &#125; &lt;RPAREN&gt; ] [ &lt;COMMENT&gt; &lt;QUOTED_STRING&gt; &#123; String p = SqlParserUtil.parseString(token.image); comment = SqlLiteral.createCharString(p, getPos()); &#125;] [ &lt;WITH&gt; propertyList = TableProperties() ] &#123; return new SqlCreateTable(startPos.plus(getPos()), tableName, columnList, primaryKeyList, propertyList, comment, sideFlag, eventTimeField, maxOutOrderless); &#125;&#125; TableColumn将 TableCreationContext 作为参数传递给 TableColumn，解析得到的结果都会设置到 TableCreationContext 类中。分别会解析字段列、主键信息、计算列、watermark 标识、维表标识。 12345678910111213141516171819/** * 获取表字段列 */void TableColumn(TableCreationContext context) :&#123;&#125;&#123; (LOOKAHEAD(3) TableColumn2(context.columnList) | context.primaryKeyList = PrimaryKey() | Watermark(context) | ComputedColumn(context) | context.sideFlag = SideFlag() )&#125; TableColumn2解析建表语句的字段值，类型为自定义的 FlinkSqlDataTypeSpec 1234567891011121314151617181920212223void TableColumn2(List&lt;SqlNode&gt; list) :&#123; SqlParserPos pos; SqlIdentifier name; SqlDataTypeSpec type; SqlIdentifier alias = null; SqlCharStringLiteral comment = null;&#125;&#123; name = SimpleIdentifier() &lt;#-- #FlinkDataType already takes care of the nullable attribute. --&gt; type = FlinkDataType() [ &lt;AS&gt; alias = SimpleIdentifier() ] [ &lt;COMMENT&gt; &lt;QUOTED_STRING&gt; &#123; String p = SqlParserUtil.parseString(token.image); comment = SqlLiteral.createCharString(p, getPos()); &#125;] &#123; SqlTableColumn tableColumn = new SqlTableColumn(name, type , alias ,comment , getPos()); list.add(tableColumn); &#125;&#125; FlinkDataTypeFlink Sql有许多自定义类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/*** Parse a Flink data type with nullable options, NULL -&gt; nullable, NOT NULL -&gt; not nullable.* Default to be nullable.*/SqlDataTypeSpec FlinkDataType() :&#123; final SqlIdentifier typeName; SqlIdentifier collectionTypeName = null; int scale = -1; int precision = -1; String charSetName = null; final Span s; boolean nullable = true; boolean elementNullable = true;&#125;&#123; typeName = FlinkTypeName() &#123; s = span(); &#125; [ &lt;LPAREN&gt; precision = UnsignedIntLiteral() [ &lt;COMMA&gt; scale = UnsignedIntLiteral() ] &lt;RPAREN&gt; ] elementNullable = NullableOpt() [ collectionTypeName = FlinkCollectionsTypeName() nullable = NullableOpt() ] &#123; if (null != collectionTypeName) &#123; return new FlinkSqlDataTypeSpec( collectionTypeName, typeName, precision, scale, charSetName, nullable, elementNullable, s.end(collectionTypeName)); &#125; nullable = elementNullable; return new FlinkSqlDataTypeSpec(typeName, precision, scale, charSetName, null, nullable, elementNullable, s.end(this)); &#125;&#125; PrimaryKey解析主键信息，主键字段可能有多个 12345678910111213141516171819/** * 主键信息 */SqlNodeList PrimaryKey() :&#123; List&lt;SqlNode&gt; pkList = new ArrayList&lt;SqlNode&gt;(); SqlParserPos pos; SqlIdentifier columnName;&#125;&#123; &lt;PRIMARY&gt; &#123; pos = getPos(); &#125; &lt;KEY&gt; &lt;LPAREN&gt; columnName = SimpleIdentifier() &#123; pkList.add(columnName); &#125; (&lt;COMMA&gt; columnName = SimpleIdentifier() &#123; pkList.add(columnName); &#125;)* &lt;RPAREN&gt; &#123; return new SqlNodeList(pkList, pos.plus(getPos())); &#125;&#125; Watermark解析watermark，事件时间窗口需要使用 watermark 12345678910111213141516void Watermark(TableCreationContext context) :&#123; SqlNode identifier; SqlNode expr; boolean hidden = false; SqlParserPos pos; SqlIdentifier eventTimeField; SqlNode maxOutOrderless;&#125;&#123; &lt;WATERMARK&gt; &lt;FOR&gt; eventTimeField = SimpleIdentifier() &lt;AS&gt; &lt;withOffset&gt; &lt;LPAREN&gt; eventTimeField = SimpleIdentifier() &#123; context.eventTimeField = eventTimeField; &#125; &lt;COMMA&gt; maxOutOrderless = StringLiteral() &#123;context.maxOutOrderless = maxOutOrderless; &#125; &lt;RPAREN&gt;&#125; ComputedColumn解析计算列 123456789101112131415void ComputedColumn(TableCreationContext context) :&#123; SqlNode identifier = null; SqlNode expr; boolean hidden = false; SqlParserPos pos;&#125;&#123; identifier = SimpleIdentifier() &#123;pos = getPos();&#125; &lt;AS&gt; expr = Expression(ExprContext.ACCEPT_SUB_QUERY) &#123; expr = SqlStdOperatorTable.AS.createCall(Span.of(identifier, expr).pos(), expr, identifier); context.columnList.add(expr); &#125;&#125; SideFlag解析维表标识，如果为true，说明创建的是维表 12345678910111213/** * 维表标识 */boolean SideFlag() :&#123; SqlParserPos pos; SqlIdentifier columnName;&#125;&#123; &lt;PERIOD&gt; &#123; pos = getPos(); &#125; &lt;FOR&gt; &lt;SYSTEM_TIME&gt; &#123; return true; &#125; | &#123; return false; &#125;&#125; TableProperties解析出WITH参数列表 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Parse a table properties. */SqlNodeList TableProperties():&#123; SqlNode property; final List&lt;SqlNode&gt; proList = new ArrayList&lt;SqlNode&gt;(); final Span span;&#125;&#123; &lt;LPAREN&gt; &#123; span = span(); &#125; [ property = TableOption() &#123; proList.add(property); &#125; ( &lt;COMMA&gt; property = TableOption() &#123; proList.add(property); &#125; )* ] &lt;RPAREN&gt; &#123; return new SqlNodeList(proList, span.end(this)); &#125;&#125;SqlNode TableOption() :&#123; SqlIdentifier key; SqlNode value; SqlParserPos pos;&#125;&#123; key = CompoundIdentifier() &#123; pos = getPos(); &#125; &lt;EQ&gt; value = StringLiteral() &#123; return new SqlTableOption(key, value, getPos()); &#125;&#125; 自定义字段类型 FlinkSqlDataTypeSpec SqlBytesType123456SqlIdentifier SqlBytesType() :&#123;&#125;&#123; &lt;BYTES&gt; &#123; return new SqlBytesType(getPos()); &#125;&#125; 12345678910public class SqlBytesType extends SqlIdentifier implements ExtendedSqlType&#123; public SqlBytesType(SqlParserPos pos) &#123; super("BYTES", pos); &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword("BYTES"); &#125;&#125; SqlStringType123456SqlIdentifier SqlStringType() :&#123;&#125;&#123; &lt;STRING&gt; &#123; return new SqlStringType(getPos()); &#125;&#125; 12345678910public class SqlStringType extends SqlIdentifier implements ExtendedSqlType &#123; public SqlStringType(SqlParserPos pos) &#123; super("STRING", pos); &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword("STRING"); &#125;&#125; SqlArrayType123456789101112131415SqlIdentifier SqlArrayType() :&#123; SqlParserPos pos; SqlDataTypeSpec elementType; boolean nullable = true;&#125;&#123; &lt;ARRAY&gt; &#123; pos = getPos(); &#125; &lt;LT&gt; elementType = FlinkDataType() &lt;GT&gt; &#123; return new SqlArrayType(pos, elementType); &#125;&#125; 1234567891011121314151617181920public class SqlArrayType extends SqlIdentifier implements ExtendedSqlType &#123; private final SqlDataTypeSpec elementType; public SqlArrayType(SqlParserPos pos, SqlDataTypeSpec elementType) &#123; super(SqlTypeName.ARRAY.getName(), pos); this.elementType = elementType; &#125; public SqlDataTypeSpec getElementType() &#123; return elementType; &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword("ARRAY&lt;"); ExtendedSqlType.unparseType(this.elementType, writer, leftPrec, rightPrec); writer.keyword("&gt;"); &#125;&#125; SqlMultisetType123456789101112131415SqlIdentifier SqlMultisetType() :&#123; SqlParserPos pos; SqlDataTypeSpec elementType; boolean nullable = true;&#125;&#123; &lt;MULTISET&gt; &#123; pos = getPos(); &#125; &lt;LT&gt; elementType = FlinkDataType() &lt;GT&gt; &#123; return new SqlMultisetType(pos, elementType); &#125;&#125; 12345678910111213141516171819public class SqlMultisetType extends SqlIdentifier implements ExtendedSqlType&#123; private final SqlDataTypeSpec elementType; public SqlMultisetType(SqlParserPos pos, SqlDataTypeSpec elementType) &#123; super(SqlTypeName.MULTISET.getName(), pos); this.elementType = elementType; &#125; public SqlDataTypeSpec getElementType() &#123; return elementType; &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword("MULTISET&lt;"); ExtendedSqlType.unparseType(this.elementType, writer, leftPrec, rightPrec); writer.keyword("&gt;"); &#125;&#125; SqlMapType1234567891011121314151617SqlIdentifier SqlMapType() :&#123; SqlDataTypeSpec keyType; SqlDataTypeSpec valType; boolean nullable = true;&#125;&#123; &lt;MAP&gt; &lt;LT&gt; keyType = FlinkDataType() &lt;COMMA&gt; valType = FlinkDataType() &lt;GT&gt; &#123; return new SqlMapType(getPos(), keyType, valType); &#125;&#125; 1234567891011121314151617181920212223242526272829public class SqlMapType extends SqlIdentifier implements ExtendedSqlType&#123; private final SqlDataTypeSpec keyType; private final SqlDataTypeSpec valType; public SqlMapType(SqlParserPos pos, SqlDataTypeSpec keyType, SqlDataTypeSpec valType) &#123; super(SqlTypeName.MAP.getName(), pos); this.keyType = keyType; this.valType = valType; &#125; public SqlDataTypeSpec getKeyType() &#123; return keyType; &#125; public SqlDataTypeSpec getValType() &#123; return valType; &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword("MAP"); SqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.FUN_CALL, "&lt;", "&gt;"); writer.sep(","); ExtendedSqlType.unparseType(keyType, writer, leftPrec, rightPrec); writer.sep(","); ExtendedSqlType.unparseType(valType, writer, leftPrec, rightPrec); writer.endList(frame); &#125;&#125; SqlRowType1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/*** Parse Row type, we support both Row(name1 type1, name2 type2) and Row&lt;name1 type1, name2 type2&gt;.* Every item type can have suffix of `NULL` or `NOT NULL` to indicate if this type is nullable.* i.e. Row(f0 int not null, f1 varchar null).*/SqlIdentifier SqlRowType() :&#123; List&lt;SqlIdentifier&gt; fieldNames = new ArrayList&lt;SqlIdentifier&gt;(); List&lt;SqlDataTypeSpec&gt; fieldTypes = new ArrayList&lt;SqlDataTypeSpec&gt;(); List&lt;SqlCharStringLiteral&gt; comments = new ArrayList&lt;SqlCharStringLiteral&gt;();&#125;&#123; &lt;ROW&gt; ( &lt;NE&gt; | &lt;LT&gt; FieldNameTypeCommaList(fieldNames, fieldTypes, comments) &lt;GT&gt; | &lt;LPAREN&gt; FieldNameTypeCommaList(fieldNames, fieldTypes, comments) &lt;RPAREN&gt; ) &#123; return new SqlRowType(getPos(), fieldNames, fieldTypes, comments); &#125;&#125;/*** Parse a &quot;name1 type1 [&apos;i&apos;m a comment&apos;], name2 type2 ...&quot; list.*/void FieldNameTypeCommaList( List&lt;SqlIdentifier&gt; fieldNames, List&lt;SqlDataTypeSpec&gt; fieldTypes, List&lt;SqlCharStringLiteral&gt; comments) :&#123; SqlIdentifier fName; SqlDataTypeSpec fType;&#125;&#123; [ fName = SimpleIdentifier() fType = FlinkDataType() &#123; fieldNames.add(fName); fieldTypes.add(fType); &#125; ( &lt;QUOTED_STRING&gt; &#123; String p = SqlParserUtil.parseString(token.image); comments.add(SqlLiteral.createCharString(p, getPos())); &#125; | &#123; comments.add(null); &#125; ) ] ( &lt;COMMA&gt; fName = SimpleIdentifier() fType = FlinkDataType() &#123; fieldNames.add(fName); fieldTypes.add(fType); &#125; ( &lt;QUOTED_STRING&gt; &#123; String p = SqlParserUtil.parseString(token.image); comments.add(SqlLiteral.createCharString(p, getPos())); &#125; | &#123; comments.add(null); &#125; ) )*&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Datapublic class SqlRowType extends SqlIdentifier implements ExtendedSqlType&#123; private final List&lt;SqlIdentifier&gt; fieldNames; private final List&lt;SqlDataTypeSpec&gt; fieldTypes; private final List&lt;SqlCharStringLiteral&gt; comments; public SqlRowType(SqlParserPos pos, List&lt;SqlIdentifier&gt; fieldNames, List&lt;SqlDataTypeSpec&gt; fieldTypes, List&lt;SqlCharStringLiteral&gt; comments) &#123; super(SqlTypeName.ROW.getName(), pos); this.fieldNames = fieldNames; this.fieldTypes = fieldTypes; this.comments = comments; &#125; public int getArity() &#123; return fieldNames.size(); &#125; public SqlIdentifier getFieldName(int i) &#123; return fieldNames.get(i); &#125; public SqlDataTypeSpec getFieldType(int i) &#123; return fieldTypes.get(i); &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.print("ROW"); if (getFieldNames().size() == 0) &#123; writer.print("&lt;&gt;"); &#125; else &#123; SqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.FUN_CALL, "&lt;", "&gt;"); int i = 0; for (Pair&lt;SqlIdentifier, SqlDataTypeSpec&gt; p : Pair.zip(this.fieldNames, this.fieldTypes)) &#123; writer.sep(",", false); p.left.unparse(writer, 0, 0); ExtendedSqlType.unparseType(p.right, writer, leftPrec, rightPrec); if (comments.get(i) != null) &#123; comments.get(i).unparse(writer, leftPrec, rightPrec); &#125; i += 1; &#125; writer.endList(frame); &#125; &#125;&#125; SqlTimeType123456789101112131415SqlIdentifier SqlTimeType() :&#123; int precision = -1; boolean withLocalTimeZone = false;&#125;&#123; &lt;TIME&gt; ( &lt;LPAREN&gt; precision = UnsignedIntLiteral() &lt;RPAREN&gt; | &#123; precision = -1; &#125; ) withLocalTimeZone = WithLocalTimeZone() &#123; return new SqlTimeType(getPos(), precision, withLocalTimeZone); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class SqlTimeType extends SqlIdentifier implements ExtendedSqlType &#123; private final int precision; private final boolean withLocalTimeZone; public SqlTimeType(SqlParserPos pos, int precision, boolean withLocalTimeZone) &#123; super(getTypeName(withLocalTimeZone), pos); this.precision = precision; this.withLocalTimeZone = withLocalTimeZone; &#125; private static String getTypeName(boolean withLocalTimeZone) &#123; if (withLocalTimeZone) &#123; return SqlTypeName.TIME_WITH_LOCAL_TIME_ZONE.name(); &#125; else &#123; return SqlTypeName.TIME.name(); &#125; &#125; public SqlTypeName getSqlTypeName() &#123; if (withLocalTimeZone) &#123; return SqlTypeName.TIME_WITH_LOCAL_TIME_ZONE; &#125; else &#123; return SqlTypeName.TIME; &#125; &#125; public int getPrecision() &#123; return this.precision; &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword(SqlTypeName.TIME.name()); if (this.precision &gt;= 0) &#123; final SqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.FUN_CALL, "(", ")"); writer.print(precision); writer.endList(frame); &#125; if (this.withLocalTimeZone) &#123; writer.keyword("WITH LOCAL TIME ZONE"); &#125; &#125;&#125; SqlTimestampType123456789101112131415SqlIdentifier SqlTimestampType() :&#123; int precision = -1; boolean withLocalTimeZone = false;&#125;&#123; &lt;TIMESTAMP&gt; ( &lt;LPAREN&gt; precision = UnsignedIntLiteral() &lt;RPAREN&gt; | &#123; precision = -1; &#125; ) withLocalTimeZone = WithLocalTimeZone() &#123; return new SqlTimestampType(getPos(), precision, withLocalTimeZone); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class SqlTimestampType extends SqlIdentifier implements ExtendedSqlType &#123; private final int precision; private final boolean withLocalTimeZone; public SqlTimestampType(SqlParserPos pos, int precision, boolean withLocalTimeZone) &#123; super(getTypeName(withLocalTimeZone), pos); this.precision = precision; this.withLocalTimeZone = withLocalTimeZone; &#125; private static String getTypeName(boolean withLocalTimeZone) &#123; if (withLocalTimeZone) &#123; return SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE.name(); &#125; else &#123; return SqlTypeName.TIMESTAMP.name(); &#125; &#125; public SqlTypeName getSqlTypeName() &#123; if (withLocalTimeZone) &#123; return SqlTypeName.TIMESTAMP_WITH_LOCAL_TIME_ZONE; &#125; else &#123; return SqlTypeName.TIMESTAMP; &#125; &#125; public int getPrecision() &#123; return this.precision; &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.keyword(SqlTypeName.TIMESTAMP.name()); if (this.precision &gt;= 0) &#123; final SqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.FUN_CALL, "(", ")"); writer.print(precision); writer.endList(frame); &#125; if (this.withLocalTimeZone) &#123; writer.keyword("WITH LOCAL TIME ZONE"); &#125; &#125;&#125; 测试使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Testpublic void createKafkaSourceWithComputedColumnForWatermark() &#123; String sql = "CREATE TABLE table_source(\n" + " channel varchar, -- 频道\n" + " pv int, -- 点击次数\n" + " xctime varchar, -- yyyyMMddHHmmss格式时间戳，字符串类型\n" + " ts AS TO_TIMESTAMP(xctime,'yyyyMMddHHmmss'), -- rowtime,必须为TIMESTAMP类型\n" + " WATERMARK FOR ts AS withOffset( ts , '120000' ) --watermark计算方法,允许2分钟的乱序时间,即允许数据迟到2分钟\n" + " )WITH(\n" + " type='kafka11',\n" + " kafka.bootstrap.servers='mwt:9092',\n" + " kafka.auto.offset.reset='latest',\n" + " kafka.kerberos.enabled='false',\n" + " kafka.data.type='json',\n" + " kafka.topic='table_source',\n" + " parallelism='1'\n" + " )\n"; // 创建解析器 SqlParser sqlParser = SqlParser.create(sql, // 解析配置 SqlParser.configBuilder() // 设置解析工厂 .setParserFactory(SqlParserImpl.FACTORY) .setQuoting(Quoting.BACK_TICK) .setUnquotedCasing(Casing.UNCHANGED) .setQuotedCasing(Casing.UNCHANGED) .setConformance(conformance) .build()); final SqlNode sqlNode; try &#123; // 解析sql语句 sqlNode = sqlParser.parseStmt(); &#125; catch (SqlParseException e) &#123; throw new RuntimeException("Error while parsing SQL: " + sql, e); &#125; // 将SqlNode直接转换成相应的解析结果类 assert sqlNode instanceof SqlCreateTable; final SqlCreateTable sqlCreateTable = (SqlCreateTable) sqlNode; SqlIdentifier tableName = sqlCreateTable.getTableName(); LOG.debug("tableName: &#123;&#125;", tableName); SqlNodeList columns = sqlCreateTable.getColumnList(); LOG.debug("columns: &#123;&#125;", columns); // set with properties SqlNodeList propertyList = sqlCreateTable.getPropertyList(); Map&lt;String, String&gt; properties = new HashMap&lt;&gt;(); if (propertyList != null) &#123; propertyList.getList().forEach(p -&gt; properties.put(((SqlTableOption) p).getKeyString().toLowerCase(), ((SqlTableOption) p).getValueString())); &#125; LOG.debug("properties: &#123;&#125;", properties); LOG.debug("eventTimeField:&#123;&#125;", sqlCreateTable.getEventTimeField()); LOG.debug("maxOutOrderless:&#123;&#125;", sqlCreateTable.getMaxOutOrderless()); String nodeInfo = sqlNode.toString(); LOG.debug("test allows secondary parsing: &#123;&#125;", nodeInfo); &#125; CREATE VIEW 和 CREATE FUNCTION 操作类似，具体参考示例代码。 CREATE VIEWSQL语法123456CREATE VIEW table_sink_view AS SELECT a.*,b.info FROM table_source a JOIN table_side b ON a.name=b.name; javacc语法模板123456789101112131415161718192021222324252627/*** Parses a create view or replace existing view statement.* CREATE [OR REPLACE] VIEW view_name [ (field1, field2 ...) ] AS select_statement*/SqlCreate SqlCreateView(Span s, boolean replace) : &#123; SqlIdentifier viewName; SqlCharStringLiteral comment = null; SqlNode query; SqlNodeList fieldList = SqlNodeList.EMPTY;&#125;&#123; &lt;VIEW&gt; viewName = CompoundIdentifier() [ fieldList = ParenthesizedSimpleIdentifierList() ] [ &lt;COMMENT&gt; &lt;QUOTED_STRING&gt; &#123; String p = SqlParserUtil.parseString(token.image); comment = SqlLiteral.createCharString(p, getPos()); &#125; ] &lt;AS&gt; query = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY) &#123; return new SqlCreateView(s.pos(), viewName, fieldList, query, replace, comment); &#125;&#125; CREATE FUNCTIONSQL语法1CREATE FUNCTION StringLengthUdf AS 'com.dtwave.example.flink.udx.udf.StringLengthUdf'; javacc语法模板1234567891011121314151617/** * 创建函数 */SqlCreateFunction SqlCreateFunction(Span s, boolean replace):&#123; SqlParserPos pos; SqlIdentifier functionName; SqlNode className;&#125;&#123; &#123; pos = getPos(); &#125; &lt;FUNCTION&gt; functionName = CompoundIdentifier() &lt;AS&gt; className = StringLiteral() &#123; return new SqlCreateFunction(s.pos(),functionName,className); &#125;&#125; DMLRichInsertSQL语法1234567891011INSERT INTO table_sink SELECT a.name, a.channel, a.pv, a.xctime, StringLengthUdf(a.channel), b.info FROM table_source a JOIN table_side b ON a.name = b.name WHERE a.channel='channel1' AND a.pv&gt;0 定义解析结果类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class RichSqlInsert extends SqlInsert implements ExtendedSqlNode &#123; private final SqlNodeList staticPartitions; private final SqlNodeList extendedKeywords; public RichSqlInsert(SqlParserPos pos, SqlNodeList keywords, SqlNodeList extendedKeywords, SqlNode targetTable, SqlNode source, SqlNodeList columnList, SqlNodeList staticPartitions) &#123; super(pos, keywords, targetTable, source, columnList); this.extendedKeywords = extendedKeywords; this.staticPartitions = staticPartitions; &#125; /** * @return the list of partition key-value pairs, * returns empty if there is no partition specifications. */ public SqlNodeList getStaticPartitions() &#123; return staticPartitions; &#125; /** * Get static partition key value pair as strings. * * &lt;p&gt;Caution that we use &#123;@link SqlLiteral#toString()&#125; to get * the string format of the value literal. If the string format is not * what you need, use &#123;@link #getStaticPartitions()&#125;. * * @return the mapping of column names to values of partition specifications, * returns an empty map if there is no partition specifications. */ public LinkedHashMap&lt;String, String&gt; getStaticPartitionKVs() &#123; LinkedHashMap&lt;String, String&gt; ret = new LinkedHashMap&lt;&gt;(); if (this.staticPartitions.size() == 0) &#123; return ret; &#125; for (SqlNode node : this.staticPartitions.getList()) &#123; SqlProperty sqlProperty = (SqlProperty) node; String value = SqlLiteral.value(sqlProperty.getValue()).toString(); ret.put(sqlProperty.getKey().getSimple(), value); &#125; return ret; &#125; @Override public void unparse(SqlWriter writer, int leftPrec, int rightPrec) &#123; writer.startList(SqlWriter.FrameTypeEnum.SELECT); String insertKeyword = "INSERT INTO"; if (isUpsert()) &#123; insertKeyword = "UPSERT INTO"; &#125; else if (isOverwrite()) &#123; insertKeyword = "INSERT OVERWRITE"; &#125; writer.sep(insertKeyword); final int opLeft = getOperator().getLeftPrec(); final int opRight = getOperator().getRightPrec(); getTargetTable().unparse(writer, opLeft, opRight); if (getTargetColumnList() != null) &#123; getTargetColumnList().unparse(writer, opLeft, opRight); &#125; writer.newlineAndIndent(); if (staticPartitions != null &amp;&amp; staticPartitions.size() &gt; 0) &#123; writer.keyword("PARTITION"); staticPartitions.unparse(writer, opLeft, opRight); writer.newlineAndIndent(); &#125; getSource().unparse(writer, 0, 0); &#125; //~ Tools ------------------------------------------------------------------ public static boolean isUpsert(List&lt;SqlLiteral&gt; keywords) &#123; for (SqlNode keyword : keywords) &#123; SqlInsertKeyword keyword2 = ((SqlLiteral) keyword).symbolValue(SqlInsertKeyword.class); if (keyword2 == SqlInsertKeyword.UPSERT) &#123; return true; &#125; &#125; return false; &#125; /** * Returns whether the insert mode is overwrite (for whole table or for specific partitions). * * @return true if this is overwrite mode */ public boolean isOverwrite() &#123; return getModifierNode(RichSqlInsertKeyword.OVERWRITE) != null; &#125; private SqlNode getModifierNode(RichSqlInsertKeyword modifier) &#123; for (SqlNode keyword : extendedKeywords) &#123; RichSqlInsertKeyword keyword2 = ((SqlLiteral) keyword).symbolValue(RichSqlInsertKeyword.class); if (keyword2 == modifier) &#123; return keyword; &#125; &#125; return null; &#125; @Override public void validate() throws SqlParseException &#123; // no-op &#125;&#125; javacc语法模板12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/*** Parses an INSERT statement.*/SqlNode RichSqlInsert() :&#123; final List&lt;SqlLiteral&gt; keywords = new ArrayList&lt;SqlLiteral&gt;(); final SqlNodeList keywordList; final List&lt;SqlLiteral&gt; extendedKeywords = new ArrayList&lt;SqlLiteral&gt;(); final SqlNodeList extendedKeywordList; SqlNode table; SqlNodeList extendList = null; SqlNode source; final SqlNodeList partitionList = new SqlNodeList(getPos()); SqlNodeList columnList = null; final Span s;&#125;&#123; ( &lt;INSERT&gt; | &lt;UPSERT&gt; &#123; keywords.add(SqlInsertKeyword.UPSERT.symbol(getPos())); &#125; ) ( &lt;INTO&gt; | &lt;OVERWRITE&gt; &#123; if (!((FlinkSqlConformance) this.conformance).allowInsertOverwrite()) &#123; throw new ParseException(&quot;OVERWRITE expression is only allowed for HIVE dialect&quot;); &#125; else if (RichSqlInsert.isUpsert(keywords)) &#123; throw new ParseException(&quot;OVERWRITE expression is only used with INSERT mode&quot;); &#125; extendedKeywords.add(RichSqlInsertKeyword.OVERWRITE.symbol(getPos())); &#125; ) &#123; s = span(); &#125; SqlInsertKeywords(keywords) &#123; keywordList = new SqlNodeList(keywords, s.addAll(keywords).pos()); extendedKeywordList = new SqlNodeList(extendedKeywords, s.addAll(extendedKeywords).pos()); &#125; table = CompoundIdentifier() [ LOOKAHEAD(5) [ &lt;EXTEND&gt; ] extendList = ExtendList() &#123; table = extend(table, extendList); &#125; ] [ LOOKAHEAD(2) &#123; final Pair&lt;SqlNodeList, SqlNodeList&gt; p; &#125; p = ParenthesizedCompoundIdentifierList() &#123; if (p.right.size() &gt; 0) &#123; table = extend(table, p.right); &#125; if (p.left.size() &gt; 0) &#123; columnList = p.left; &#125; &#125; ] source = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY) &#123; return new RichSqlInsert(s.end(source), keywordList, extendedKeywordList, table, source, columnList, partitionList); &#125;&#125; 测试使用生成统一的Operation操作SqlToOperationConverter.java]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[About Scala]]></title>
    <url>%2F2019%2F11%2F02%2FAbout-Scala%2F</url>
    <content type="text"><![CDATA[本文将简单介绍Scala的相关语法。资料来自于菜鸟教程。Scala 是一门多范式的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。Scala 运行在Java虚拟机上，并兼容现有的Java程序。Scala源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库。Kafka、Spark、Flink等主流的大数据组件都支持Scala语言，因此我们有必要了解一下Scala的基本语法。旨在通过本文，了解Scala的基本语法，能在以后做到无障碍阅读使用Scala编写的源码，并能写出简单的Scala应用程序(有些Spark的应用程序必须使用Scala语言，使用Java会有一些不兼容的问题)。直接贴出不同语法的示例代码。 学习Scala语言的demo代码参考：scala-learning 变量1234567891011121314151617181920212223242526272829/** * Description: * * @author mwt * @version 1.0 * @date 2019-11-01 */object VariableDemo &#123; def main(args: Array[String]): Unit = &#123; // 变量 var myVar: String = "Foo" // 常量,不允许修改 val myVal: String = "Foo" // xmax, ymax都声明为100 val xmax, ymax = 100 println("xmax: " + xmax) println("ymax: " + ymax) // 声明一个元组 var pa: (Int, String) = (40, "Foo") println("pa._1: " + pa._1) println("pa._2: " + pa._2) &#125;&#125; 访问修饰符1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * Description: * https://www.runoob.com/scala/scala-access-modifiers.html * * @author mwt * @version 1.0 * @date 2019-11-01 */object AccessModifierDemo &#123; def main(args: Array[String]): Unit = &#123; &#125; class Outer &#123; class Inner &#123; // 私有成员，在类内部访问没问题 private def f(): Unit = &#123; println("f") &#125; class InnerMost &#123; f() &#125; &#125; //错误，因为f在Inner类中声明为了 // (new Inner).f() &#125; class Super &#123; protected def f() &#123; println("f") &#125; class Sub extends Super &#123; f() &#125; class Other &#123; (new Super).f() &#125; &#125; class Outer1 &#123; class Inner &#123; def f() &#123; println("f") &#125; class InnerMost &#123; f() // 正确 &#125; &#125; // 正确因为 f() 是 public, 默认的属性是public (new Inner).f() &#125;&#125; 运算符1234567891011121314151617181920212223242526/** * Description: * https://www.runoob.com/scala/scala-operators.html * * 与java中的运算符类似 * * @author mwt * @version 1.0 * @date 2019-11-01 */object OperatorDemo &#123; def main(args: Array[String]): Unit = &#123; val a = 10 val b = 20 val c = 25 val d = 25 println("a + b = " + (a + b)) println("a - b = " + (a - b)) println("a * b = " + (a * b)) println("b / a = " + (b / a)) println("b % a = " + (b % a)) println("c % a = " + (c % a)) &#125;&#125; 方法和函数123456789101112131415161718192021222324252627/** * Description: * https://www.runoob.com/scala/scala-functions.html * * @author mwt * @version 1.0 * @date 2019-11-01 */object MethodFunctionDemo &#123; def main(args: Array[String]): Unit = &#123; println("Returned Value : " + addInt(5, 7)) printMe() &#125; def addInt(a: Int, b: Int): Int = &#123; var sum: Int = 0 sum = a + b sum &#125; def printMe(): Unit = &#123; println("Hello, Scala!") &#125;&#125; 闭包123456789101112131415161718192021/** * Description: * https://www.runoob.com/scala/scala-closures.html * * @author mwt * @version 1.0 * @date 2019-11-01 */object ClosureDemo &#123; def main(args: Array[String]): Unit = &#123; println("muliplier(1) value = " + multiplier(1)) println("muliplier(2) value = " + multiplier(2)) &#125; var factor = 3 // 闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量 val multiplier = (i: Int) =&gt; i * factor&#125; 字符串1234567891011121314151617181920212223242526272829303132333435/** * Description: * https://www.runoob.com/scala/scala-strings.html * * @author mwt * @version 1.0 * @date 2019-11-01 */object StringDemo &#123; def main(args: Array[String]): Unit = &#123; val greeting: String = "Hello,World!" println(greeting) val buf = new StringBuilder buf += 'a' buf ++= "bcdef" println("buf is : " + buf.toString) var website = "www.runoob.com" var len = website.length println("String Length is : " + len) println("菜鸟教程官网： ".concat("www.runoob.com")) var floatVar = 12.456 var intVar = 2000 var stringVar = "菜鸟教程!" var fs = printf("浮点型变量为 " + "%f, 整型变量为 %d, 字符串为 " + " %s", floatVar, intVar, stringVar) println(fs) &#125;&#125; 数组123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * Description: * https://www.runoob.com/scala/scala-arrays.html * * @author mwt * @version 1.0 * @date 2019-11-01 */object ArrayDemo &#123; def main(args: Array[String]): Unit = &#123; var myList = Array(1.9, 2.9, 3.4, 3.5) // 输出所有数组元素 for (x &lt;- myList) &#123; println(x) &#125; // 计算数组所有元素的总和 var total = 0.0 for (i &lt;- 0 to (myList.length - 1)) &#123; total += myList(i) &#125; println("总和为: " + total) // 查找数组中的最大元素 var max = myList(0) for (i &lt;- 1 to (myList.length - 1)) &#123; if (myList(i) &gt; max) max = myList(i) &#125; println("最大值为: " + max) // 创建矩阵 var myMatrix = ofDim[Int](3, 3) for (i &lt;- 0 to 2) &#123; for (j &lt;- 0 to 2) &#123; myMatrix(i)(j) = j &#125; &#125; // 打印二维阵列 for (i &lt;- 0 to 2) &#123; for (j &lt;- 0 to 2) &#123; print(" " + myMatrix(i)(j)) &#125; println() &#125; // 合并数组 var myList1 = Array(1.9, 2.9, 3.4, 3.5) var myList2 = Array(8.9, 7.9, 0.4, 1.5) var myList3 = concat(myList1, myList2) for (x &lt;- myList3) &#123; println(x) &#125; // 创建区间数组 var myList4 = range(10, 20, 2) var myList5 = range(10, 20) for (x &lt;- myList4) &#123; print(" " + x) &#125; println() for (x &lt;- myList5) &#123; print(" " + x) &#125; &#125;&#125; 集合1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * Description: * https://www.runoob.com/scala/scala-maps.html * * @author mwt * @version 1.0 * @date 2019-11-01 */object MapDemo &#123; def main(args: Array[String]): Unit = &#123; val colors1 = Map("red" -&gt; "#FF0000", "azure" -&gt; "#F0FFFF", "peru" -&gt; "#CD853F") val nums: Map[Int, Int] = Map() // Map基本操作 println("colors 中的键为 : " + colors1.keys) println("colors 中的值为 : " + colors1.values) println("检测 colors 是否为空 : " + colors1.isEmpty) println("检测 nums 是否为空 : " + nums.isEmpty) println(colors1.get("red")) println(colors1("red")) println(colors1.get("yellow")) try &#123; println(colors1("yellow")) &#125; catch &#123; case e: Exception =&gt; println(e) &#125; val colors2 = Map("blue" -&gt; "#0033FF", "yellow" -&gt; "#FFFF00", "red" -&gt; "#FF0000") // Map合并 val colors = colors1 ++ colors2 println("colors1 ++ colors2 : " + colors) colors == colors1.++(colors2) println("colors1.++(colors2) : " + colors) val sites = Map("runoob" -&gt; "http://www.runoob.com", "baidu" -&gt; "http://www.baidu.com", "taobao" -&gt; "http://www.taobao.com") // 输出Map中的key/value键值对 sites.keys.foreach &#123; i =&gt; print("Key = " + i) println(" Value = " + sites(i)) &#125; if (sites.contains("baidu")) &#123; println("baidu 键存在，对应的值为: " + sites("baidu")) &#125; else &#123; println("baidu 键不存在") &#125; &#125;&#125; 迭代器12345678910111213141516171819202122232425262728293031/** * Description: * https://www.runoob.com/scala/scala-iterators.html * * @author mwt * @version 1.0 * @date 2019-11-01 */object IteratorDemo &#123; def main(args: Array[String]): Unit = &#123; val iterator = Iterator("Baidu", "Google", "Runoob", "Taobao") // 最简单的遍历操作 while (iterator.hasNext) &#123; println(iterator.next()) &#125; val ita = Iterator(20,40,2,50,69,90) val itb = Iterator(20,40,2,50,69,90) println("最大元素是: " + ita.max) println("最小元素是: " + itb.min) val itc = Iterator(20,40,2,50,69,90) val itd = Iterator(20,40,2,50,69,90) println("itc.size的值: " + itc.size) println("itd.length的值: " + itd.length) &#125;&#125; 类和对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Description: * https://www.runoob.com/scala/scala-classes-objects.html * * Scala中的类不声明为public，一个Scala源文件中可以有多个类。 * * @author mwt * @version 1.0 * @date 2019-11-01 */object ClassObjectDemo &#123; def main(args: Array[String]): Unit = &#123; val pt = new Point(10, 20) // 移到一个新的位置 pt.move(10, 10) &#125;&#125;class Point(val xc: Int, val yc: Int) &#123; var x: Int = xc var y: Int = yc def move(dx: Int, dy: Int) &#123; x = x + dx y = y + dy println("x 的坐标点: " + x) println("y 的坐标点: " + y) &#125;&#125;/** * 继承类Point */class Location(override val xc: Int, override val yc: Int, val zc :Int) extends Point(xc, yc)&#123; var z: Int = zc def move(dx: Int, dy: Int, dz: Int) &#123; x = x + dx y = y + dy z = z + dz println ("x 的坐标点 : " + x) println ("y 的坐标点 : " + y) println ("z 的坐标点 : " + z) &#125;&#125; 特征1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Description: * https://www.runoob.com/scala/scala-traits.html * * 相当于java的接口，比接口功能还强大，可以定义属性和方法的实现。 * 一般情况下scala的类只能继承单一父类，但是如果Trait就可以继承多个，从结果来看就是实现了多重继承。 * 特征构造器在超类构造器之后、类构造器之前执行，特征由左到右被构造。 * 每个特征当中，父特征先被构造。 * 如果多个特征共有一个父特征，父特征不会被重复构造。 * 所有特征被构造完毕，子类被构造。 * * @author mwt * @version 1.0 * @date 2019-11-02 */object TraitDemo &#123; def main(args: Array[String]): Unit = &#123; val p1 = new Point1(2,3) val p2 = new Point1(2,4) val p3 = new Point1(3,4) println(p1.isEqual(p2)) println(p1.isNotEqual(p3)) &#125;&#125;trait Equal &#123; def isEqual(x: Any): Boolean def isNotEqual(x: Any): Boolean = !isEqual(x)&#125;class Point1(xc: Int, yc: Int) extends Equal &#123; var x: Int = xc var y: Int = yc override def isEqual(obj: Any) = obj.isInstanceOf[Point1] &amp;&amp; obj.asInstanceOf[Point1].x == x&#125; 模式匹配1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Description: * https://www.runoob.com/scala/scala-pattern-matching.html * * scala提供了强大的模式匹配机制，应用也非常广泛。 * 一个模式匹配包含了一系列备选项，每个都开始于关键字case。 * 每个备选项都包含了一个模式及一到多个表达式。 * 符号 =&gt; 隔开了模式和表达式 * * @author mwt * @version 1.0 * @date 2019-11-02 */object PatternMatchingDemo &#123; def main(args: Array[String]): Unit = &#123; println(matchTest("two")) println(matchTest("test")) println(matchTest(1)) println(matchTest(6)) val alice = Person("Alice", 25) val bob = Person("Bob", 32) val charlie = Person("Charlie", 32) for (person &lt;- List(alice, bob, charlie)) &#123; person match &#123; case Person("Alice", 25) =&gt; println("Hi Alice!") case Person("Bob", 32) =&gt; println("Hi Bob!") case Person(name, age) =&gt; println("Age: " + age + " years old, name: " + name) &#125; &#125; &#125; def matchTest(x: Any): Any = x match &#123; case 1 =&gt; "one" case "two" =&gt; 2 case y: Int =&gt; "scala.Int" case _ =&gt; "many" &#125; /** * 使用了case关键字的类定义的就是样例类，样例类是特殊的类，经过优化以用于模式匹配。 * 构造器的每个参数都成为val，除非显示被声明为var。 * 在伴生对象中提供了apply方法，所以可以不使用new关键字就可构建对象。 * 提供unapply方法使模式匹配可以工作。 * 生成toString、equals、hashCode和copy方法，除非显示给出这些方法的定义。 */ case class Person(name: String, age: Int)&#125; 正则表达式1234567891011121314151617181920212223242526/** * Description: * https://www.runoob.com/scala/scala-regular-expressions.html * * @author mwt * @version 1.0 * @date 2019-11-02 */object RegularDemo &#123; def main(args: Array[String]): Unit = &#123; // 使用了String类的r方法构造了一个Regex对象 val pattern = "Scala".r val str = "Scala is Scalable and cool" // 然后使用findFirstIn找到首个匹配项 println(pattern findFirstIn str) // 首字母可以是大写 S 或小写 s val pattern1 = new Regex("(S|s)cala") // 使用逗号连接返回结果 println((pattern1 findAllIn str).mkString(",")) println(pattern replaceFirstIn(str, "Java")) &#125;&#125; 异常处理123456789101112131415161718192021222324252627/** * Description: * * @author mwt * @version 1.0 * @date 2019-11-02 */object ExceptionDemo &#123; def main(args: Array[String]): Unit = &#123; // 捕获异常 try&#123; val f = new FileReader("input.txt") &#125; catch &#123; case ex: FileNotFoundException =&gt; &#123; println("Missing file exception") &#125; case ex: IOException =&gt; &#123; println("IO exception") &#125; &#125; finally &#123; println("Exiting finally...") &#125; &#125;&#125; 提取器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Description: * https://www.runoob.com/scala/scala-extractors.html * * @author mwt * @version 1.0 * @date 2019-11-02 */object ExtractorDemo &#123; def main(args: Array[String]): Unit = &#123; println("Apply 方法：" + apply("Zara", "gmail.com")) println("Unapply 方法：" + unapply("Zara@gmail.com")) println("Unapply 方法：" + unapply("Zara Ali")) val x = ExtractorDemo(5) println(x) x match &#123; case ExtractorDemo(num) =&gt; println(x + "是" + num + " 的两倍! ") //unapply被调用 case _ =&gt; println("无法计算") &#125; &#125; // 注入方法（可选）。通过apply方法我们无需使用new操作就可以创建对象。 def apply(user: String, domain: String) = &#123; user + "@" + domain &#125; // 提取方法（必选）。是apply的反向操作，接收一个对象，然后从对象中提取值，提取的值通常是用来构造该对象的值。 // 实例中从对象中提取用户名和邮件地址的后缀。 def unapply(str: String): Option[(String, String)] = &#123; val parts = str split "@" if (parts.length == 2) &#123; Some(parts(0), parts(1)) &#125; else &#123; None &#125; &#125; def apply(x: Int) = x * 2 def unapply(z: Int): Option[Int] = if (z % 2 == 0) Some(z / 2) else None&#125; 文件I/O1234567891011121314151617181920212223242526272829303132333435import java.io.&#123;File, PrintWriter&#125;import scala.io.&#123;Source, StdIn&#125;/** * Description: * https://www.runoob.com/scala/scala-file-io.html * * scala进行文件读写操作，直接用的是java类中的I/O类 * * @author mwt * @version 1.0 * @date 2019-11-02 */object IODemo &#123; def main(args: Array[String]): Unit = &#123; val writer = new PrintWriter(new File("test.txt")) writer.write("This is an IO demo.") writer.close() // 写完之后 cat test.txt 查看文本内容 println("从屏幕上读取用户输入：") val line = StdIn.readLine() println("输入的内容是：" + line) println("从文件中读取内容：") Source.fromFile("test.txt").foreach( print ) &#125;&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink]]></title>
    <url>%2F2019%2F10%2F19%2FApache-Flink%2F</url>
    <content type="text"><![CDATA[本文将全局介绍 Flink 的整体架构。 系统架构Components JobManager一个 application 对应一个 JobManager，JobManager 接收待执行的 application。application 包含一个 JobGraph 和 JAR （包含所有需要的classes,libraries 和其他资源）。JobManager 将 JobGraph 转成 ExecutionGraph，ExecutionGraph中包含可以并发执行的 tasks。JobManager 向 ResourceManager 申请需要的资源（TaskManager slots），一旦分配到足够的slots，则分发 tasks 到 TaskManager 执行。执行期间，JobManager 负责中央协调，如协调checkpoint等 ResourceManager负责管理 TaskManager slots，当 JobManager 申请 slots 时，ResourceManager 会通知拥有闲置 slots 的 TaskManager 提供 slots 给 JobManager 。 TaskManagerTaskManager 是 Flink 的工作进程，每个 TaskManager 提供一定数量的 slots ，slots的数量限制了 TaskManager 可以执行的 task 数。启动之后，TaskManager 向 ResourceManager 注册 slots 数，当接收到 ResourceManager 的分配通知后，会向 JobManager 提供一个或多个slots，紧接着 JobManager 将 tasks 分配到 slots 执行。执行期间，不同的 TaskManager 之间会进行数据交换 Dispatcher提供 REST 接口供提交任务，一旦 application 被提交执行，就会启动一个 JobManager 。还运行 Web Dashboard。 Task Execution 图左边是JobGraph，包含5个operators，A和C代表source，E代表sink。因为最大并行度为4，所以此 application 至少需要4个 slots。A、B、D的并行度为4，会被分配到每一个slot，C被分配到了Slot1.1和Slot2.1，E被分配到了Slot1.2和Slot2.2。 TaskManager在同一个JVM进程中多线程执行tasks。 高可用实时应用程序通常设计成7*24的运行模式，因此应该保证 application 不停机。当进程出现故障时，第一步要重启失败的进程，第二步是重启 application 并恢复其状态。 TaskManager failures假设一个Flink集群有4个 TaskManagers ，每个 TaskManager 提供2个 slots，所以一个 application 的最大并发为8。当其中一个 TaskManager 故障，可用的 slots 数就降为了6。此时，JobManager 会向 ResourceManager 申请新的 slots。在standalone模式下，将永远申请不到新的 slots，JobManager 也就不能重启 application，直到获得足够的 slots。重启策略决定 JobManager 重启 application 的次数以及重启的间隔。 JobManager failures比起 TaskManager 故障，JobManager 故障是更具有挑战性的问题。JobManager 控制 application 的执行，保存执行相关的元数据，如指向完整 checkpoints 的指针等。如果 JobManager 进程消失，application 将不能再继续执行，因此必须解决 JobManager 的单点故障问题。Flink提供了高可用模式，在当前 JobManager 进程消失时，将 JobManager 负责的功能和元数据传递给另一个 JobManager，依赖于 zookeeper。高可用模式下，JobManager 会将 JobGraph 和其所需的元数据，如 application 的jar包文件，持久化到远程公共存储上，并将存储路径写到 zookeeper 上。执行期间，JobManager会收到各个task checkpoints 的状态处理，当一次 checkpoint 完成，即所有task都将状态持久化到远程存储上了，JobManager 将状态处理持久化到远程存储，并将存储路径写到 zookeeper 上。因此，当 JobManager 故障时，需要结合 zookeeper及远程存储来恢复 application。 当JobManager故障，其所属 application 的所有 tasks 都会自动取消，新的 JobManager 将会接管： 从 Zookeeper 获取远程存储路径，取得 JobGraph 、jar包文件、上一次 checkpoint 的状态处理 从 ResourceManager 申请 slots ，继续执行 application 重启 application , 并根据上一次完整的 checkpoint 重置所有 tasks 的状态 yarn模式下，Flink 尚存的进程会触发故障 JobManager 和 TaskManager 的重启。standalone模式下，Flink 不提供重启失败进程的方法，因此需要运行备用的 JobManager 和 TaskManager 。 数据传输 每个sender task至少需要4个 network buffer，来发送数据到4个 receiver task 。每个 receiver task 至少需要4个 network buffer 来接收数据。不同 TaskManager 之间的数据传输使用相同的网络连接，多路复用。 Credit-Based 流控Flink流控相关博客：Flink网络流控及反压剖析 Task Chaining为了减少本地 tasks 间的通信成本，产生了Task chainning技术。想要chain在一起的operators，必须具有相同的 parallelism ，且由 local forward channel 相连。 Task chaining 可以减少本地本地 tasks 间的通信成本，operators 的 functions 被整合在一个线程中执行，function 产生的结果数据通过方法调用传递给下一个 function。因此，function之间是不产生通信以及序列化反序列化的成本的。 有的时候，需要打断一个很长的 chained tasks 的 pipeline ，将比较耗资源的 opeator 放到单独的 slot 中执行，以不影响其他 operator的执行，operators 的 functions 会运行在不同的线程中。 Watermarks大家都知道，流处理从事件产生，到流经source，再到operator，中间是有一定过程和时间的，虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生。所谓乱序，就是指Flink接收到的事件的先后顺序不是严格按照事件的Event Time顺序排列的。 那么此时出现一个问题，一旦出现乱序，如果只根据 eventTime 决定 window 的运行，我们不能明确数据是否全部到位，但又不能无限期地等待下去，此时必须要有个机制来保证一个特定的时间后，必须要触发 window 去进行计算了，这个特别的机制，就是 watermark。 Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark 。数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据都已经到达了，因此 window 的执行也是由 Watermark 触发的。 Watermarks 有两个基本特征： 单调递增，保证event time时钟不会倒退 与 record 的 timestamp 相关，一个带有时间戳为 T 的 watermark 代表后续到来的 record 的时间戳必须 &gt; T 第二个特征是用来处理 record 乱序迟到问题。如图中的 record 3 和 record 5 为乱序数据，record 4 为迟到数据，因为时间戳 4 &lt; 上一次接收到的 watermark 。后续会介绍 Flink 是如何处理迟到数据的。 当 Flink 接收到每一条数据时，都会产生一条 Watermark ，这条 Watermark 就等于当前所有到达数据中的maxEventTime - 延迟时长（允许迟到的最大时长），也就是说，Watermark 是数据携带的，一旦数据携带的 Watermark 比当前未触发的窗口的停止时间要晚，那么就会触发相应窗口的执行。由于 Watermark 是由数据携带的，因此，如果运行过程中无法获取新的数据，那么没有被触发的窗口将永远都不被触发。 有序流的 Watermark 如下图所示：（Watermark 延迟设置为0） 乱序流的 Watermark 如下图所示：（Watermark 延迟设置为2） 上图中，我们设置的允许最大延迟时间为2s，所以时间戳为7s的事件对应的 Watermark 是5s，时间戳为12s的事件对应的 Watermark 是10s。如果窗口1是1s～5s，窗口2是6s～10s，那么时间戳为7s的事件到达时的 Watermark 恰好触发窗口1，时间戳为12s的事件到达时的 Watermark 恰好触发窗口2。 EventTimeWindow每一条记录来了之后会根据时间属性值采用不同的 window assigner 方法分配给一个或者多个窗口。 当使用 EventTimeWindow 时，所有的 window 都是在 event time 的时间轴上切分的，也就是说，在 window 启动后，会根据初始的 event time 时间每隔一段时间划分一个窗口，如果 window 大小是3s，那么1分钟内会把窗口划分为如下的形式: 1234[00:00:00,00:00:03)[00:00:03,00:00:06)...[00:00:57,00:01:00) 如果 window 大小是10s，那么1分钟内会把窗口划分为如下的形式: 1234[00:00:00,00:00:10)[00:00:10,00:00:20)...[00:00:50,00:01:00) 窗口是左闭右开的。Window 的设定无关数据本身，而是系统定义好了的，也就是说，Window 会一直按照指定的时间间隔进行划分，不论这个 Window 中有没有数据，EventTime 在这个 Window 期间的数据会进入这个 Window。 Window 会不断产生，属于这个 Window 范围的数据会被不断加入到 Window 中，所有未被触发的 Window 都会等待触发，只要 Window 还没触发，属于这个 Window 范围的数据就会一直被加入到 Window 中，直到 Window 被触发才会停止数据的追加，而当 Window 触发之后才接受到的属于被触发 Window 的数据会被丢弃。 Window 会在以下的条件满足时被触发执行： watermark 时间 &gt;= window_end_time 在[window_start_time,window_end_time)中有数据存在 Tumble滚动窗口分配: 一条记录只属于一个窗口 窗口大小是5s，key为A的数据分别在0，4999ms，5000ms产生了数据，那么形成的window如下，窗口允许等待时间为5s: Slide滑动窗口分配: 一条记录属于多个窗口 窗口大小是5s，滑动间隔为1s，key为A的数据分别在0，4999ms，5000ms产生了数据，那么形成的window如下： Session会话窗口分配: 一条记录属于一个窗口 间隔5s中，key为A的数据分别在0，4999ms，5000ms产生了数据，那么形成的window如下： 自定义WatermarkPeriodic Watermark 示例一1234567891011121314151617181920public class BoundedOutOfOrdernessGenerator implements AssignerWithPeriodicWatermarks&lt;MyEvent&gt; &#123; private final long maxOutOfOrderness = 3000; // 3.0 seconds private long currentMaxTimestamp; @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; long timestamp = element.getCreationTime(); currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp); return timestamp; &#125; @Override public Watermark getCurrentWatermark() &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound // 以迄今为止收到的最大时间戳来生成 watermark return new Watermark(currentMaxTimestamp - maxOutOfOrderness); &#125;&#125; 效果解析： 图中是一个10s大小的窗口，10000～20000为一个窗口。当 eventTime 为 23000 的数据到来，生成的 watermark 的时间戳为20000，&gt;= 窗口的结束时间，会触发窗口计算。 示例二123456789101112131415public class TimeLagWatermarkGenerator implements AssignerWithPeriodicWatermarks&lt;MyEvent&gt; &#123; private final long maxTimeLag = 3000; // 3 seconds @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; return element.getCreationTime(); &#125; @Override public Watermark getCurrentWatermark() &#123; // return the watermark as current time minus the maximum time lag return new Watermark(System.currentTimeMillis() - maxTimeLag); &#125;&#125; 效果解析： 只是简单的用当前系统时间减去最大延迟时间生成 Watermark ，当 Watermark 为 20000时，&gt;= 窗口的结束时间，会触发10000～20000窗口计算。再当 eventTime 为 19500 的数据到来，它本应该是属于窗口 10000～20000窗口的，但这个窗口已经触发计算了，所以此数据会被丢弃。 示例31234567891011121314151617181920212223242526272829303132333435public class TumblingEventWindowExample &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStream&lt;String&gt; socketStream = env.socketTextStream("localhost", 9999); DataStream&lt;Tuple2&lt;String, Long&gt;&gt; resultStream = socketStream // Time.seconds(3)有序的情况修改为0 .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(3)) &#123; @Override public long extractTimestamp(String element) &#123; long eventTime = Long.parseLong(element.split(" ")[0]); System.out.println(eventTime); return eventTime; &#125; &#125;) .map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; map(String value) throws Exception &#123; return Tuple2.of(value.split(" ")[1], 1L); &#125; &#125;) .keyBy(0) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception &#123; return new Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1); &#125; &#125;); resultStream.print(); env.execute(); &#125;&#125; 运行程序之前，在本地启动命令行监听: 1nc -l 9999 有序的情况下，watermark延迟时间为01234567891011121314miaowenting@miaowentingdeMacBook-Pro flink$ nc -l 999910000 a11000 a12000 b13000 b14000 a19888 a13000 a20000 a 时间戳20000触发第一个窗口计算，实际上19999也会触发，因为左闭右开的原则，20000这个时间戳并不会在第一个窗口计算，第一个窗口是[10000-20000)，第二个窗口是[20000-30000)，以此类推11000 a12000 b21000 b22000 a29999 a 第一个窗口触发计算后，后续来的11000，12000这两条数据被抛弃，29999直接触发窗口计算，并且本身也属于第二个窗口，所以也参与计算了。 无序的情况下，watermark延迟时间为31234567891011121314miaowenting@miaowentingdeMacBook-Pro flink$ nc -l 999910000 a11000 a12000 b20000 a 从数据中可以验证，第一个窗口在20000的时候没有触发计算21000 a22000 b23000 a 在23000的时候触发计算，计算内容是第一个窗口[10000-20000)，所以20000，21000，22000，23000属于第二个窗口，没有参与计算。24000 a29000 b30000 a22000 a23000 a33000 a 第二个窗口[20000-30000)，它是在33000触发计算，并且，迟到的数据22000，23000也被计算在内（如果这个数据在水印33000后到达，则会被抛弃），30000和33000是第三个窗口的数据，没有计算 Watermark传播Tasks 内部有一个 time services，维护 timers ，当接收到 watermark 时触发。例如，一个窗口 operator 为每一个活跃窗口在 time servive 注册一个 timer，当event time大于窗口结束时间时，清除窗口状态。 当 task 接收到 watermark 后，会执行以下操作： task 根据 watermark 的时间戳，更新内部的 event_time clock。 time service 区分出所有时间戳小于更新之后的 event_time 的 timers，对超时的 timer，task 执行回调函数触发计算并发射数据。 task 发射 watermark，时间戳为更新之后的 event_time。 Flink 会将数据流分成多个 partition，task 维护每个 partition 对应的 watermark。每个 partition 分别将 watermark 更新成接收到的最大值。task 将 event_time clock 更新成所有 partition 的 watermark 的最小值。如果 clock 向前走动，task 处理所有达到触发时间的 timers，并向下游 partitions 传递 watermark 。 状态管理 task 接收输入数据，结合 state 作处理。例如，统计接收到记录的数量，接收到一个 record，会向 state 查询当前的 count 大小，此 count + 1，向 state 更新 count 值，向下发射新的 count 值。 state 总是和一个具体的 operator 相关联的。为了让 Flink 运行时感知到 operator 的状态，operator 需要注册它的状态。有两种类型的 state ，分别为 operator state 和 keyed state 。 Operator Stateoperator state 是 operator tasks 共享的。 List state 代表state是一个list Union list state 不同于List state，它是在故障的时候或者当一个应用从savepoint启动时被存储。 Broadcast state 当做checkpoints时或者扩展operator并行度时被使用。 Keyed State Value state 为每一个key存储任意类型的单值value ，复杂类型的数据结构也可以被存储在 value state中。 List state 为每一个key存储一个value的list，list中的value可以是任意类型。 Map state 为每一个key存储一个k-v的map，map的key、value可以是任意类型。 State Backends负责两件事情： 管理本地状态 向远程存储做checkpointing Scaling Stateful Operators流式应用通常要调整并行度，状态也需要被重分区到新的task。Flink 将key组织成key group，分配到task上。 list state 会被按比例平均分配到新的 tasks。如果 list 中的 entries 小于 operator 的并发度，那么有些 task 将以 empty state 启动。 union list state 将被广播到新的 tasks，task 自己选择使用或丢弃其中的状态。 broadcast state 将被复制到新的 tasks， 确保所有的tasks 拥有相同的状态。并发度变小的情况下，多余的 task 直接取消即可。 Checkpoints，Savepoints 和状态恢复需要处理进程被杀、机器故障、网络连接故障等问题。tasks 本地维护各自的状态，Flink 必须保证状态不丢失，并且在发生故障时可以持久化这些状态。 持久化CheckpointsFlink 的恢复机制是基于应用状态做持久化 checkpoints。持久化 checkpoint 是在某一个时间点复制所有 tasks 的 state，此时所有的tasks都处理了相同的输入数据。这个过程可以描述为 navie 算法，步骤如下： 暂停所有输入流的摄入 等待所有 in-flight 数据被完全处理，意味着所有 tasks 都处理了 input 数据 做checkpoint，复制所有tasks的state到远程的持久化存储，当所有task复制完成，此次checkpoint也就完成 恢复所有流摄入 输入的是一组数字流，数字分奇数和偶数统计和。source task记录消费的offset作为其state，sum tasks记录奇数和与偶数和作为其state。图中记录了input offset为5，统计值为6和9。 从Checkpoint恢复发生故障时，Flink会从最后一次记录的checkpoint处恢复应用的状态并重启。 持久化 input offset 为5，偶数和为6，奇数和为9。 应用恢复的步骤如下： 重启整个应用。 根据上次完整的checkpoint重置状态。 恢复tasks的所有相关进程。 Checkpointing算法Chandy-Lamport算法。这个算法不会暂停整个应用去做 checkpoint ，解耦checkpointing操作，一些tasks接着处理数据，另一些tasks在持久化它们的状态。Flink 的 checkpointing 算法使用了一种特殊的数据类型，叫做 checkpoint barrier。类似于 watermark，checkpoint barrier（checkpoint ID） 也随着数据流动。checkpoint ID 标识 checkpoint barrier 属于那一次 checkpoint 操作，逻辑上讲数据流分成两部分。所有在 checkpoint barrier 之前的数据状态改变，都属于本次 checkpoint 操作，之后的数据状态改变就属于下一次 checkpoint 操作了。 一次 checkpoint 操作是通过 JobManager 向每一个 source task 发送一条带 checkpoint ID 的消息发起的。 当 source task 接收到上述消息，便会停止发射数据，在 state backend 做本地状态的 checkpoint ，并向所有的下游分区广播发送 checkpoint barrier。持久化完成之后会向 JobManager 发送确认消息，当所有的 checkpoint barrier 发送完毕之后，source task 会继续进行逻辑处理。 当一个下游 task 接收到 checkpoint barrier，会等待上游所有 input partitions 的 checkpoint barrier 到来。等待期间，会继续处理那些还没有发送 checkpoint barrier 的 input partition 的数据，而已经接收到 checkpoint barrier 的分区数据将不能再被处理，而是会被缓存起来。这一过程叫做 barrier alignment。 当 task 接收到所有上游的 input partitions 的 checkpoint barrier，便发起本 task 的 checkpoint 并向下游所有相连的 tasks 发送 checkpoint barrier。 一旦所有的 checkpoint barrier 都发送完，task 开始处理之前缓存的数据 4 。 最终，checkpoint barriers 流到了 sink task。当 sink task 接收到 checkpint barrier ，会等待 barrier alignment，做 checkpoint ，并向 JobManager 汇报。JobManager 接收到所有 tasks 的 acknowledgement 之后，便标识本次 checkpoint 成功。 Checkpoint的性能影响SavepointsFlink的恢复算法是基于状态的 checkpoints ，checkpoints 定期产生并根据一定的策略丢弃，一般用于发生故障时应用重启。但是除此之外，持久化快照还有其他用途。Flink中最有价值之一且独一无二的特性就是 savepoints ，savepoints 和 checkpoints 使用相同的算法，因此是在 checkpoints 的基础上额外添加了一些 metadata 。Flink不会自动生成 savepoints ，用户必须额外明确触发 savepoints 的生成。Flink 也不会自动清除 savepoints 。 使用Savepoints给定一个应用和一个与其兼容的 savepoint，可以从此 savepoint 启动应用。根据 savepoint 中记录的 state 初始化应用的 state。这整个操作类似于应用故障时从 checkpoint 恢复的过程，但是应用故障仅仅是其中一种场景，限制在相同的集群上使用相同的配置重启相同的应用。而从 savepoint 启动应用就没有那么多限制了，可以做如下操作： 从 savepoint 启动一个不同但是兼容的应用。这种情况下，可以改动代码逻辑的bug，只要做到 application 和 savepoint 兼容即可，即 application 代码必须要能加载 savepoint 中的状态。 可以修改 application 的并行度再启动。 可以在不同的集群上启动相同的 application ，这就允许迁移 flink 应用到新的 flink 版本，或者不同的集群，或者是不同的数据中心。 可以使用 savepoint 暂停应用，恢复应用。这就可以做到输入端没有数据流入时，当前应用可以释放资源给优先级比较高的应用，最大化利用集群资源。 使用 savepoint 作为 application 的版本管理，记录每次版本迭代。 基于以上 savepoint 的强大功能，许多用户会定期生成 savepoints，以便能及时回滚。也可以持续不断的将流应用迁移到数据中心。 从Savepoint启动applicationFlink 是如何从 savepoint 中初始化 state 的？ 一个 application 由多个 operators 组成，一个 operator 可以定义一个或多个 keyed states 和 operator states，Opeators 被多个 Operator tasks 并发执行。因此一个典型的应用下，状态是分布到不同的 Operator tasks，在不同的 TaskManager 进程中执行的。 以下 application 有3个Operators，OP-1 只有Operator state（OS-1），OP-2 有2个keyed states (KS-1, KS-2)。 问题说说Flink中的checkpoint？从checkpoint的恢复流程？一文搞懂Flink的Exactly Once和At Least Once Flink Checkpoint问题排查实用指南 Apache Flink管理大型状态之增量Checkpoint详解 谈谈流计算中的Exactly Once特性 Flink中的反压是怎么实现的？如何分析及处理Flink反压 共享内存 Flink网络协议栈深入了解Flink的网络协议栈 Flink中State管理State Processor API：如何读取，写入和修改Flink应用程序的状态 先在DataSet API上构建该功能，并将其对DataSet API的依赖性降到最低。 Flink state有可能代替数据库吗？ Flink的内存管理Flink原理与实现：内存管理 Flink中join的实现Flink SQL如何实现数据流的Join？ Flink DataStream关联维表实战 离线 Batch SQL join 的实现传统的离线 Batch SQL （面向有界数据集的 SQL）有三种基础的实现方式，分别是 Nested-loop Join、Sort-Merge Join 和 Hash Join。 Nested-loop Join 最为简单直接，将两个数据集加载到内存，并用内嵌遍历的方式来逐个比较两个数据集内的元素是否符合 Join 条件。Nested-loop Join 虽然时间效率以及空间效率都是最低的，但胜在比较灵活适用范围广，因此其变体 BNL 常被传统数据库用作为 Join 的默认基础选项。 Sort-Merge Join 顾名思义，分为两个 Sort 和 Merge 阶段。首先将两个数据集进行分别排序，然后对两个有序数据集分别进行遍历和匹配，类似于归并排序的合并。值得注意的是，Sort-Merge 只适用于 Equi-Join（Join 条件均使用等于作为比较算子）。Sort-Merge Join 要求对两个数据集进行排序，成本很高，通常作为输入本就是有序数据集的情况下的优化方案。 Hash Join 同样分为两个阶段，首先将一个数据集转换为 Hash Table，然后遍历另外一个数据集元素并与 Hash Table 内的元素进行匹配。第一阶段和第一个数据集分别称为 build 阶段和 build table，第二个阶段和第二个数据集分别称为 probe 阶段和 probe table。Hash Join 效率较高但对空间要求较大，通常是作为 Join 其中一个表为适合放入内存的小表的情况下的优化方案。和 Sort-Merge Join 类似，Hash Join 也只适用于 Equi-Join。 实时领域 Streaming SQL 中的 Join 与离线 Batch SQL 中的 Join 最大不同点在于无法缓存完整数据集，而是要给缓存设定基于时间的清理条件以限制 Join 涉及的数据范围。根据清理策略的不同，Flink SQL 分别提供了 Regular Join、Time-Windowed Join 和 Temporal Table Join 来应对不同业务场景。 实时Streaming SQL join相对于离线的 Join，实时 Streaming SQL（面向无界数据集的 SQL）无法缓存所有数据，因此 Sort-Merge Join 要求的对数据集进行排序基本是无法做到的，而 Nested-loop Join 和 Hash Join 经过一定的改良则可以满足实时 SQL 的要求。 Table A 有 1、42 两个元素，Table B 有 42 一个元素，所以此时的 Join 结果会输出 42： 接着 Table B 依次接受到三个新的元素，分别是 7、3、1。因为 1 匹配到 Table A 的元素，因此结果表再输出一个元素 1： 随后 Table A 出现新的输入 2、3、6，3 匹配到 Table B 的元素，因此再输出 3 到结果表。 可以看到在 Nested-Loop Join 中我们需要保存两个输入表的内容，而随着时间的增长 Table A 和 Table B 需要保存的历史数据无止境地增长，导致很不合理的内存磁盘资源占用，而且单个元素的匹配效率也会越来越低。类似的问题也存在于 Hash Join 中。 双流join的实现双流join与传统数据库表join的区别： 左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入。 JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果。 查询计算的双边驱动，双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。 数据Shuffle：分布式流计算所有数据会进行Shuffle，怎么才能保障左右两边流的要JOIN的数据会在相同的节点进行处理呢？在双流JOIN的场景，我们会利用JOIN中ON的联接key进行partition，确保两个流相同的联接key会在同一个节点处理。 数据的保存：不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储，左右两边的数据到来时候，进行如下操作： LeftEvent到来存储到LState，RightEvent到来的时候存储到RState；LeftEvent会去RightState进行JOIN，并发出所有JOIN之后的Event到下游；RightEvent会去LeftState进行JOIN，并发出所有JOIN之后的Event到下游。 Flink SQL的joinRegular joinRegular Join 是最为基础的没有缓存剔除策略的 Join。Regular Join 中两个表的输入和更新都会对全局可见，影响之后所有的 Join 结果。举例，在一个如下的 Join 查询里，Orders 表的新纪录会和 Product 表所有历史纪录以及未来的纪录进行匹配。 123SELECT * FROM OrdersINNER JOIN ProductON Orders.productId = Product.id 因为历史数据不会被清理，所以 Regular Join 允许对输入表进行任意种类的更新操作（insert、update、delete）。然而因为资源问题 Regular Join 通常是不可持续的，一般只用做有界数据流的 Join。 Time-windowed joinTime-Windowed Join 利用窗口的给两个输入表设定一个 Join 的时间界限，超出时间范围的数据则对 JOIN 不可见并可以被清理掉。值得注意的是，这里涉及到的一个问题是时间的语义，时间可以是指计算发生的系统时间（即 Processing Time），也可以是指从数据本身的时间字段提取的 Event Time。如果是 Processing Time，Flink 根据系统时间自动划分 Join 的时间窗口并定时清理数据；如果是 Event Time，Flink 分配 Event Time 窗口并依据 Watermark 来清理数据。 1234567SELECT *FROM Orders o, Shipments sWHERE o.id = s.orderId AND s.shiptime BETWEEN o.ordertime AND o.ordertime + INTERVAL '4' HOUR 这个查询会为 Orders 表设置了 o.ordertime &gt; s.shiptime- INTERVAL ‘4’HOUR 的时间下界： 并为 Shipmenets 表设置了 s.shiptime &gt;= o.ordertime 的时间下界： 因此两个输入表都只需要缓存在时间下界以上的数据，将空间占用维持在合理的范围。 Temporal Table join然 Timed-Windowed Join 解决了资源问题，但也限制了使用场景: Join 两个输入流都必须有时间下界，超过之后则不可访问。这对于很多 Join 维表的业务来说是不适用的，因为很多情况下维表并没有时间界限。针对这个问题，Flink 提供了 Temporal Table Join 来满足用户需求。 Temporal Table Join 类似于 Hash Join，将输入分为 Build Table 和 Probe Table。前者一般是纬度表的 changelog，后者一般是业务数据流，典型情况下后者的数据量应该远大于前者。在 Temporal Table Join 中，Build Table 是一个基于 append-only 数据流的带时间版本的视图，所以又称为 Temporal Table。Temporal Table 要求定义一个主键和用于版本化的字段（通常就是 Event Time 时间字段），以反映记录内容在不同时间的内容。 比如典型的一个例子是对商业订单金额进行汇率转换。假设有一个 Oders 流记录订单金额，需要和 RatesHistory 汇率流进行 Join。RatesHistory 代表不同货币转为日元的汇率，每当汇率有变化时就会有一条更新记录。 我们将 RatesHistory 注册为一个名为 Rates 的 Temporal Table，设定主键为 currency，版本字段为 time： 此后给 Rates 指定时间版本，Rates 则会基于 RatesHistory 来计算符合时间版本的汇率转换内容： 1234567SELECT o.amount * r.rateFROM Orders o, LATERAL Table(Rates(o.time)) rWHERE o.currency = r.currency 值得注意的是，不同于在 Regular Join 和 Time-Windowed Join 中两个表是平等的，任意一个表的新记录都可以与另一表的历史记录进行匹配，在 Temporal Table Join 中，Temoparal Table 的更新对另一表在该时间节点以前的记录是不可见的。这意味着我们只需要保存 Build Side 的记录直到 Watermark 超过记录的版本字段。因为 Probe Side 的输入理论上不会再有早于 Watermark 的记录，这些版本的数据可以安全地被清理掉。 inner join的实现的示例最直接的两个进行INNER JOIN，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行INNER JOIN，JION条件是产品ID。 双流JOIN两边事件都会存储到State里面，如上，事件流按照标号先后流入到join节点，我们假设右边流比较快，先流入了3个事件，3个事件会存储到state中，但因为左边还没有数据，所有右边前3个事件流入时候，没有join结果流出，当左边第一个事件序号为4的流入时候，先存储左边state，再与右边已经流入的3个事件进行join，join的结果如图 三行结果会流入到下游节点sink。当第5号事件流入时候，也会和左边第4号事件进行join，流出一条jion结果到下游节点。这里关于INNER JOIN的语义和大家强调两点： INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事件；INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中。 left outer join的实现LEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL。同样我们以最简单的场景说明LEFT JOIN的实现，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行LEFT JOIN，JION条件是产品ID。 上图主要关注点是当左边先流入1，2事件时候，右边没有可以join的事件时候会向下游发送左边事件并补NULL向下游发出，当右边第一个相同的Join key到来的时候会将左边先来的事件发出的带有NULL的事件撤回（对应上面command的-记录，+代表正向记录，-代表撤回记录）。这里强调三点： 左流的事件当右边没有JOIN的事件时候，将右边事件列补NULL后流向下游；* 当右边事件流入发现左边已经有可以JOIN的key的时候，并且是第一个可以JOIN上的右边事件（比如上面的3事件是第一个可以和左边JOIN key P001进行JOIN的事件）需要撤回左边下发的NULL记录，并下发JOIN完整（带有右边事件列）的事件到下游。后续来的4，5，6，8等待后续P001的事件是不会产生撤回记录的。在Apache Flink系统内部事件类型分为正向事件标记为“+”和撤回事件标记为“-”。 多级LEFT JOIN会让流上的数据不断膨胀，造成JOIN节点性能较慢，JOIN之后的下游节点边堵(数据量大导致，非热点)。 双流join的state数据结构数据结构：Map&lt;JoinKey,Map&lt;rowDara,count&gt;&gt; 第一级MAP的key是Join key，比如示例中的P001， value是流上面的所有完整事件;第二级MAP的key是行数据，比如示例中的P001, 2，value是相同事件值的个数。 记录重复记录 - 利用第二级MAP的value记录重复记录的个数，这样大大减少存储和读取正向记录和撤回记录 - 利用第二级MAP的value记录，当count=0时候删除该元素判断右边是否产生撤回记录 - 根据第一级MAP的value的size来判断是否产生撤回，只有size由0变成1的时候（第一条和左可以JOIN的事件）才产生撤回。 双流join的应用优化NULL造成的热点假设在实际业务中有这样的特点，大部分时候当A事件流入的时候，B还没有可以JOIN的数据，但是B来的时候，A已经有可以JOIN的数据了，这特点就会导致，A LEFT JOIN B 会产生大量的 (A, NULL),其中包括B里面的 cCol 列也是NULL，这时候当与C进行LEFT JOIN的时候，首先Blink内部会利用cCol对AB的JOIN产生的事件流进行Shuffle， cCol是NULL进而是下游节点大量的NULL事件流入，造成热点。 通过改变join的先后顺序，来保证A LEFT JOIN B后不会产生NULL热点问题： join reorder之后的结果，可以一定程度消除state存储的性能瓶颈：]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink网络流控及反压剖析]]></title>
    <url>%2F2019%2F09%2F22%2FFlink%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%8F%8A%E5%8F%8D%E5%8E%8B%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[网络流控是为了在上下游速度不匹配的情况下，如何防止下游出现过载的手段网络流控有静态限速和动态反压两种手段Flink 1.5以前是基于TCP流控+bounded buffer来实现反压Flink 1.5之后实现了自己托管的credit-based流控机制，在应用层模拟TCP流控的机制 本文将对以上几点逐一介绍，并介绍几种常见的反压问题的处理方式。 网络流控的概念与背景为什么需要网络流控 5s后，将面临以下两种情况之一： bounded receive buffer： consumer丢弃新到达的数据。 unbounded receive buffer：buffer持续扩张，耗尽consumer的内存。 静态限速 静态限流有两种限制： 通常无法预估consumer端能承受的最大速率。 consumer承受能力通常会动态波动。 guava包中基于令牌桶实现的静态限速动态反馈/自动反压 动态反馈氛围两种，广义上的反压机制都涵盖： 负反馈：接收速率小于发送速率时发生。 正反馈：接收速率大于发送速率时发生。 Storm反压 每个bolt中都有检测 backpressure 的线程，此线程检测到 bolt 中的阻塞队列有严重的阻塞情况。 将反压信息 feedback 写入 zookeeper。 zookeeper 被 spout 监听，监听到阻塞 feedback，就会停止发送。 Spark Streaming反压 从中间的 buffer 或者每个处理节点中实时收集一些动态指标，将速度控制的情况返回到接收端，将接收端的速度也降下来。 Flink网络传输的数据流向为什么Flink（before V1.5）里没有类似的feedback机制？因为TCP天然具备feedback流控机制，Flink基于它实现反压。 每个 operator 经过自己的Network stack，底层使用Netty通信： TCP流控机制TCP Packet TCP流控：滑动窗口 定义生产者速度为 3packets/s，消费者速度为 1packet/s。 发送窗口初始大小为3，接收窗口初始大小为5。 生产者端向消费者端发送数据&lt;1,2,3&gt;。 用户态消费者消费了数据&lt;1&gt;，消费者端的滑动窗口向前滑动 1 格。并向生产者端发送ACK = 4，以及自己的窗口内的剩余缓冲区大小 3。发送窗口向前滑动 3 格。 生产者端向消费者端发送数据&lt;4,5,6&gt;。 用户态消费者消费了数据&lt;2&gt;，消费者端的滑动窗口向前滑动 1 格。并向生产者端发送ACK = 7，以及自己的窗口内的剩余缓冲区大小 1。发送窗口向前滑动 1 格。发送端此时已被节流，速度降为 1packet/s。 生产者端向消费者端发送数据&lt;7&gt;。 用户态消费者此时停止消费，向生产者端发送ACK = 8，以及自己的窗口内的剩余缓冲区大小 0。发送端速度降为 0packet/s。 生产者端向消费者端发送轮询探针，如果消费者端缓冲区有空间，将会应答window大小。 用户态消费者恢复消费，消费了数据&lt;3&gt;，消费者端的滑动窗口向前滑动 1 格。接收到了生产者端的probe。 向生产者端发送ACK = 8，以及自己的窗口内的剩余缓冲区大小 1。生产者端的生产速度恢复为 1packet/s。 Flink TCP-based反压机制（before V1.5）Flink代码中的网络模块抽象示例：WindowWordCount1234567891011121314public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; dataStream = env .socketTextStream("localhost",9999) .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); dataStream.print(); env.execute("Window WordCount");&#125; 编译阶段：生成JobGraphClient端通过 StreamGraph 生成 JobGraph： JobGraph 是向集群提交的最基本单元，对任务做了部分优化，合并了相应节点，因为这些节点之间没有任何的shuffle。 调度阶段：调度ExecutionGraph在 JobManager 中创建 ExecutionGraph： 已经具备了执行的雏形，每个任务都拆解出了subtask。Execution Edge 之前会有一个发送数据的模块，Intermediate Result Partition。将 ExecutionGraph 交给 JobManager 的调度器，调度器负责把整个 ExecutionGraph 给调度起来，形成虚拟的物理执行图。每个 subtask 之前都有统一接收数据的 InputGate ，ResultPartition 可以认为是发送数据，二者是对应的关系。ResultPartition 内部又会做分区，因为输出结果可能会shuffle给下游的多个task。 反压传播两个阶段 跨TaskManager：反压如何从 InputChannel 传播到 ResultSubpartition。 TaskManager：反压如何从 ResultSubpartition 传播到 InputChannel。 跨TaskManager数据传输 LBP：Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 IG 和 RS 分别创建一个 LocalBufferPool 并设置可申请的 MemorySegment 数量。给每一个 task 创建一个 Local Buffer Pool。NBP：TM在启动时，会先初始化 NetworkEnvironment 对象，TM种所有与网络相关的东西都由该类来管理（如Netty连接），其中就包括 NetworkBufferPool 。NetworkEnvironment 和 NetworkBufferPool 是Task间共享的，每个TM只会实例化一个 ，初始化时从 Off-heap Memory中申请，不依赖于JVM。Off-heap Memory：ResultSubPartition 的 buffer 拷贝至 Netty 的 buffer，Netty 的用户态 buffer 拷贝至 Socket 的内核态 buffer。 跨TaskManager反压过程假设生产速度为2，消费速度为1，模拟速度上下游不匹配的情况。数据一直在发送。 运行一段时间之后，InputChannel 的 buffer 已满，向 LBP 申请新的 buffer，被申请过的 buffer 标识为 ‘Used’。 又运行一段时间之后，LBP 已满，只能向 NBP 申请新的 buffer，每个 LBP 都有最大可用 buffer。 又运行一段时间之后，NBP 已满 或者 LBP 达到了使用份额，Netty 还是一直想把数据写到 InputChannel 中，此时就会在 Socket 层 disable Netty 的 autoRead，Netty 不再从 Socket 中读取消息，只能在 Socket 自己的 buffer 中一直做缓存。 又运行一段时间之后，Socket receive buffer 很快就会满，就会把 window = 0 的消息反馈给发送端。 又运行一段时间之后，发送端接收到 window = 0，停止发送。Socket send buffer 也很快就会被写满。Netty 也写不进去到 Socket 了，也会停止写。 又运行一段时间之后，Netty 的 buffer 是无界的，通过 HW 控制是否可写入 Netty 的 buffer，ResultSubpartition 每次写之前会检查 channel.isWritable() ，避免 Netty 的 buffer 无限增长。 又运行一段时间之后，ResultSubpartition 的 buffer 会不断累积，然后会向 LBP 申请内存. 又运行一段时间之后，LBP 已满，向 NBP 申请内存。 又运行一段时间之后，NBP 也满，申请不到内存时 Record Writer 就会 block 住，申请不到 buffer 时就会有一个 wait 操作。 TaskManager内反压过程 假定 TaskA 的 ResultPartition 已经由于下游消费过慢出现反压，已经写阻塞了。将写的阻塞传播给读，输入和输出是在同一个线程里的，线程已经被写 block 住了，当然也没办法消费数据了。InputGate 也会被 block 住。 紧接着会打满各层的 buffer ，并向上游的 TaskManger 推进反压情况。 Flink Credit-based反压机制（since v1.5）TCP-based反压的弊端 由于多路复用Socket，单个Task导致的反压，会阻断整个TM-TM的Socket，连checkpoint barrier也无法发出，导致 checkpoint 延时增大。 反压传播路径太长，导致生效延迟比较大。 Credit-based反压在Flink层面实现类似 TCP 流控的 feedback 机制，credit 可类比为 TCP window。不会要占满 Socket 的 buffer 才出现反压，在ResultSubpartition 和 InputGate 就可以直接产生反压。 ResultSubpartition 向 InputGate 发送数据时，会打上 backlog size ，代表 ResultSubpartition 端积压了多少数据。 InputGate 就根据 backlog size 去估算内存情况，返回credit给发送端。 ResultSubpartition 接收到 credit 之后才会真正发送数据。 Credit-based反压过程仍然假设生产速度为2，消费速度为1，模拟速度上下游不匹配的情况。 反压问题排查定位到反压节点后，分析造成原因的办法和我们分析一个普通程序的性能瓶颈的办法是十分类似的，可能还要更简单一点，因为我们要观察的主要是 Task Thread。 在实践中，很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI 各个 SubTask 的 Records Sent 和 Record Received 来确认，另外 Checkpoint detail 里不同 SubTask 的 State size 也是一个分析数据倾斜的有用指标。 此外，最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题）。最有用的办法就是对 TaskManager 进行 CPU profile，从中我们可以分析到 Task Thread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面，比如我们生产环境中就偶尔遇到卡在 Regex 的用户函数（ReDoS）；如果不是的话要看 Task Thread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是 checkpoint 或者 GC 等系统活动导致的暂时系统暂停。 当然，性能分析的结果也可能是正常的，只是作业申请的资源不足而导致了反压，这就通常要求拓展并行度。值得一提的，在未来的版本 Flink 将会直接在 WebUI 提供 JVM 的 CPU 火焰图[5]，这将大大简化性能瓶颈的分析。 另外 TaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManager JVM 各区内存不合理导致的频繁 Full GC 甚至失联。推荐可以通过给 TaskManager 启用 G1 垃圾回收器来优化 GC，并加上 -XX:+PrintGCDetails 来打印 GC 日志的方式来观察 GC 的问题。 反压监控在storm中，只要监控队列满了，就可以记录下拓扑进入反压了。但是Flink的反压过于天然，无法简单的通过监控队列来监控反压状态。如果一个Task因为反压而降速了，那么它会卡在向 LocalBufferPool 申请内存块上。此时该 Task 的 stack trace 如下： 1234java.lang.Object.wait(Native Method)o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:163)o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) &lt;--- BLOCKING request[...] Flink 就是通过不断采样每个 Task 的 stack trace 来实现反压监控的。JM 会通过 Akka 给每个 TM 发送 TriggerStackTraceSample 消息。 业务代码优化left join 可能会产生大量的 null 数据，导致网络流量剧增，最终导致反压、checkpoint超时等问题。此时可以考虑优化代码，减少 left join、增大operator并发度、拆解任务等。 网络调参// 待补充 反压不触发时的静态限速有了动态反压，静态限速是不是完全没有作用了？ Storage 到 Sink 之间的反压是整个反压的源头，但是不一定被触发，取决于 Storage 的内部实现，如Kafka内部有quato限流机制。但是不是所有的Storage都有这个能力，如 ES ，可能在 ES 端已经出现了大量的 timeout ，也不会向上游反压，此时就需要在 Source 端做静态限流。Flink 1.8 的 kafka source 支持限流，setRate 设置 consumer 每秒钟不能消费超过多少的数据。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink 1.9.0 with Hive]]></title>
    <url>%2F2019%2F09%2F17%2FFlink-1-9-0-with-Hive%2F</url>
    <content type="text"><![CDATA[Apache Flink 从 1.9.0 版本开始增加了与Hive集成的功能，用户可以通过Flink来访问Hive的元数据，以及读写Hive中的表。 新增功能元数据原先Flink提供的 ExternalCatalog 定义非常不完整，基本不可用。提出了一套全新的 Catalog 接口来取代 ExternalCatalog 。新的 Catalog能够支持数据库、表、分区等多种元数据对象。允许一个用户session中维护多个 Catalog 实例，从而访问多个外部系统。并且 Catalog 以可插拔的方式接入Flink，允许用户提供自定义实现。 表数据新增了 flink-connector-hive 模块 Flink SQL Client中使用Hive 安装flink-1.9.0 安装hive-2.3.4 在/usr/local/flink-1.9.0/lib下添加依赖包： 12345678910antlr-runtime-3.5.2.jarantlr4-runtime-4.5.jardatanucleus-api-jdo-4.2.4.jardatanucleus-core-4.1.17.jardatanucleus-rdbms-4.1.19.jarflink-connector-hive_2.11-1.9.0.jarflink-hadoop-compatibility_2.11-1.9.0.jarflink-shaded-hadoop-2-uber-2.7.5-7.0.jarhive-exec-2.3.4.jarjavax.jdo-3.2.0-m3.jar 修改sql-client的配置文件/usr/local/flink-1.9.0/conf/sql-client-defaults.yaml: 123456789# catalogs: [] # empty listcatalogs:# A typical catalog definition looks like: - name: myhive_catalog type: hive #property-version: 2 hive-conf-dir: /usr/local/hive/conf/ #default-database: default hive-version: 2.3.4 启动Flink集群： 1./start-cluster.sh 启动SQL Client： 1./sql-client.sh embedded 列举所有的catalog： 1234Flink SQL&gt; show catalogs;......default_catalogmyhive_catalog 使用catalog、database等： 12345678910111213141516Flink SQL&gt; use catalog myhive_catalog;......Flink SQL&gt; show databases;......defaulttestFlink SQL&gt; use test;......Flink SQL&gt; show tables;......employeeemplyeestudent 查看表结构： 123456Flink SQL&gt; describe employee;......2019-09-06 22:04:15,935 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=miaowenting ip=unknown-ip-addr cmd=get_table : db=test tbl=employee root |-- id: INT |-- name: STRING 查询表数据： 1Flink SQL&gt; select * from employee; 插入表数据有问题，待解决： 12345678Flink SQL&gt; insert into employee(id,name) values (4,&apos;test&apos;);[INFO] Submitting SQL update statement to the cluster...2019-09-06 21:47:29,944 INFO org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default2019-09-06 21:47:29,945 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=miaowenting ip=unknown-ip-addr cmd=get_database: default 2019-09-06 21:47:29,948 INFO org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: test2019-09-06 21:47:29,948 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=miaowenting ip=unknown-ip-addr cmd=get_database: test [ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Partial inserts are not supported Table API中使用HiveTable API创建HiveCatalog12345678910String name = "myhive_catalog";String defaultDatabase = "default";String hiveConfDir = "/usr/local/hive/conf/";String version = "2.3.4";EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().TableEnvironment tableEnv = ...;HiveCatalog hiveCatalog = new HiveCatalog(name,defaultDatabase,hiveConfDir,version);tableEnv.registerCatalog(name,hiveCatalog);tableEnv.useCatalog(name); Table API读写Hive表1234567TableEnvironment tableEnv = ...;tableEnv.registerCatalog(name,hiveCatalog);tableEnv.useCatalog(name);Table source = tableEnv.sqlQuery("select * from source");tableEnv.sqlUpdate("insert into source values('newKey','newValue')");tableEnv.execute("insert into source");]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink Streaming examples]]></title>
    <url>%2F2019%2F09%2F17%2FFlink-Streaming-examples%2F</url>
    <content type="text"><![CDATA[本文主要分析下flink源码中flink-examples-streaming模块，为了方便分析，会把代码都拷贝在文中。 wordcount实时统计单词数量，每来一个计算一次并输出一次。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class WordCount &#123; // ************************************************************************* // PROGRAM // ************************************************************************* public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); DataStream&lt;String&gt; text; if (params.has("input")) &#123; // read the text file from given input path text = env.readTextFile(params.get("input")); &#125; else &#123; // get default test text data text = env.fromElements(new String[] &#123; "miao,She is a programmer", "wu,He is a programmer", "zhao,She is a programmer" &#125;); &#125; DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = // split up the lines in pairs (2-tuples) containing: (word,1) text.flatMap(new Tokenizer()) // group by the tuple field "0" and sum up tuple field "1" .keyBy(0).sum(1); // emit result if (params.has("output")) &#123; counts.writeAsText(params.get("output")); &#125; else &#123; System.out.println("Printing result to stdout. Use --output to specify output path."); counts.print(); &#125; // execute program env.execute("Streaming WordCount"); &#125; // ************************************************************************* // USER FUNCTIONS // ************************************************************************* public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123; // normalize and split the line String[] tokens = value.toLowerCase().split("\\W+"); // emit the pairs for (String token : tokens) &#123; if (token.length() &gt; 0) &#123; out.collect(new Tuple2&lt;&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 输出的结果： 1234567891011121314155&gt; (miao,1)6&gt; (a,1)8&gt; (wu,1)4&gt; (programmer,1)6&gt; (zhao,1)2&gt; (he,1)3&gt; (she,1)8&gt; (is,1)4&gt; (programmer,2)6&gt; (a,2)6&gt; (a,3)3&gt; (she,2)8&gt; (is,2)4&gt; (programmer,3)8&gt; (is,3) socket监听socket端口输入的单词，进行单词统计。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the host and the port to connect to final String hostname; final int port; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); hostname = params.has("hostname") ? params.get("hostname") : "localhost"; port = 9999; &#125; catch (Exception e) &#123; return; &#125; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket // 数据来源是从socket读取，元素可以用分隔符切分 DataStream&lt;String&gt; text = env.socketTextStream(hostname, port, "\n"); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; @Override public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split("\\s")) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy("word") .timeWindow(Time.seconds(10)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; // 统计单词个数 // reduce返回单个的结果值，并且reduce每处理一个元素总是创建一个新值。常用的average,sum,min,max,count,使用reduce方法都可以实现 @Override public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute("Socket Window WordCount"); &#125; // ------------------------------------------------------------------------ /** * Data type for words with count. */ public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123; &#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + " : " + count; &#125; &#125;&#125; 本机启用监听端口： 1nc -l 9999 socket监听端口输入以下内容： 123miao she is a programmerwu he is a programmerzhao she is a programmer 输出的结果： 12345678she : 2programmer : 3he : 1a : 3zhao : 1miao : 1wu : 1is : 3 async主要通过以下示例了解下AsyncFunction作用到DataStream上的使用方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278public class AsyncIOExample &#123; private static final Logger LOG = LoggerFactory.getLogger(AsyncIOExample.class); private static final String EXACTLY_ONCE_MODE = "exactly_once"; private static final String EVENT_TIME = "EventTime"; private static final String INGESTION_TIME = "IngestionTime"; private static final String ORDERED = "ordered"; public static void main(String[] args) throws Exception &#123; // obtain execution environment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // parse parameters final ParameterTool params = ParameterTool.fromArgs(args); // 状态存放路径 final String statePath; // checkpoint模式 final String cpMode; // source生成的最大值 final int maxCount; // RichAsyncFunction 中 线程休眠的因子 final long sleepFactor; // 模拟RichAsyncFunction出错的概率因子 final float failRatio; // 标志RichAsyncFunction 中的消息是有序还是无序的 final String mode; // 设置任务的并行度 final int taskNum; // 使用的Flink时间类型 final String timeType; // 优雅停止RichAsyncFunction中线程池的等待毫秒数 final long shutdownWaitTS; // RichAsyncFunction中执行异步操作的超时时间 final long timeout; try &#123; // check the configuration for the job statePath = params.get("fsStatePath", null); cpMode = params.get("checkpointMode", "exactly_once"); maxCount = params.getInt("maxCount", 100000); sleepFactor = params.getLong("sleepFactor", 100); failRatio = params.getFloat("failRatio", 0.001f);// failRatio = params.getFloat("failRatio", 0.5f); mode = params.get("waitMode", "ordered"); taskNum = params.getInt("waitOperatorParallelism", 1); timeType = params.get("eventType", "EventTime"); shutdownWaitTS = params.getLong("shutdownWaitTS", 20000); timeout = params.getLong("timeout", 10000L); &#125; catch (Exception e) &#123; printUsage(); throw e; &#125; StringBuilder configStringBuilder = new StringBuilder(); final String lineSeparator = System.getProperty("line.separator"); configStringBuilder .append("Job configuration").append(lineSeparator) .append("FS state path=").append(statePath).append(lineSeparator) .append("Checkpoint mode=").append(cpMode).append(lineSeparator) .append("Max count of input from source=").append(maxCount).append(lineSeparator) .append("Sleep factor=").append(sleepFactor).append(lineSeparator) .append("Fail ratio=").append(failRatio).append(lineSeparator) .append("Waiting mode=").append(mode).append(lineSeparator) .append("Parallelism for async wait operator=").append(taskNum).append(lineSeparator) .append("Event type=").append(timeType).append(lineSeparator) .append("Shutdown wait timestamp=").append(shutdownWaitTS); LOG.info(configStringBuilder.toString()); if (statePath != null) &#123; // setup state and checkpoint mode env.setStateBackend(new FsStateBackend(statePath)); &#125; if (EXACTLY_ONCE_MODE.equals(cpMode)) &#123; // 生成checkpoint的默认间隔是1s env.enableCheckpointing(1000L, CheckpointingMode.EXACTLY_ONCE); &#125; else &#123; env.enableCheckpointing(1000L, CheckpointingMode.AT_LEAST_ONCE); &#125; // enable watermark or not if (EVENT_TIME.equals(timeType)) &#123; env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); &#125; else if (INGESTION_TIME.equals(timeType)) &#123; env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); &#125; // 创建一个数据源 // create input stream of an single integer DataStream&lt;Integer&gt; inputStream = env.addSource(new SimpleSource(maxCount)); // 创建async函数，通过等待来模拟异步i/o的过程 // create async function, which will *wait* for a while to simulate the process of async i/o AsyncFunction&lt;Integer, String&gt; function = new SampleAsyncFunction(sleepFactor, failRatio, shutdownWaitTS); // add async operator to streaming job DataStream&lt;String&gt; result; if (ORDERED.equals(mode)) &#123; result = AsyncDataStream.orderedWait( inputStream, function, timeout, TimeUnit.MILLISECONDS, 20).setParallelism(taskNum); &#125; else &#123; result = AsyncDataStream.unorderedWait( inputStream, function, timeout, TimeUnit.MILLISECONDS, 20).setParallelism(taskNum); &#125; // add a reduce to get the sum of each keys. // 统计 key-&gt; 源头数据除以10的余数，分别对应的个数 result.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; private static final long serialVersionUID = -938116068682344455L; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; out.collect(new Tuple2&lt;&gt;(value, 1)); &#125; &#125;).keyBy(0).sum(1).print(); // execute the program env.execute("Async IO Example"); &#125; /** * A checkpointed source. * 具体功能：一个数据流 -&gt; 不断发送一个从0递增的整数 */ private static class SimpleSource implements SourceFunction&lt;Integer&gt;, ListCheckpointed&lt;Integer&gt; &#123; private static final long serialVersionUID = 1L; private volatile boolean isRunning = true; /** * 计数器 */ private int counter = 0; /** * 起始值 */ private int start = 0; /** * 储存快照状态：每次作业执行成功之后，会保存成功的上一条数据的状态，也就是start的值 */ @Override public List&lt;Integer&gt; snapshotState(long checkpointId, long timestamp) throws Exception &#123; return Collections.singletonList(start); &#125; /** * 当执行到某个作业流发生异常时，Flink会调用次方法，将状态还原到上一次的成功checkpoint的那个状态点 */ @Override public void restoreState(List&lt;Integer&gt; state) throws Exception &#123; // 找到最新的一次checkpoint成功时start的值 for (Integer i : state) &#123; this.start = i; &#125; &#125; public SimpleSource(int maxNum) &#123; this.counter = maxNum; &#125; @Override public void run(SourceContext&lt;Integer&gt; ctx) throws Exception &#123; while ((start &lt; counter || counter == -1) &amp;&amp; isRunning) &#123; synchronized (ctx.getCheckpointLock()) &#123; ctx.collect(start); ++start; // loop back to 0 if (start == Integer.MAX_VALUE) &#123; start = 0; &#125; &#125; Thread.sleep(10L); &#125; &#125; @Override public void cancel() &#123; isRunning = false; &#125; &#125; /** * An sample of &#123;@link AsyncFunction&#125; using a thread pool and executing working threads * to simulate multiple async operations. * * &lt;p&gt;For the real use case in production environment, the thread pool may stay in the * async client. * * 一个异步函数示例：用线程池模拟多个异步操作 * 具体功能：处理流任务的异步函数 */ private static class SampleAsyncFunction extends RichAsyncFunction&lt;Integer, String&gt; &#123; private static final long serialVersionUID = 2098635244857937717L; private transient ExecutorService executorService; /** * The result of multiplying sleepFactor with a random float is used to pause * the working thread in the thread pool, simulating a time consuming async operation. * 模拟耗时的异步操作用的：就是假装这个异步操作很耗时，耗时时长为sleepFactor */ private final long sleepFactor; /** * The ratio to generate an exception to simulate an async error. For example, the error * may be a TimeoutException while visiting HBase. * 模拟异步操作出现了异常：就是假装我的流任务的异步操作出现异常啦~ 会报错：Exception : wahahaha... */ private final float failRatio; private final long shutdownWaitTS; SampleAsyncFunction(long sleepFactor, float failRatio, long shutdownWaitTS) &#123; this.sleepFactor = sleepFactor; this.failRatio = failRatio; this.shutdownWaitTS = shutdownWaitTS; &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); executorService = Executors.newFixedThreadPool(30); &#125; @Override public void close() throws Exception &#123; super.close(); ExecutorUtils.gracefulShutdown(shutdownWaitTS, TimeUnit.MILLISECONDS, executorService); &#125; /** * 真正执行异步IO的方法 * 这里用线程池模拟 source支持异步发送数据流 */ @Override public void asyncInvoke(final Integer input, final ResultFuture&lt;String&gt; resultFuture) &#123; executorService.submit(() -&gt; &#123; // wait for while to simulate async operation here // 模拟元素的操作时长：就是这个元素与外部系统交互的时长，然后sleep这么长的时间 long sleep = (long) (ThreadLocalRandom.current().nextFloat() * sleepFactor); try &#123; Thread.sleep(sleep); if (ThreadLocalRandom.current().nextFloat() &lt; failRatio) &#123; // 模拟触发异常：就是与外部系统交互时，假装出错发出了一个异常，此处可以查看日志，观察flink如何checkpoint恢复 // restart-strategy.fixed-delay.attempts 默认为3，重启3次 resultFuture.completeExceptionally(new Exception("wahahahaha...")); &#125; else &#123; // 根据输入的input/10的余数生成key resultFuture.complete( Collections.singletonList("key-" + (input % 10))); &#125; &#125; catch (InterruptedException e) &#123; resultFuture.complete(new ArrayList&lt;&gt;(0)); &#125; &#125;); &#125; &#125;&#125; iteration本示例为输入int值键值对，迭代计算斐波那契数列值大于100时需要计算的步长。 斐波那契数列：F(n) = F(n-1) + F(n-2) 假设初始键值对为(34,11)，则产生的斐波那契数列为：34,11,45,56,101经过本代码运行的结果应该是((34,11),3)，需要经历3步长，即累加3次才能使得数列值101 &gt; 100，才能打到output流中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182public class IterateExample &#123; private static final int BOUND = 100; // ************************************************************************* // PROGRAM // ************************************************************************* public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); // obtain execution environment and set setBufferTimeout to 1 to enable // continuous flushing of the output buffers (lowest latency) StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment() .setBufferTimeout(1); // make parameters available in the web interface env.getConfig().setGlobalJobParameters(params); // create input stream of integer pairs DataStream&lt;Tuple2&lt;Integer, Integer&gt;&gt; inputStream; if (params.has("input")) &#123; inputStream = env.readTextFile(params.get("input")).map(new FibonacciInputMap()); &#125; else &#123; inputStream = env.addSource(new RandomFibonacciSource()); &#125; // create an iterative data stream from the input with 5 second timeout // 经过InputMap() 将Tuple2 转成 Tuple5 // 指定等待反馈输入的最大时间间隔，如果超过该时间间隔没有反馈元素到来，那么该迭代将会终止 IterativeStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; it = inputStream.map(new InputMap()) .iterate(5000); // apply the step function to get the next Fibonacci number // increment the counter and split the output with the output selector // 再经过Step()步函数 获取下一个斐波那契数 // 并进行分流 SplitStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; step = it.map(new Step()) .split(new MySelector()); // close the iteration by selecting the tuples that were directed to the // 'iterate' channel in the output selector it.closeWith(step.select("iterate")); // to produce the final output select the tuples directed to the // 'output' channel then get the input pairs that have the greatest iteration counter // on a 1 second sliding window DataStream&lt;Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt;&gt; numbers = step.select("output") .map(new OutputMap()); // emit results if (params.has("output")) &#123; numbers.writeAsText(params.get("output")); &#125; else &#123; System.out.println("Printing result to stdout. Use --output to specify output path."); numbers.print(); &#125; long startTime = System.currentTimeMillis(); System.out.println("Start time: " + startTime); // execute the program env.execute("Streaming Iteration Example"); System.out.println("Spent time: " + (System.currentTimeMillis() - startTime)); &#125; // ************************************************************************* // USER FUNCTIONS // ************************************************************************* /** * Generate BOUND number of random integer pairs from the range from 1 to BOUND/2. * 随机生成Integer键值对的source */ private static class RandomFibonacciSource implements SourceFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt; &#123; private static final long serialVersionUID = 1L; private Random rnd = new Random(); private volatile boolean isRunning = true; private int counter = 0; @Override public void run(SourceContext&lt;Tuple2&lt;Integer, Integer&gt;&gt; ctx) throws Exception &#123; while (isRunning &amp;&amp; counter &lt; BOUND) &#123; int first = rnd.nextInt(BOUND / 2 - 1) + 1; int second = rnd.nextInt(BOUND / 2 - 1) + 1; ctx.collect(new Tuple2&lt;&gt;(first, second)); counter++; Thread.sleep(50L); &#125; &#125; @Override public void cancel() &#123; isRunning = false; &#125; &#125; /** * Generate random integer pairs from the range from 0 to BOUND/2. */ private static class FibonacciInputMap implements MapFunction&lt;String, Tuple2&lt;Integer, Integer&gt;&gt; &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Integer, Integer&gt; map(String value) throws Exception &#123; String record = value.substring(1, value.length() - 1); String[] splitted = record.split(","); return new Tuple2&lt;&gt;(Integer.parseInt(splitted[0]), Integer.parseInt(splitted[1])); &#125; &#125; /** * Map the inputs so that the next Fibonacci numbers can be calculated while preserving the original input tuple. * A counter is attached to the tuple and incremented in every iteration step. */ public static class InputMap implements MapFunction&lt;Tuple2&lt;Integer, Integer&gt;, Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; &#123; private static final long serialVersionUID = 1L; @Override public Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; map(Tuple2&lt;Integer, Integer&gt; value) throws Exception &#123; // 结果map转换将Tuple2转成Tuple5 return new Tuple5&lt;&gt;(value.f0, value.f1, value.f0, value.f1, 0); &#125; &#125; /** * Iteration step function that calculates the next Fibonacci number. */ public static class Step implements MapFunction&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;, Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; &#123; private static final long serialVersionUID = 1L; @Override public Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; map(Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; value) throws Exception &#123; return new Tuple5&lt;&gt;(value.f0, value.f1, value.f3, value.f2 + value.f3, ++value.f4); &#125; &#125; /** * OutputSelector testing which tuple needs to be iterated again. */ public static class MySelector implements OutputSelector&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; select(Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; value) &#123; List&lt;String&gt; output = new ArrayList&lt;&gt;(); if (value.f2 &lt; BOUND &amp;&amp; value.f3 &lt; BOUND) &#123; // 指定流的一部分用于反馈给迭代头 // 反馈流反馈给迭代头就意味着一个迭代的完整逻辑的完成 output.add("iterate"); &#125; else &#123; // 指定流的另一部分发给下游 output.add("output"); &#125; return output; &#125; &#125; /** * Giving back the input pair and the counter. */ public static class OutputMap implements MapFunction&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;, Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt;&gt; &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt; map(Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; value) throws Exception &#123; return new Tuple2&lt;&gt;(new Tuple2&lt;&gt;(value.f0, value.f1), value.f4); &#125; &#125;&#125; 输出的部分结果： 123456789101112136&gt; ((43,24),3)7&gt; ((23,25),3)8&gt; ((4,46),3)1&gt; ((42,49),2)2&gt; ((29,22),3)3&gt; ((36,3),4)4&gt; ((30,36),2)5&gt; ((18,24),3)6&gt; ((38,9),3)7&gt; ((19,44),2)8&gt; ((42,40),2)1&gt; ((16,9),5)2&gt; ((2,7),6) join本示例演示了如何使用DataStream API进行双流join。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class WindowJoin &#123; // ************************************************************************* // PROGRAM // ************************************************************************* public static void main(String[] args) throws Exception &#123; // parse the parameters final ParameterTool params = ParameterTool.fromArgs(args); final long windowSize = params.getLong("windowSize", 2000); final long rate = params.getLong("rate", 3L); System.out.println("Using windowSize=" + windowSize + ", data rate=" + rate); System.out.println("To customize example, use: WindowJoin [--windowSize &lt;window-size-in-millis&gt;] [--rate &lt;elements-per-second&gt;]"); // obtain execution environment, run this example in "ingestion time" StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); // make parameters available in the web interface env.getConfig().setGlobalJobParameters(params); // create the data sources for both grades and salaries // john,4 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades = GradeSource.getSource(env, rate); // john,18000 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries = SalarySource.getSource(env, rate); // run the actual window join program // for testability, this functionality is in a separate method. DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; joinedStream = runWindowJoin(grades, salaries, windowSize); // print the results with a single thread, rather than in parallel // 输出 john,4,18000 joinedStream.print().setParallelism(1); // execute program env.execute("Windowed Join Example"); &#125; public static DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; runWindowJoin( DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades, DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries, long windowSize) &#123; return grades.join(salaries) .where(new NameKeySelector()) .equalTo(new NameKeySelector()) .window(TumblingEventTimeWindows.of(Time.milliseconds(windowSize))) .apply(new JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;&gt;() &#123; @Override public Tuple3&lt;String, Integer, Integer&gt; join( Tuple2&lt;String, Integer&gt; first, Tuple2&lt;String, Integer&gt; second) &#123; return new Tuple3&lt;String, Integer, Integer&gt;(first.f0, first.f1, second.f1); &#125; &#125;); &#125; private static class NameKeySelector implements KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt; &#123; @Override public String getKey(Tuple2&lt;String, Integer&gt; value) &#123; return value.f0; &#125; &#125;&#125; sideoutput当想要拆分数据流时，通常需要复制流，使用旁路输出可以直接过滤出不要的数据。示例中过滤出了长度 &gt; 5的单词 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class SideOutputExample &#123; /** * We need to create an &#123;@link OutputTag&#125; so that we can reference it when emitting * data to a side output and also to retrieve the side output stream from an operation. * OutputTag用来标识一个旁路输出流 */ private static final OutputTag&lt;String&gt; rejectedWordsTag = new OutputTag&lt;String&gt;("rejected") &#123;&#125;; public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); // make parameters available in the web interface env.getConfig().setGlobalJobParameters(params); // get input data DataStream&lt;String&gt; text; if (params.has("input")) &#123; // read the text file from given input path text = env.readTextFile(params.get("input")); &#125; else &#123; // get default test text data text = env.fromElements(WordCountData.WORDS); &#125; SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tokenized = text .keyBy(new KeySelector&lt;String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Integer getKey(String value) throws Exception &#123; return 0; &#125; &#125;) .process(new Tokenizer()); // 旁路输出流 DataStream&lt;String&gt; rejectedWords = tokenized // 通过OutputTag从源source中获取旁路输出 .getSideOutput(rejectedWordsTag) .map(new MapFunction&lt;String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public String map(String value) throws Exception &#123; return "rejected: " + value; &#125; &#125;); // 正常输出流，对正常数据做5s翻滚窗口的单词统计 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = tokenized .keyBy(0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) // group by the tuple field "0" and sum up tuple field "1" .sum(1); // emit result if (params.has("output")) &#123; counts.writeAsText(params.get("output")); rejectedWords.writeAsText(params.get("rejected-words-output")); &#125; else &#123; System.out.println("Printing result to stdout. Use --output to specify output path."); counts.print(); rejectedWords.print(); &#125; // execute program env.execute("Streaming WordCount SideOutput"); &#125; // ************************************************************************* // USER FUNCTIONS // ************************************************************************* public static final class Tokenizer extends ProcessFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; private static final long serialVersionUID = 1L; @Override public void processElement( String value, Context ctx, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; // normalize and split the line String[] tokens = value.toLowerCase().split("\\W+"); // emit the pairs for (String token : tokens) &#123; if (token.length() &gt; 5) &#123; // 长度大于5的单词会作为旁路输出，ctx.output是将数据发送到旁路输出中 ctx.output(rejectedWordsTag, token); &#125; else if (token.length() &gt; 0) &#123; // 将数据发送到常规输出中 out.collect(new Tuple2&lt;&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 输出的结果： 123456789106&gt; rejected: question6&gt; rejected: whether6&gt; rejected: nobler6&gt; rejected: suffer......1&gt; (them,1)4&gt; (not,2)2&gt; (would,2)1&gt; (weary,1)...... windowingsession window示例演示了基于EventTime的会话窗口，分析了watermark生成以及触发窗口计算的时机。对于将被窗口丢弃的数据，如(“a”,1L,2)，可以sideoutput。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class SessionWindowing &#123; @SuppressWarnings("serial") public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); env.setParallelism(1); final boolean fileOutput = params.has("output"); final List&lt;Tuple3&lt;String, Long, Integer&gt;&gt; input = new ArrayList&lt;&gt;(); // key、event time时间戳、key出现的次数 input.add(new Tuple3&lt;&gt;("a", 1L, 1)); input.add(new Tuple3&lt;&gt;("b", 1L, 1)); input.add(new Tuple3&lt;&gt;("b", 3L, 1)); input.add(new Tuple3&lt;&gt;("b", 5L, 1)); input.add(new Tuple3&lt;&gt;("c", 6L, 1)); // 即将被窗口丢弃的数据 input.add(new Tuple3&lt;&gt;("a", 1L, 2)); // We expect to detect the session "a" earlier than this point (the old // functionality can only detect here when the next starts) input.add(new Tuple3&lt;&gt;("a", 10L, 1)); // We expect to detect session "b" and "c" at this point as well input.add(new Tuple3&lt;&gt;("c", 11L, 1)); DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; source = env .addSource(new SourceFunction&lt;Tuple3&lt;String, Long, Integer&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public void run(SourceContext&lt;Tuple3&lt;String, Long, Integer&gt;&gt; ctx) throws Exception &#123; for (Tuple3&lt;String, Long, Integer&gt; value : input) &#123; ctx.collectWithTimestamp(value, value.f1); // 发射watermark ctx.emitWatermark(new Watermark(value.f1 - 1)); &#125; // input输入流中的数据读取完毕之后，发射一个大的watermark，确保触发最后的窗口计算 // 无限流，表示终止的watermark，需要一个超过window的end time的watermark来触发window计算 ctx.emitWatermark(new Watermark(Long.MAX_VALUE)); &#125; @Override public void cancel() &#123; &#125; &#125;); // We create sessions for each id with max timeout of 3 time units DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; aggregated = source .keyBy(0) .window(EventTimeSessionWindows.withGap(Time.milliseconds(3L))) .sum(2); if (fileOutput) &#123; aggregated.writeAsText(params.get("output")); &#125; else &#123; System.out.println("Printing result to stdout. Use --output to specify output path."); aggregated.print(); &#125; env.execute(); &#125;&#125; 输出的结果： 1234567(a,1,1)(b,1,3)(c,6,1)(a,10,1)(c,11,1) 在代码中添加一些日志打印，watermark生成与触发计算详情如下： 1234567891011121314151617181920212223Advanced watermark 0Advanced watermark 2Advanced watermark 4# watermark 4 = a 窗口的结束时间4，所以触发a计算输出Timer&#123;timestamp=3, key=(a), namespace=TimeWindow&#123;start=1, end=4&#125;&#125;(a,1,1)Advanced watermark 5Advanced watermark 9# (b,1,1)、(b,3,1)、b(b,5,1)，这3条数据会进行窗口合并，所以这里的结束时间是8# watermark 9 &gt; b 窗口的结束时间8，所以触发b计算输出Timer&#123;timestamp=7, key=(b), namespace=TimeWindow&#123;start=1, end=8&#125;&#125;(b,1,3)# watermark 9 = c 窗口的结束时间9，所以触发c计算输出Timer&#123;timestamp=8, key=(c), namespace=TimeWindow&#123;start=6, end=9&#125;&#125;(c,6,1)Advanced watermark 10Advanced watermark 9223372036854775807# watermark 9223372036854775807 &gt; a 窗口的结束时间13，所以触发a计算输出Timer&#123;timestamp=12, key=(a), namespace=TimeWindow&#123;start=10, end=13&#125;&#125;(a,10,1)# watermark 9223372036854775807 &gt; c 窗口的结束时间14，所以触发c计算输出Timer&#123;timestamp=13, key=(c), namespace=TimeWindow&#123;start=11, end=14&#125;&#125;(c,11,1) Flink何时触发window ？ 121. watermark &gt; window_end_time (对于out-of-order以及正常的数据而言)2. 在[window_start_time,window_end_time)区间有数据存在 窗口合并过程示例： count window slide count window 12345678910111213141516171819202122232425262728293031323334353637383940414243public class SlideCountWindowExample &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); env.setParallelism(1); final int windowSize = params.getInt("window", 3); final int slideSize = params.getInt("slide", 2); Tuple2[] elements = new Tuple2[]&#123; Tuple2.of("a", "1"), Tuple2.of("a", "2"), Tuple2.of("a", "3"), Tuple2.of("a", "4"), Tuple2.of("a", "5"), Tuple2.of("a", "6"), Tuple2.of("b", "7"), Tuple2.of("b", "8"), Tuple2.of("b", "9"), Tuple2.of("b", "0") &#125;; // read source data DataStreamSource&lt;Tuple2&lt;String, String&gt;&gt; inStream = env.fromElements(elements); // calculate DataStream&lt;Tuple2&lt;String, String&gt;&gt; outStream = inStream .keyBy(0) // sliding count window of 3 elements size and 2 elements trigger interval .countWindow(windowSize, slideSize) .reduce( new ReduceFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123; @Override public Tuple2&lt;String, String&gt; reduce(Tuple2&lt;String, String&gt; value1, Tuple2&lt;String, String&gt; value2) throws Exception &#123; return Tuple2.of(value1.f0, value1.f1 + "" + value2.f1); &#125; &#125; ); outStream.print(); env.execute("WindowWordCount"); &#125;&#125; 输出的结果： 12345(a,12)(a,234)(a,456)(b,78)(b,890) 每进来2个元素，就对最近的3个元素计算一遍 查看内部源码： 1234567public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size, long slide) &#123; return window(GlobalWindows.create()) // 在触发计算之前之后的剔除操作 .evictor(CountEvictor.of(size)) // 触发条件是slide步长的个数 .trigger(CountTrigger.of(slide));&#125; tumble count window 123456789101112131415161718192021222324252627282930313233343536373839404142public class TumbleCountWindowExample &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); env.setParallelism(1); final int windowSize = params.getInt("window", 3); Tuple2[] elements = new Tuple2[]&#123; Tuple2.of("a", "1"), Tuple2.of("a", "2"), Tuple2.of("a", "3"), Tuple2.of("a", "4"), Tuple2.of("a", "5"), Tuple2.of("a", "6"), Tuple2.of("b", "7"), Tuple2.of("b", "8"), Tuple2.of("b", "9"), Tuple2.of("b", "0") &#125;; // read source data DataStreamSource&lt;Tuple2&lt;String, String&gt;&gt; inStream = env.fromElements(elements); // calculate DataStream&lt;Tuple2&lt;String, String&gt;&gt; outStream = inStream .keyBy(0) // tumbling count window of 3 elements size .countWindow(windowSize) .reduce( new ReduceFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123; @Override public Tuple2&lt;String, String&gt; reduce(Tuple2&lt;String, String&gt; value1, Tuple2&lt;String, String&gt; value2) throws Exception &#123; return Tuple2.of(value1.f0, value1.f1 + "" + value2.f1); &#125; &#125; ); outStream.print(); env.execute("WindowWordCount"); &#125;&#125; 输出的结果： 123(a,123)(a,456)(b,789) 最后一条 Tuple2.of(“b”, “0”) 被丢弃，因为最后一条数据已经无法触发计算了]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
</search>
